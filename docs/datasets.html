<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>gwadama.datasets API documentation</title>
<meta name="description" content="datasets.py â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gwadama.datasets</code></h1>
</header>
<section id="section-intro">
<p>datasets.py</p>
<p>Main classes to manage GW datasets.</p>
<p>There are two basic type of datasets, clean and injected:</p>
<ul>
<li>
<p>Clean datasets' classes inherit from the Base class, extending their properties
as needed.</p>
</li>
<li>
<p>Injected datasets' classes inherit from the BaseInjected class, and
optionally from other UserDefined(Base) classes.</p>
</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;datasets.py

Main classes to manage GW datasets.

There are two basic type of datasets, clean and injected:

- Clean datasets&#39; classes inherit from the Base class, extending their properties
  as needed.

- Injected datasets&#39; classes inherit from the BaseInjected class, and
  optionally from other UserDefined(Base) classes.

&#34;&#34;&#34;
from copy import deepcopy
import itertools
from typing import Callable

# from clawdia.estimators import find_merger  # Imported only when instancing CoReWaves.
from gwpy.timeseries import TimeSeries
import numpy as np
import pandas as pd
import scipy as sp
from scipy.interpolate import make_interp_spline as sp_make_interp_spline
from sklearn.model_selection import train_test_split

from . import ioo
from . import detectors
from . import dictools
from . import fat
from . import synthetic
from . import tat
from .units import *


__all__ = [&#39;Base&#39;, &#39;BaseInjected&#39;, &#39;SyntheticWaves&#39;, &#39;InjectedSyntheticWaves&#39;,
           &#39;CoReWaves&#39;, &#39;InjectedCoReWaves&#39;]


class Base:
    &#34;&#34;&#34;Base class for all datasets.

    TODO: Update docstring.

    Any dataset made of &#39;clean&#39; (noiseless) GW must inherit this class.
    It is designed to store strains as nested dictionaries, with each level&#39;s
    key identifying a class/property of the strain. Each individual strain is a
    1D NDArray containing the features.
    
    By default there are two basic levels:
        
        - Class; to group up strains in categories.
        
        - Id; An unique identifier for each strain, which must exist in the
          metadata DataFrame as Index.
    
    Extra depths can be added, and will be thought of as modifications of the
    same original strains from the upper identifier level. If splitting the
    dataset into train and test susbsets, only combinations of (Class, Id) will
    be considered.

    NOTE: This class shall not be called directly. Use one of its subclasses.
    
    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).

    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the ID).
        The total number of different waves must be equal to `len(metadata)`;
        this does not include possible variations such polarizations or
        multiple scallings of the same waveform when performing injections.
    
    strains : dict[dict [...]]
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        
        - Shape: {class: {id: strain} }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        - Extra depths can be added as variations of each strain, such as
          polarizations.
    
    labels : dict
        Class label of each wave ID, with shape {id: class_label}.
        Each ID points to the label of its class in the &#39;classes&#39; attribute.
        Can be automatically constructed by calling the &#39;_gen_labels()&#39; method.
    
    max_length : int
        Length of the longest strain in the dataset.
        Remember to update it if modifying the strains length.
    
    times : dict, optional
        Time samples associated with the strains, following the same structure
        up to the second depth level: {class: {id: time_points} }
        Useful when the sampling rate is variable or different between strains.
        If None, all strains are assumed to be constantly sampled to the
        sampling rate indicated by the &#39;sample_rate&#39; attribute.
    
    sample_rate : int, optional
        If the &#39;times&#39; attribute is present, this value is ignored. Otherwise
        it is assumed all strains are constantly sampled to this value.
        
        NOTE: If dealing with variable sampling rates, avoid setting this
        attribute to anything other than None.
    
    random_seed : int, optional
        Value passed to &#39;sklearn.model_selection.train_test_split&#39; to generate
        the Train and Test subsets. Saved for reproducibility purposes.
    
    Xtrain, Xtest : dict, optional
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        Shape: {id: strain}.
        The &#39;id&#39; corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
        They are just another views into the same data stored at &#39;self.strains&#39;,
        so no copies are performed.
    
    Ytrain, Ytest : NDArray[int], optional
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.
        See the attribute &#39;labels&#39; for more info.
    
    Caveats
    -------
    - The additional depths in the strains nested dictionary can&#39;t be directly
      tracked by the metadata Dataframe.
    - If working with two polarizations, they can be stored with just an
      extra depth layer.
    
    &#34;&#34;&#34;
    def __init__(self):
        &#34;&#34;&#34;Overwrite when inheriting!&#34;&#34;&#34;

        raise NotImplementedError(&#34;Base class should not be called directly.&#34;)

        #----------------------------------------------------------------------
        # Attributes whose values must be set up during initialization.
        #----------------------------------------------------------------------
    
        self.strains: dict = None
        self.classes: dict[str] = None
        self._check_classes_dict(self.classes)
        self.metadata: pd.DataFrame = None
        self.labels: dict[int] = self._gen_labels()
        
        # Number of nested layers in strains&#39; dictionary. Keep updated always:
        self._dict_depth: int = dictools.get_depth(self.strains)

        self.max_length = self._find_max_length()
        self.random_seed: int = None  # SKlearn train_test_split doesn&#39;t accept a Generator yet.
        self._track_times = False  # If True, self.times must be not None.

        #----------------------------------------------------------------------
        # Attributes whose values can be set up or otherwise left as follows.
        #----------------------------------------------------------------------

        # Whitening related attributes.
        self.whitened = False
        self.whiten_params = {}
        self.nonwhiten_strains = None

        # Time tracking related attributes.
        self.sample_rate: int = None
        self.times: dict = None
        
        # Train/Test subset splits (views into the same &#39;self.strains&#39;).
        #   Timeseries:
        self.Xtrain: np.ndarray = None
        self.Xtest: np.ndarray = None
        #   Labels:
        self.Ytrain: np.ndarray = None
        self.Ytest: np.ndarray = None

    def _check_classes_dict(self, classes: dict[str]):
        if not isinstance(classes, dict):
            raise TypeError(&#34;&#39;classes&#39; must be a dictionary&#34;)
        
        if not all(isinstance(k, str) for k in classes.keys()):
            raise TypeError(&#34;&#39;classes&#39; keys must be strings&#34;)
        
        labels = classes.values()
        if not all(isinstance(label, int) for label in labels):
            raise TypeError(&#34;&#39;classes&#39; values must be integers&#34;)
        if len(set(labels)) != len(classes):
            raise ValueError(&#34;&#39;classes&#39; values must be unique&#34;)
    
    def __len__(self):
        return len(self.metadata)

    def _gen_labels(self) -&gt; dict:
        &#34;&#34;&#34;Constructs the labels&#39; dictionary.

        The labels attribute maps each ID to the integer value of its class,
        mapped in the &#39;classes&#39; attribute.
        
        Returns
        -------
        labels : dict
            Shape {id: class_label} for each GW in the dataset.
        
        &#34;&#34;&#34;
        labels = {}
        for clas, id_ in self.keys(max_depth=2):
            labels[id_] = self.classes[clas]
        
        return labels

    def _init_strains_dict(self) -&gt; dict:
        return {clas: {} for clas in self.classes}
    
    def _init_times_dict(self) -&gt; dict:
        return dictools._replicate_structure_nested_dict(self.strains)

    def _find_max_length(self) -&gt; int:
        &#34;&#34;&#34;Return the length of the longest signal present in strains.&#34;&#34;&#34;

        max_length = 0
        for *_, strain in self.items():
            l = len(strain)
            if l &gt; max_length:
                max_length = l

        return max_length

    def _gen_times(self) -&gt; dict:
        &#34;&#34;&#34;Generate the time arrays associated to the strains.

        Assumes a constant sampling rate.
        
        Returns
        -------
        times : dict
            Nested dictionary with the same shape as &#39;self.strains&#39;.
        
        &#34;&#34;&#34;
        times = self._init_times_dict()
        for *keys, strain in self.items():
            length = len(strain)
            t_end = (length - 1) / self.sample_rate
            time = np.linspace(0, t_end, length)
            dictools.set_value_to_nested_dict(times, keys, time)
        
        return times

    def keys(self, max_depth: int = None) -&gt; list:
        &#34;&#34;&#34;Return the unrolled combinations of all strain identifiers.

        Return the unrolled combinations of all keys  of the nested dictionary
        of strains by a hierarchical recursive search.
        
        It can be thought of as the extended version of Python&#39;s
        &#39;dict().keys()&#39;, although this returns a plain list.

        Parameters
        ----------
        max_depth : int, optional
            If specified, it is the number of layers to iterate to at most in
            the nested &#39;strains&#39; dictionary.
        
        Returns
        -------
        keys : list
            The unrolled combination in a Python list.
        
        &#34;&#34;&#34;
        keys = dictools._unroll_nested_dictionary_keys(self.strains, max_depth=max_depth)

        return keys

    def items(self):
        &#34;&#34;&#34;Return a new view of the dataset&#39;s items with unrolled indices.

        Each iteration consists on a tuple containing all the nested keys in
        &#39;self.strains&#39; along with the corresponding strain,
        (clas, id, *, strain).
        
        It can be thought of as an extension of Python&#39;s `dict.items()`.
        Useful to quickly iterate over all items in the dataset.

        Example of usage with an arbitrary number of keys in the nested
        dictionary of strains:
        ```
        for *keys, strain in self.items():
            print(f&#34;Number of identifiers: {len(keys)}&#34;)
            print(f&#34;Length of the strain: {len(strain)}&#34;)
            do_something(strain)
        ```
        
        &#34;&#34;&#34;
        for indices in self.keys():
            yield (*indices, self.get_strain(*indices))

    def find_class(self, id):
        &#34;&#34;&#34;Find which &#39;class&#39; corresponds the strain &#39;id&#39;.

        Finds the &#39;class&#39; of the strain represented by the unique identifier
        &#39;id&#39;.

        Parameters
        ----------
        id : str
            Unique identifier of the string, that which also appears in the
            `metadata.index` DataFrame.
        
        Returns
        -------
        clas : int | str
            Class key associated to the strain &#39;id&#39;.
        
        &#34;&#34;&#34;
        return dictools._find_level0_of_level1(self.strains, id)

    def get_strain(self, *indices, normalize=False) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single strain from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;strains&#39; attribute.

        Parameters
        ----------
        *indices : str | int
            The indices of the strain to retrieve.
        
        normalize : bool
            If True, the returned strain will be normalized to its maximum
            amplitude.
        
        Returns
        -------
        strain : np.ndarray
            The requested strain.
        
        &#34;&#34;&#34;
        if len(indices) != self._dict_depth:
            raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)

        strain = dictools._get_value_from_nested_dict(self.strains, indices)
        if normalize:
            strain /= np.max(np.abs(strain))

        return strain

    def get_strains_array(self, length: int = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Get all strains stacked in a zero-padded Numpy 2d-array.

        Stacks all signals into an homogeneous numpy array whose length
        (axis=1) is determined by either &#39;length&#39; or, if None, by the longest
        strain in the subset.
        The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;strains_array&#39;. If None, the longest signal
            determines the length.

        Returns
        -------
        strains_array : np.ndarray
            train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.

        &#34;&#34;&#34;
        strains_flat = dictools.flatten_nested_dict(self.strains)
        strains_array, lengths = dictools.dict_to_stacked_array(strains_flat, target_length=length) 

        return strains_array, lengths

    def get_times(self, *indices) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single time array from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;times&#39; attribute.
        
        &#34;&#34;&#34;        
        if len(indices) != self._dict_depth:
            raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)
        
        return dictools._get_value_from_nested_dict(self.times, indices)

    def shrink_strains(self, limits: tuple | dict) -&gt; None:
        &#34;&#34;&#34;Shrink strains to a specific interval.

        Shrink strains (and their associated time arrays if present) to the
        interval given by &#39;limits&#39;.
        
        It also updates the &#39;max_length&#39; attribute.

        Parameters
        ----------
        limits : tuple | dict
            The limits of the interval to shrink to.
            If limits is a tuple, it must be of the form (start, end) in
            samples.
            If limits is a dictionary, it must be of the form {id: (start, end)},
            where id is the identifier of each strain.
            
            NOTE: If extra layers below ID are present, they will be shrunk
            accordingly.

        &#34;&#34;&#34;
        if isinstance(limits, tuple):
            limits_d = {id: limits for id in self.metadata.index}
        else:
            limits_d = limits

        for clas, id, *keys in self.keys():
            strain = self.get_strain(clas, id, *keys)
            # Same shrinking limits for all possible strains below ID layer.
            start, end = limits_d[id]
            strain = strain[start:end]
            dictools.set_value_to_nested_dict(self.strains, [clas,id,*keys], strain)

            if self._track_times:
                times = self.get_times(clas, id, *keys)
                times = times[start:end]
                dictools.set_value_to_nested_dict(self.times, [clas,id,*keys], times)

        self.max_length = self._find_max_length()

    def resample(self, sample_rate, verbose=False) -&gt; None:
        &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

        This assumes time tracking either with time arrays or with the
        sampling rate provided during initialization, which will be used to
        generate the time arrays previous to the resampling.

        This method updates the sample_rate and the max_length.

        Parameters
        ----------
        sample_rate : int
            The new sampling rate in Hz.

        verbose : bool
            If True, print information about the resampling.
        
        &#34;&#34;&#34;
        # Set up the time points associated to each strain in case it is not
        # provided.
        #
        if self._track_times:
            times = self.times
        else:
            if sample_rate == self.sample_rate:
                raise ValueError(&#34;trying to resample to the same sampling rate&#34;)
            if self.sample_rate is None:
                raise ValueError(&#34;neither time samples nor a global sampling rate were defined&#34;)
            
            self.times = self._gen_times()
            self._track_times = True

        for *keys, strain in self.items():
            time = dictools._get_value_from_nested_dict(self.times, keys)
            strain_resampled, time_resampled, sf_up, factor_down = tat.resample(
                strain, time, sample_rate, full_output=True
            )
            dictools.set_value_to_nested_dict(self.strains, keys, strain_resampled)
            dictools.set_value_to_nested_dict(self.times, keys, time_resampled)
            
            if verbose:
                print(
                    f&#34;Strain {keys[0]}::{keys[1]} up. to {sf_up} Hz, down by factor {factor_down}&#34;
                )

        self.sample_rate = sample_rate
        self.max_length = self._find_max_length()
    
    def whiten(self,
               asd_array: np.ndarray = None,
               pad: int = 0,
               highpass: int = None,
               flength: float = None,
               normed: bool = False) -&gt; None:
        &#34;&#34;&#34;Whiten the strains.
        
        Calling this method performs the whitening of all strains.
        Optionally, strains are first zero-padded, whitened and then shrunk to
        their initial size. This is useful to remove the vignetting effect.
        
        NOTE: Original (non-whitened) strains will be stored in the
        &#39;nonwhiten_strains&#39; attribute.
        
        &#34;&#34;&#34;
        if self.whitened:
            raise RuntimeError(&#34;dataset already whitened&#34;)

        if self.strains is None:
            raise RuntimeError(&#34;no strains have been given or generated yet&#34;)
        
        self.nonwhiten_strains = deepcopy(self.strains)
        
        for *keys, strain in self.items():
            strain_w = fat.whiten(
                strain, asd=asd_array, sample_rate=self.sample_rate, flength=flength,
                highpass=highpass, pad=pad, normed=normed
            )
            # Update strains attribute.
            dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
        
        self.whitened = True
        self.whiten_params = {
            &#34;asd_array&#34;: asd_array,
            &#34;pad&#34;: pad,
            &#34;highpass&#34;: highpass,
            &#34;flength&#34;: flength,
            &#34;normed&#34;: normed
        }
        
        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def build_train_test_subsets(self, train_size: int | float, random_seed: int = None):
        &#34;&#34;&#34;Generate a random Train and Test subsets.

        Only entries in the index of &#39;metadata&#39; DataFrame are considered
        independent waveforms, any extra key (layer) in the &#39;strains&#39; dict
        is treated monolithically during the shuffle.
        
        The strain values are just new views into the &#39;strains&#39; attribute.
        The shuffling is performed by Scikit-Learn&#39;s function
        &#39;train_test_split&#39;, with stratification enabled.

        Parameters
        ----------
        train_size : int | float
            If float, should be between 0.0 and 1.0 and represent the proportion
            of the dataset to include in the train subset.
            If int, represents the absolute number of train waves.
            
            Ref: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
        
        random_seed : int, optional
            Passed directly to &#39;sklearn.model_selection.train_test_split&#39;.
            It is also saved in its homonymous attribute.
            
        &#34;&#34;&#34;
        indices = list(self.metadata.index)
        i_train, i_test = train_test_split(
            indices,
            train_size=train_size,
            random_state=random_seed,
            shuffle=True,
            stratify=list(self.labels.values())
        )
        self.Xtrain, self.Ytrain = self._build_subset_strains(i_train)
        self.Xtest, self.Ytest = self._build_subset_strains(i_test)
        self.random_seed = random_seed
    
    def _build_subset_strains(self, indices):
        &#34;&#34;&#34;Return a subset of strains and their labels based on their ID.

        Return a new view into &#39;self.strains&#39; using the input indices (ID) as
        the first layer of the nested dictionary.

        This collapses the first layer, the class, leaving the unique
        identifier ID as first layer. Nevertheless, the rest of possible layers
        beneath &#39;ID&#39; are monolithically preserved.
        
        Parameters
        ----------
        indices : array-like
            The indices are w.r.t. the Pandas &#39;self.metadata.index&#39;.

        Returns
        -------
        strains : dict {id: strain}
            The id key is the strain&#39;s index at &#39;self.metadata&#39;.
        
        labels : NDArray
            1D Array containing the labels associated to &#39;strains&#39;.
        
        &#34;&#34;&#34;
        strains = {}
        labels = np.empty(len(indices), dtype=int)
        for i, id_ in enumerate(indices):
            labels[i] = self.labels[id_]
            clas = self.find_class(id_)
            strains[id_] = self.strains[clas][id_]
        
        return strains, labels

    def _update_train_test_subsets(self):
        &#34;&#34;&#34;Builds again the Train/Test subsets from the main strains attribute.&#34;&#34;&#34;

        id_train = list(self.Xtrain.keys())
        id_test = list(self.Xtest.keys())
        self.Xtrain, self.Ytrain = self._build_subset_strains(id_train)
        self.Xtest, self.Ytest = self._build_subset_strains(id_test)
    
    def get_xtrain_array(self, length=None):
        &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the train subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;train_array&#39;. If None, the longest signal
            determines the length.

        Returns
        -------
        train_array : np.ndarray
            train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.

        &#34;&#34;&#34;
        return dictools.dict_to_stacked_array(self.Xtrain, target_length=length)
    
    def get_xtest_array(self, length=None):
        &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the test subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional

        Returns
        -------
        test_array : np.ndarray
            test subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;test_array&#39;.

        &#34;&#34;&#34;
        return dictools.dict_to_stacked_array(self.Xtest, target_length=length)
    
    def get_ytrain_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered training labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.

        with_id : bool
            If True, return also the list of related IDs.

        with_index : bool
            If True, return also the related GLOBAL indices; w.r.t. the stacked
            arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
            False by default.

        Returns
        -------
        np.ndarray
            Filtered train labels.

        np.ndarray, optional
            IDs associated to the filtered train labels.
        
        np.ndarray, optional
            Indices associated to the filtered train labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytrain, list(self.Xtrain), classes,
            with_id=with_id, with_index=with_index
        )

    def get_ytest_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered test labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.

        with_id : bool
            If True, return also the list of related IDs.

        with_index : bool
            If True, return also the related GLOBAL indices; w.r.t. the stacked
            arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

        Returns
        -------
        np.ndarray
            Filtered test labels.

        np.ndarray, optional
            IDs associated to the filtered test labels.
        
        np.ndarray, optional
            Indices associated to the filtered test labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytest, list(self.Xtest), classes,
            with_id=with_id, with_index=with_index
        )
    
    def _filter_labels(self, labels, labels_id, classes, with_id=False, with_index=False):
        &#34;&#34;&#34;Filter labels based on &#39;classes&#39;.

        This is a helper function for &#39;get_ytrain_array&#39; and &#39;get_ytest_array&#39;.
        
        Parameters
        ----------
        labels : np.ndarray
            The array containing the labels.
        
        labels_id : list
            IDs associated to the labels.
        
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related indices w.r.t. the stacked array
            returned by &#39;_stack_subset&#39; given the strains related to &#39;labels&#39;
            WITHOUT filters.
            False by default.

        Returns
        -------
        filtered_labels : np.ndarray
            Filtered labels.

        filtered_ids : np.ndarray, optional
            IDs associated to the filtered labels.

        filtered_indices : np.ndarray, optional
            Indices associated to the filtered labels.

        &#34;&#34;&#34;
        if len(labels) != len(labels_id):
            raise ValueError(&#34;&#39;labels&#39; and &#39;labels_id&#39; must have the same length.&#34;)

        if isinstance(classes, str):
            if classes == &#39;all&#39;:
                return labels
            else:
                classes = [classes]
        elif not isinstance(classes, list):
            raise TypeError(&#34;&#39;classes&#39; must be a string or list of strings.&#34;)
        
        filtered_labels = []
        filtered_ids = []
        filtered_indices = []
        
        i = 0
        for label, id in zip(labels, labels_id):
            if self.find_class(id) in classes:
                filtered_labels.append(label)
                filtered_ids.append(id)
                filtered_indices.append(i)
                i += 1  # Indices w.r.t. the FILTERED set!!!
        
        filtered_labels = np.array(filtered_labels)
        filtered_ids = np.array(filtered_ids)
        filtered_indices = np.array(filtered_indices)
        
        if with_id and with_index:
            return filtered_labels, filtered_ids, filtered_indices
        if with_id:
            return filtered_labels, filtered_ids
        if with_index:
            return filtered_labels, filtered_indices
        return filtered_labels
        
    def stack_by_id(self, id_list: list, length: int = None):
        &#34;&#34;&#34;Stack an subset of strains by their ID into a Numpy array.

        Stack an arbitrary selection of strains by their original ID into a
        zero-padded 2d-array. The resulting order is the same as the order of
        that in &#39;id_list&#39;.

        Parameters
        ----------
        id_list : list
            The IDs of the strains to be stacked.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        Notes
        -----
        - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
          not filter by &#39;classes&#39; since it would be redundant, as IDs are
          unique.

        &#34;&#34;&#34;
        if not isinstance(id_list, list):
            raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)

        # Collapse the Class layer.
        strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

        # Filter out those not in the &#39;id_list&#39;.
        strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)
        assert len(strains) == len(id_list)

        # Sort them to match the order in &#39;id_list&#39;.
        strains = {id: strains[id] for id in id_list}

        strains = dictools.flatten_nested_dict(strains)
        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        return stacked_signals, lengths

        


class BaseInjected(Base):
    &#34;&#34;&#34;Manage an injected dataset with multiple SNR values.

    It is designed to store strains as nested dictionaries, with each level&#39;s
    key identifying a class/property of the strain. Each individual strain is a
    1D NDArray containing the features.

    NOTE: Instances of this class or any other Class(BaseInjected) are
    initialized from an instance of any Class(Base) instance (clean dataset).
    
    By default there are THREE basic levels:
        
        - Class; to group up strains in categories.
        
        - Id; An unique identifier for each strain, which must exist in the
          metadata DataFrame as Index.
        
        - SNR; the signal-to-noise ratio at which has been injected w.r.t. a
          power spectral density of reference (e.g. the sensitivity of a GW
          detector).
    
    Extra depths can be added, and will be thought of as modifications of the
    same original strains from the upper identifier level. However they should
    be added between the &#39;Id&#39; and &#39;SNR&#39; layer, since the SNR is the final
    realization of any variations made of a given (Class, Id) signal.


    Attributes
    ----------
    classes : list[str]
        List of labels, one per class (category).
    
    metadata : pandas.DataFrame
        All parameters and data related to the original strains, inherited
        (copied) from a clean Class(Base) instance.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the ID).
        The total number of different waves must be equal to `len(metadata)`;
        this does not include possible variations such polarizations or
        multiple scallings of the same waveform when performing injections.
    
    strains_clean : dict[dict]
        Strains inherited (copied) from a clean Class(Base) instance.
        This copy is kept in order to perform new injections.
        
        - Shape: {class: {id: strain} }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        NOTE: These strains should be not modified. If new clean strains are
        needed, create a new clean dataset instance first, and then initialise
        this class with it.
    
    strains : dict[dict]
        Injected trains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        
        - Shape: {class: {id: {snr: strain} } }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        - Extra depths can be added as variations of each strain, such as
          polarizations. However they should be added between the &#39;id&#39; and
          the &#39;snr&#39; layer!
        
        - The &#39;snr&#39; key is an integer indicating the signal-to-noise ratio of
          the injection.
        
    labels : dict
        Indices of the class of each wave ID, inherited from a clean
        Class(Base) instance, with shape {id: class_index}.
        Each ID points to the index of its class in the &#39;classes&#39; attribute.
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.

    times : dict, optional
        Time samples associated with the strains, following the same structure.
        Useful when the sampling rate is variable or different between strains.
        If None, all strains are assumed to be constantly sampled to the
        sampling rate indicated by the &#39;sample_rate&#39; attribute.
    
    sample_rate : int
        Inherited from the parent Class(Base) instance.
    
    max_length : int
        Length of the longest strain in the dataset.
        Remember to update it if manually changing strains&#39; length.
    
    random_seed : int
        Value passed to &#39;sklearn.model_selection.train_test_split&#39; to generate
        the Train and Test subsets. Saved for reproducibility purposes.
        Also used to initialize Numpy&#39;s default RandomGenerator.

    rng : np.random.Generator
        Random number generator used for sampling the background noise.
        Initialized with `np.random.default_rng(random_seed)`.

    detector : str
        GW detector name.

    psd_ : NDArray
        Numerical representation of the Power Spectral Density (PSD) of the
        detector&#39;s sensitivity.
    
    asd_ : NDArray
        Numerical representation of the Amplitude Spectral Density (ASD) of the
        detector&#39;s sensitivity.

    noise : gwadama.ioo.NonwhiteGaussianNoise
        Background noise instance from NonwhiteGaussianNoise.

    snr_list : list

    pad : dict
        Padding introduced at each SNR injection, used in case the strains will
        be whitened after, to remove the vigneting at edges.
        It is associated to SNR values because the only implemented way to
        pad the signals is during the signal injection.
    
    whitened : bool
        Flat indicating whether the dataset has been whitened. Initially will
        be set to False, and changed to True after calling the &#39;whiten&#39; method.
        Once whitened, this flag will remain True, since the whitening is
        implemented to be irreversible instance-wise.
    
    whiten_params : dict
        TODO

        freq_cutoff : int | float
            Frequency cutoff below which no noise bins will be generated in the
            frequency space, and also used for the high-pass filter applied to
            clean signals before injection.

        freq_butter_order : int
            Butterworth filter order.
            See (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html)
            for more information.

    Xtrain, Xtest : dict, optional
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        Shape adds the SNR layer: {id: {snr: strain}}.
        The &#39;id&#39; corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
    
    Ytrain, Ytest : NDArray[int], optional
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.
        
        NOTE: Does not include the SNR layer, therefore labels are not repeated.

    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: Base,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 whiten_params: dict,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        &#34;&#34;&#34;Base constructor for injected datasets.

        TODO: Update docstring.

        When inheriting from this class, it is recommended to run this method
        first in your __init__ function.

        Relevant attributes are inherited from the &#39;clean_dataset&#39; instance,
        which can be any inherited from BaseDataset whose strains have not
        been injected yet.

        If train/test subsets are present, they too are updated when performing
        injections or changing units, but only through re-building them from
        the main &#39;strains&#39; attribute using the already generated indices.
        Original train/test subsets from the clean dataset are not inherited.
        
        WARNING: Initializing this class does not perform the injections! For
        that use the method &#39;gen_injections&#39;.

        Parameters
        ----------
        clean_dataset : Base
            Instance of a Class(Base) with noiseless signals.

        psd : np.ndarray | Callable
            Power Spectral Density of the detector&#39;s sensitivity in the range
            of frequencies of interest. Can be given as a callable function
            whose argument is expected to be an array of frequencies, or as a
            2d-array with shape (2, psd_length) so that
            
            ```
            psd[0] = frequency_samples
            psd[1] = psd_samples
            ```.
            
            NOTE: It is also used to compute the &#39;asd&#39; attribute (ASD).

        detector : str
            GW detector name.
            Not used, just for identification.

        noise_length : int
            Length of the background noise array to be generated for later use.
            It should be at least longer than the longest signal expected to be
            injected.
        
        whiten_params : dict
            Parameters of the whitening filter, with the following entries:
            
            - &#39;flength&#39; : int
                Length (in samples) of the time-domain FIR whitening.
            
            - &#39;highpass&#39; : float
                Frequency cutoff.
            
            - &#39;normed&#39; : bool
                Normalization applied after the whitening filter.

        freq_cutoff : int | float
            Frequency cutoff below which no noise bins will be generated in the
            frequency space, and also used for the high-pass filter applied to
            clean signals before injection.

        freq_butter_order : int | float
            Butterworth filter order.
            See (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html)
            for more information.
        
        flength : int
            Length (in samples) of the time-domain FIR whitening filter.

        random_seed : int
            Value passed to &#39;sklearn.model_selection.train_test_split&#39; to
            generate the Train and Test subsets.
            Saved for reproducibility purposes, and also used to initialize
            Numpy&#39;s default RandomGenerator.
        
        &#34;&#34;&#34;
        # Inherit clean strain instance attributes.
        #----------------------------------------------------------------------

        self.classes = clean_dataset.classes.copy()
        self._check_classes_dict(self.classes)
        self.labels = clean_dataset.labels.copy()
        self.metadata = deepcopy(clean_dataset.metadata)
        self.strains_clean = deepcopy(clean_dataset.strains)
        self._track_times = clean_dataset._track_times
        if self._track_times:
            self.times = deepcopy(clean_dataset.times)
        self.sample_rate = clean_dataset.sample_rate
        self.max_length = clean_dataset.max_length

        # Noise instance and related attributes.
        #----------------------------------------------------------------------

        self.random_seed = random_seed
        self.rng = np.random.default_rng(random_seed)
        self.detector = detector
        # Highpass parameters applied when generating the noise array.
        self.freq_cutoff = freq_cutoff
        self.freq_butter_order = freq_butter_order
    
        self._psd, self.psd_array = self._setup_psd(psd)
        self._asd, self.asd_array = self._setup_asd_from_psd(psd)
        self.noise = self._generate_background_noise(noise_length)

        # Injection related:
        #----------------------------------------------------------------------

        # TODO: Â¿Implement the case when clean_dataset is already whitened?
        # It should mark it and use the clean copy of nonwhitened data instead.

        self.strains = None
        self._dict_depth = clean_dataset._dict_depth + 1  # Depth of the strains dict.
        self.snr_list = []
        self.pad = {}  # {snr: pad}
        self.whitened = False  # Switched to True after calling self.whiten().
        self.whiten_params = whiten_params
        # NOTE: I designed this while building the InjectedCoReWaves class, so
        # chances are this is not general enough.
        self.whiten_params.update({
            &#39;asd_array&#39;: self.asd_array,  # Referenced here again for consistency.
            &#39;pad&#39;: 0,  # Signals are expected to be already padded.
            &#39;unpad&#39;: self.pad,  # Referenced here again for consistency.
            &#39;highpass&#39;: self.freq_cutoff  # Referenced here again for consistency.
        })

        # Train/Test subset views:
        #----------------------------------------------------------------------

        if clean_dataset.Xtrain is not None:
            self.Xtrain = {k: None for k in clean_dataset.Xtrain.keys()}
            self.Xtest = {k: None for k in clean_dataset.Xtest.keys()}
            self.Ytrain = clean_dataset.Ytrain
            self.Ytest = clean_dataset.Ytest
        else:
            self.Xtrain = None
            self.Xtest = None
            self.Ytrain = None
            self.Ytest = None
    
    def __getstate__(self):
        &#34;&#34;&#34;Avoid error when trying to pickle PSD and ASD interpolants.
        
        Turns out Pickle tries to serialize the PSD and ASD interpolants,
        however Pickle is not able to serialize encapsulated functions.
        This is solved by removing said functions and computing the
        interpolants from their array representations when unpickling.

        NOTE: The loss of accuracy over repeated (de)serialization using this
        method has not been studied, use at your own discretion.
        
        &#34;&#34;&#34;
        state = self.__dict__.copy()
        del state[&#39;_psd&#39;]
        del state[&#39;_asd&#39;]
        
        return state
    
    def __setstate__(self, state):
        &#34;&#34;&#34;Avoid error when trying to unpickle PSD and ASD interpolants.
        
        Turns out Pickle tries to serialize the PSD and ASD interpolants,
        however Pickle is not able to serialize encapsulated functions.
        This is solved by removing said functions and computing the
        interpolants from their array representations when unpickling.
        
        NOTE: The loss of accuracy over repeated (de)serialization using this
        method has not been studied, use at your own discretion.
        
        &#34;&#34;&#34;
        _psd, _ = self._setup_psd(state[&#39;psd_array&#39;])
        _asd, _ = self._setup_asd_from_psd(state[&#39;psd_array&#39;])
        state[&#39;_psd&#39;] = _psd
        state[&#39;_asd&#39;] = _asd
        self.__dict__.update(state)
    
    def _setup_psd(self, psd: np.ndarray | Callable) -&gt; tuple[Callable, np.ndarray]:
        &#34;&#34;&#34;Setup the PSD function or array depending on the input.
        
        Setup the power spectral density function and array from any of those.
        
        &#34;&#34;&#34;
        if callable(psd):
            psd_fun = psd
            # Compute a realization of the PSD function with 16 bins per
            # integer frequency to ensure the numerical representation has
            # enough precision.
            freqs = np.linspace(0, self.sample_rate//2, self.sample_rate*8)
            psd_array = np.stack([freqs, psd(freqs)])
        
        elif isinstance(psd, np.ndarray):
            # Build a spline quadratic interpolant for the input PSD array.
            psd_fun = sp_make_interp_spline(psd[0], psd[1], k=2)
            psd_array = np.asarray(psd)
        
        else:
            raise TypeError(&#34;&#39;psd&#39; type not recognized&#34;)
            
        return psd_fun, psd_array

    def _setup_asd_from_psd(self, psd):
        &#34;&#34;&#34;Setup the ASD function or array depending on the input.
        
        Setup the amplitude spectral density function and array from any of
        those.
        
        &#34;&#34;&#34;
        if callable(psd):
            asd_fun = lambda f: np.sqrt(psd)
            # Compute a realization of the ASD function with 16 bins per
            # integer frequency to ensure the numerical representation has
            # enough precision.
            freqs = np.linspace(0, self.sample_rate//2, self.sample_rate*8)
            asd_array = np.stack([freqs, asd_fun(freqs)])
        
        elif isinstance(psd, np.ndarray):
            # Build a spline quadratic interpolant for the input ASD array.
            asd_array = psd.copy()
            asd_array[1] = np.sqrt(psd[1])
            asd_fun = sp_make_interp_spline(asd_array[0], asd_array[1], k=2)
        
        else:
            raise TypeError(&#34;&#39;psd&#39; type not recognized&#34;)
            
        return asd_fun, asd_array

    def psd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
        &#34;&#34;&#34;Power spectral density (PSD) of the detector at given frequencies.

        Interpolates the PSD at the given frequencies from their array
        representation. If during initialization the PSD was given as its
        array representation, the interpolant is computed using SciPy&#39;s
        quadratic spline interpolant function.

        &#34;&#34;&#34;
        return self._psd(frequencies)

    def asd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
        &#34;&#34;&#34;Amplitude spectral density (ASD) of the detector at given frequencies.

        Interpolates the ASD at the given frequencies from their array
        representation. If during initialization the ASD was given as its
        array representation, the interpolant is computed using SciPy&#39;s
        quadratic spline interpolant function.

        &#34;&#34;&#34;
        return self._asd(frequencies)
    
    def _generate_background_noise(self, noise_length: int) -&gt; synthetic.NonwhiteGaussianNoise:
        &#34;&#34;&#34;The noise realization is generated by NonwhiteGaussianNoise.&#34;&#34;&#34;

        d: float = noise_length / self.sample_rate
        noise = synthetic.NonwhiteGaussianNoise(
            duration=d, psd=self.psd, sample_rate=self.sample_rate,
            rng=self.rng, freq_cutoff=self.freq_cutoff
        )

        return noise
    
    def _init_strains_dict(self) -&gt; dict[dict[dict]]:
        &#34;&#34;&#34;Initializes the nested dictionary of strains.
        
        Initializes the nested dictionary of strains following the hierarchy
        in the clean strains attribute, and adding the (lowest) SNR layer.
        
        &#34;&#34;&#34;
        strains_dict = dictools._replicate_structure_nested_dict(self.strains_clean)
        for indices in dictools._unroll_nested_dictionary_keys(strains_dict):
            dictools.set_value_to_nested_dict(strains_dict, indices, {})

        return strains_dict
    
    def get_times(self, *indices) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single time array from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;times&#39; attribute.
        
        &#34;&#34;&#34;
        return dictools._get_value_from_nested_dict(self.times, indices)
    
    def gen_injections(self, snr: int|float|list, pad: int = 0):
        &#34;&#34;&#34;Inject all strains in simulated noise with the given SNR values.

        
        - The SNR is computed using a matched filter against the noise PSD.
        
        - If `pad &gt; 0`, it also updates the time arrays.
        
        - If strain units are in geometrized, they will be converted first to
          IS, injected, and converted back to geometrized.
        
        - After each injection, applies a highpass filter at the low-cut
          frequency specified at __init__.
        
        - If the method &#39;whiten&#39; has been already called, all further
          injections will automatically be whitened and their pad removed.
        
        Parameters
        ----------
        snr : int | float | list
        
        pad : int
            Number of zeros to pad the signal at both ends before the
            injection.
        
        Notes
        -----
        - If whitening is intended to be applied afterwards it is useful to
          pad the signal in order to avoid the window vignetting produced by
          the whitening itself. This pad will be cropped afterwards.
        
        - New injections are stored at the &#39;strains&#39; atrribute, with the pad
          associated to all the injections performed at once. Even when
          whitening is also performed right after the injections.
        
        Raises
        ------
        ValueError
            Once injections have been performed at a certain SNR value, there
            cannot be injected again at the same value. Trying it will trigger
            this exception.
        
        &#34;&#34;&#34;
        if isinstance(snr, (int, float)):
            snr_list = list(snr)
        elif isinstance(snr, list):
            snr_list = snr
        else:
            raise TypeError(f&#34;&#39;{type(snr)}&#39; is not a valid &#39;snr&#39; type&#34;)
        
        if set(snr_list) &amp; set(self.snr_list):
            raise ValueError(&#34;one or more SNR values are already present in the dataset&#34;)

        if self._track_times:
            # Replaced temporarily because when injecting for the first time
            # we need to keep the original time arrays.
            times_new = self.times

        # 1st time making injections.
        if self.strains is None:
            self.strains = self._init_strains_dict()
            if self._track_times:
                # Redo the dictionary structure to include the SNR layer.
                times_new = self._init_strains_dict()
        
        for clas, id_ in dictools._unroll_nested_dictionary_keys(self.strains_clean):
            gw_clean = self.strains_clean[clas][id_]
            strain_clean_padded = np.pad(gw_clean, pad)
            # NOTE: Do not update the metadata nor times with this pad in case
            # the whitening is applied immediately after the injections.

            # Highpass filter to the clean signal.
            # NOTE: The noise realization is already generated without
            # frequency components lower than the cutoff (they are set to
            # 0 during the random sampling).
            strain_clean_padded = self.noise.highpass_filter(
                strain_clean_padded, f_cut=self.freq_cutoff, f_order=self.freq_butter_order
            )

            
            # Strain injections
            for snr_ in snr_list:
                # &#39;pad&#39; is added to &#39;snr_offset&#39; to compensate for the padding
                # which has not been updated in the &#39;metadata&#39; yet.
                injected = self._inject(
                    strain_clean_padded, snr_, id=id_, snr_offset=pad
                )
                if self.whitened:
                    injected = fat.whiten(
                        injected, asd=self.asd_array, unpad=pad, sample_rate=self.sample_rate,
                        # Parameters for GWpy&#39;s whiten() function:
                        highpass=self.freq_cutoff, flength=self.flength
                    )
                self.strains[clas][id_][snr_] = injected
            
            # Time arrays:
            # - All SNR entries pointing to the SAME time array.
            # - Enlarge if the strains were padded and no whitening followed.
            if self._track_times:
                times_i = self.get_times(clas, id_)
                if pad &gt; 0 and not self.whitened:
                    times_i = tat.pad_time_array(times_i, pad)
                for snr_ in snr_list:
                    times_new[clas][id_][snr_] = times_i
        
        if self._track_times:
            self.times = times_new
        
        # Record new SNR values and related padding.
        # NOTE: Even if whitening is applied (and hence the length unaltered)
        # pad values are still registered, just in case.
        self.snr_list += snr_list
        for snr_ in snr_list:
            self.pad[snr_] = pad
        
        # Side-effect attributes updated.
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def _inject(self, strain: np.ndarray, snr: int|float, **_) -&gt; np.ndarray:
        &#34;&#34;&#34;Inject &#39;strain&#39; at &#39;snr&#39; into noise using the &#39;self.noise&#39; instance.

        NOTE: This is writen as an independent method to allow for other
        classes inheriting this to modify its behaviour without having to
        rewrite the entire &#39;gen_injections&#39; method.

        Parameters
        ----------
        strain : NDArray
            Signal to be injected into noise.
        
        snr : int | float
            Signal to noise ratio.
        
        Returns
        -------
        injected : NDArray
            Injected signal.
        
        &#34;&#34;&#34;
        injected, _ = self.noise.inject(strain, snr=snr)

        return injected
    
    def export_strains_to_gwf(self,
                              path: str,
                              channel: str,  # Name of the channel in which to save strains in the GWFs.
                              t0_gps: float = 0,
                              verbose=False) -&gt; None:
        &#34;&#34;&#34;Export all strains to GWF format, one file per strain.&#34;&#34;&#34;

        from pathlib import Path

        for indices in self.keys():
            strain = self.get_strain(*indices)
            times = self.get_times(*indices)
            ts = TimeSeries(
                data=strain,
                times=t0_gps + times,
                channel=channel
            )
            key = indices[1].replace(&#39;:&#39;, &#39;_&#39;) + &#39;_snr&#39; + str(indices[2])
            fields = [
                self.detector,
                key,
                str(int(t0_gps)),
                str(int(ts.duration.value * 1000))  # In milliseconds
            ]
            file = Path(path) / (&#39;-&#39;.join(fields) + &#39;.gwf&#39;)
            ts.write(file)

            if verbose:
                print(&#34;Strain exported to&#34;, file)
    
    def whiten(self):
        &#34;&#34;&#34;Whiten injected strains.
        
        Calling this method performs the whitening of all injected strains.
        Strains are later cut to their original size before adding the pad,
        to remove the vigneting.
        
        NOTE: This is an irreversible action; if the original injections need
        to be preserved it is advised to make a copy of the instance before
        performing the whitening.
        
        &#34;&#34;&#34;
        if self.whitened:
            raise RuntimeError(&#34;dataset already whitened&#34;)

        if self.strains is None:
            raise RuntimeError(&#34;no injections have been performed yet&#34;)

        flength = self.whiten_params[&#39;flength&#39;]
        asd_array = self.whiten_params[&#39;asd_array&#39;]
        pad = self.whiten_params[&#39;pad&#39;]
        unpad = self.whiten_params[&#39;unpad&#39;]
        highpass = self.whiten_params[&#39;highpass&#39;]
        
        for *keys, strain in self.items():
            snr = keys[-1]  # Shape of self.strains dict-&gt; {class: {id: {snr: strain}}}

            # NOTE: I designed this while building the InjectedCoReWaves class,
            # so chances are the parameters here are not passed in the most
            # generalized way.
            strain_w = fat.whiten(
                strain, asd=asd_array, pad=pad, unpad=unpad[snr], sample_rate=self.sample_rate,
                highpass=highpass, flength=flength
            )
            # Update strains attribute.
            dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
        
        # Shrink time arrays accordingly.
        if self._track_times:
            key_layers = dictools._unroll_nested_dictionary_keys(
                self.strains,
                max_depth=self._dict_depth-1  # same signal at different SNR has same time points.
            )
            for keys in key_layers:
                # Since all time arrays below SNR layer are the same, get the first one:
                times_i = dictools._get_value_from_nested_dict(self.times, keys)
                snr0 = next(iter(times_i.keys()))
                times_i = times_i[snr0]
                times_i = tat.shrink_time_array(times_i, unpad[snr0])
                for snr in self.snr_list:
                    keys_all = keys + [snr]
                    dictools.set_value_to_nested_dict(self.times, keys_all, times_i)
        
        self.whitened = True
        
        # Side-effect attributes updated.
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def get_xtrain_array(self,
                         length: int = None,
                         classes: str | list = &#39;all&#39;,
                         snr: int | list | str = &#39;all&#39;,
                         with_metadata: bool = False):
        &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the train subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Allows the possibility to filter by class and SNR.

        NOTE: Same signals injected at different SNR are stacked continuously.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;train_array&#39;. If None, the longest signal
            determines the length.

        classes : str | list[str]
            Classes which to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            SNR injections which to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                 ...
            ```
            
            All injections are included by default.
        
        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            train array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with an
            additional column for the SNR values.
            False by default.

        Returns
        -------
        train_array : np.ndarray
            Train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.
        
        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;train_array&#39;.

        &#34;&#34;&#34;
        return self._stack_subset(self.Xtrain, length, classes, snr, with_metadata)
    
    def get_xtest_array(self,
                        length: int = None,
                        classes: str | list = &#39;all&#39;,
                        snr: int | list | str = &#39;all&#39;,
                        with_metadata: bool = False):
        &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the test subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Allows the possibility to filter by class and SNR.

        NOTE: Same signals injected at different SNR are stacked continuously.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;test_array&#39;. If None, the longest signal
            determines the length.

        classes : str | list[str]
            Classes which to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            SNR injections which to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                 ...
            ```
            
            All injections are included by default.

        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            test array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with an
            additional column for the SNR values.
            False by default.

        Returns
        -------
        test_array : np.ndarray
            Test subset.

        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;test_array&#39;.

        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;test_array&#39;.
        
        &#34;&#34;&#34;
        return self._stack_subset(self.Xtest, length, classes, snr, with_metadata)

    def _stack_subset(self,
                      strains: dict,
                      length:  int = None,
                      classes: str | list = &#39;all&#39;,
                      snr: int | list | str = &#39;all&#39;,
                      with_metadata: bool = False):
        &#34;&#34;&#34;Stack a subset of strains into a zero-padded 2d-array.

        This is a helper function for &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;.

        Parameters
        ----------
        strains : dict
            A dictionary containing the strains to be stacked.
            The keys of the first layer are the IDs of the strains.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        classes : str | list[str]
            The classes to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            The SNR injections to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                ...
            ```
            
            All injections are included by default.

        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            stacked array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with
            an additional column for the SNR values.
            False by default.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;stacked_signals&#39;.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with
            an additional column for the SNR values.

        Raises
        ------
        ValueError
            If the value of &#39;classes&#39; or &#39;snr&#39; is not valid.

        &#34;&#34;&#34;
        if isinstance(classes, (str, list)) and classes != &#39;all&#39;:
            if isinstance(classes, str):
                classes = [classes]

            # NOTE: Here there is no &#39;class&#39; layer, therefore it must be
            # traced back from the ID, and filtered over this same layer.
            def filter_class(id):
                clas = self.find_class(id)
                return clas in classes
            strains = dictools.filter_nested_dict(strains, filter_class, layer=0)

        elif classes != &#39;all&#39;:
            raise TypeError(&#34;the type of &#39;classes&#39; is not valid&#34;)


        if isinstance(snr, (int, list)):
            if isinstance(snr, int):
                snr = [snr]

            # NOTE: Here SNR is in Layer 1 because the Train/Test subset
            # dictionaries do not have the &#39;class&#39; first layer.
            strains = dictools.filter_nested_dict(strains, lambda k: k in snr, layer=1)

        # If `snr == &#39;all&#39;`, no filter is applied over &#39;strains&#39;.
        elif isinstance(snr, str) and snr != &#39;all&#39;:
            raise ValueError(&#34;the value of &#39;snr&#39; is not valid&#34;)
        
        else:
            raise TypeError(&#34;the type of &#39;snr&#39; is not valid&#34;)
        

        strains = dictools.flatten_nested_dict(strains)

        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        if with_metadata:
            id_list = [k[0] for k in strains]
            snr_list = [k[1] for k in strains]
            metadata = self.metadata.loc[id_list]
            metadata.reset_index(inplace=True, names=&#39;id&#39;)
            metadata.insert(1, &#39;snr&#39;, snr_list)  # after &#39;id&#39;.
            
            return stacked_signals, lengths, metadata
        return stacked_signals, lengths

    def get_ytrain_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered training labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related GLOBAL indices w.r.t. the stacked
            arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
            False by default.

        Returns
        -------
        np.ndarray
            Filtered train labels.

        np.ndarray, optional
            IDs associated to the filtered train labels.
        
        np.ndarray, optional
            Indices associated to the filtered train labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytrain, list(self.Xtrain), classes, snr,
            with_id=with_id, with_index=with_index
        )

    def get_ytest_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered test labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related GLOBAL indices w.r.t. the stacked
            arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

        Returns
        -------
        np.ndarray
            Filtered test labels.

        np.ndarray, optional
            IDs associated to the filtered test labels.
        
        np.ndarray, optional
            Indices associated to the filtered test labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytest, list(self.Xtest), classes, snr,
            with_id=with_id, with_index=with_index
        )
    
    def _filter_labels(self, labels, labels_id, classes, snr, with_id=False, with_index=False):
        &#34;&#34;&#34;Filter labels based on &#39;classes&#39; and &#39;snr&#39;.

        This is a helper function for &#39;get_ytrain_array&#39; and &#39;get_ytest_array&#39;.
        
        Parameters
        ----------
        labels : np.ndarray
            The array containing the labels.

        labels_id : list
            IDs associated to the labels.
        
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related indices w.r.t. the stacked array
            returned by &#39;_stack_subset&#39; given the strains related to &#39;labels&#39;
            WITHOUT filters.
            False by default.

        Returns
        -------
        filtered_labels : np.ndarray
            Filtered labels.

        filtered_ids : np.ndarray, optional
            IDs associated to the filtered labels.

        filtered_indices : np.ndarray, optional
            Indices associated to the filtered labels.

        &#34;&#34;&#34;
        # First get labels and IDs filtered by &#39;classes&#39;.
        filtered_labels, filtered_ids, filtered_indices = super()._filter_labels(
            labels, labels_id, classes, with_id=True, with_index=True
        )

        if isinstance(snr, str):
            if snr != &#39;all&#39;:
                raise ValueError(&#34;only the str &#39;all&#39; is allowed for &#39;snr&#39;.&#34;)
        elif isinstance(snr, int):
            snr = [snr]
        elif not isinstance(snr, list):
            raise TypeError(&#34;the type of &#39;snr&#39; is not valid&#34;)
        
        n_snr_total = len(self.snr_list)

        # Next repeat all by the total number of SNR values.
        filtered_labels = np.repeat(filtered_labels, n_snr_total)
        filtered_ids = np.repeat(filtered_ids, n_snr_total)
        filtered_indices = np.repeat(filtered_indices, n_snr_total)

        n_filtered = len(filtered_labels)

        # Then convert the indices to include the TOTAL number of SNR repetitions.
        for i in range(0, n_filtered, n_snr_total):
            i_old = filtered_indices[i]
            i_new0 = i_old * n_snr_total
            i_new1 = i_new0 + n_snr_total
            filtered_indices[i:i+n_snr_total] = np.arange(i_new0, i_new1)

        if snr != &#39;all&#39;:
            # Finally filter out those not present in the &#39;snr&#39; list.
            mask = np.isin(self.snr_list, snr)
            mask = np.tile(mask, n_filtered//n_snr_total)
            filtered_labels = filtered_labels[mask]
            filtered_ids = filtered_ids[mask]
            filtered_indices = filtered_indices[mask]

        if with_id and with_index:
            return filtered_labels, filtered_ids, filtered_indices
        if with_id:
            return filtered_labels, filtered_ids
        if with_index:
            return filtered_labels, filtered_indices
        return filtered_labels

    def stack_by_id(self,
                    id_list: list,
                    length: int = None,
                    snr_included: int | list[int] | str = &#39;all&#39;):
        &#34;&#34;&#34;Stack a subset of strains by ID into a zero-padded 2d-array.

        This may allow (for example) to group up strains by their original ID
        without leaking differnet injections (SNR) of the same strain into
        different splits.

        Parameters
        ----------
        id_list : array-like
            The IDs of the strains to be stacked.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        snr_included : int | list[int] | str, optional
            The SNR injections to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            id0 snr0
            id0 snr1
              ...
            ```
            
            All injections are included by default.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        Notes
        -----
        - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
          not filter by &#39;classes&#39; since it would be redundant, as IDs are
          unique.

        Raises
        ------
        ValueError
            If the value of &#39;snr&#39; is not valid.

        &#34;&#34;&#34;
        if not isinstance(id_list, list):
            raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)
        
        # Collapse the Class layer.
        strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

        # Filter out those not in the &#39;id_list&#39;.
        strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)

        # Filter out those injections whose SNR isnot in the &#39;snr&#39; list.
        if isinstance(snr_included, (int, list)):
            if isinstance(snr_included, int):
                snr_included = [snr_included]
            # NOTE: Here SNR is in Layer 1 because we collapsed the Class layer.
            strains = dictools.filter_nested_dict(strains, lambda k: k in snr_included, layer=1)
        elif snr_included != &#39;all&#39;:
            raise ValueError(&#34;the value of &#39;snr&#39; is not valid&#34;)

        strains = dictools.flatten_nested_dict(strains)  # keys: &#34;(id, snr)&#34;
        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        return stacked_signals, lengths
        


class SyntheticWaves(Base):
    &#34;&#34;&#34;Class for building synthetically generated wavforms and background noise.

    Part of the datasets for the CLAWDIA main paper.
    
    The classes are hardcoded:
        
        SG: Sine Gaussian,
        
        G:  Gaussian,
        
        RD: Ring-Down.


    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).
    
    strains : dict {class: {key: gw_strains} }
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        The class key is the name of the class, a string which must exist in
        the &#39;classes&#39; attribute.
        The &#39;key&#39; is an identifier of each strain.
        In this case it&#39;s just the global index ranging from 0 to &#39;self.n_samples&#39;.
    
    labels : NDArray[int]
        Indices of the classes, one per waveform.
        Each one points its respective waveform inside &#39;strains&#39; to its class
        in &#39;classes&#39;. The order is that of the index of &#39;self.metadata&#39;, and
        coincides with the order of the strains inside &#39;self.strains&#39; if
        unrolled to a flat list of arrays.
    
    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains.
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.
    
    Xtrain, Xtest : dict {key: strain}
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        The key corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
    
    Ytrain, Ytest : NDArray[int]
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.

    &#34;&#34;&#34;

    def __init__(self,
                 *,
                 classes: dict,
                 n_waves_per_class: int,
                 wave_parameters_limits: dict,
                 max_length: int,
                 peak_time_max_length: float,
                 amp_threshold: float,
                 tukey_alpha: float,
                 sample_rate: int,
                 train_size: int | float, 
                 random_seed: int = None):
        &#34;&#34;&#34;
        Parameters
        ----------
        n_waves_per_class : int
            Number of waves per class to produce.

        wave_parameters_limits : dict
            Min/Max limits of the waveforms&#39; parameters, 9 in total.
            Keys:
            
            - mf0,   Mf0:   min/Max central frequency (SG and RD).
            
            - mQ,    MQ:    min/Max quality factor (SG and RD).
            
            - mhrss, Mhrss: min/Max sum squared amplitude of the wave.
            
            - mT,    MT:    min/Max duration (only G).
        
        max_length : int
            Maximum length of the waves. This parameter is used to generate the
            initial time array with which the waveforms are computed.
        
        peak_time_max_length : float
            Time of the peak of the envelope of the waves in the initial time
            array (built with &#39;max_length&#39;).
        
        amp_threshold : float
            Fraction w.r.t. the maximum absolute amplitude of the wave envelope
            below which to end the wave by shrinking the array and applying a
            windowing to the edges.
        
        tukey_alpha : float
            Alpha parameter (width) of the Tukey window applied to each wave to
            make sure their values end at the exact duration determined by either
            the duration parameter or the amplitude threshold.
        
        train_size : int | float
            If int, total number of samples to include in the train dataset.
            If float, fraction of the total samples to include in the train
            dataset.
            For more details see &#39;sklearn.model_selection.train_test_split&#39;
            with the flag `stratified=True`.
        
        sample_rate : int
        
        random_seed : int, optional.
        
        &#34;&#34;&#34;
        self._check_classes_dict(self.classes)
        self.classes = classes
        self.n_waves_per_class = n_waves_per_class
        self.sample_rate = sample_rate
        self.wave_parameters_limits = wave_parameters_limits
        self.max_length = max_length
        self.peak_time_max_length = peak_time_max_length
        self.tukey_alpha = tukey_alpha
        self.amp_threshold = amp_threshold
        self.random_seed = random_seed
        self.rng = np.random.default_rng(random_seed)

        self._gen_metadata()
        self._track_times = False
        self._gen_dataset()
        self.labels = self._gen_labels()
        self.build_train_test_subsets(train_size)

    def _gen_metadata(self):
        &#34;&#34;&#34;Generate random metadata associated with each waveform.&#34;&#34;&#34;

        classes_list = []
        f0s_list = []
        Q_list = []
        hrss_list = []
        duration_list = []  # Will be modified afterwards to take into account
                            # the amplitude threshold.
        for clas in self.classes:
            for _ in range(self.n_waves_per_class):
                # Need to pass &#39;self&#39; explicitely since I&#39;m calling the methods
                # inside a dictionary attribute. Python doesn&#39;t seem to
                # recognise them as the same class methods this way.
                f0, Q, hrss, duration = self._gen_parameters[clas](self)
                classes_list.append(clas)
                f0s_list.append(f0)
                Q_list.append(Q)
                hrss_list.append(hrss)
                duration_list.append(duration)  

        self.metadata = pd.DataFrame({
            &#39;Class&#39;: classes_list,  # strings
            &#39;f0&#39;: f0s_list,
            &#39;Q&#39;: Q_list,
            &#39;hrss&#39;: hrss_list,
            &#39;duration&#39;: duration_list
        })

    def _gen_dataset(self):
        &#34;&#34;&#34;Generate the dataset from the previously generated metadata.

        After generating the waveforms with the analytical expressions it
        shrinks them to the specified duration in the metadata. This is
        necessary because the analytical expressions are infinite, so we apply
        a window to get perfect edges. However this does not necessary align
        with the exact duration provided by the metadata due to the signals
        being sampled at discrete values. Therefore after the windowing the
        final duration is computed again and updated in the metadata attribute.
        
        Attributes
        ----------
        strains : dict[dict]
            Creates the strains attribute with the properties stated at the
            class&#39; docstring.
        
        _dict_depth : int
            Number of nested layers in strains&#39; dictionary.
        
        metadata : pd.DataFrame
            Updates the duration of the waveforms after shrinking them.

        &#34;&#34;&#34;
        if self.metadata is None:
            raise AttributeError(&#34;&#39;metadata&#39; needs to be generated first!&#34;)

        self.strains = self._init_strains_dict()

        t_max = (self.max_length - 1) / self.sample_rate
        times = np.linspace(0, t_max, self.max_length)
        
        for i in range(len(self)):
            params = self.metadata.loc[i].to_dict()
            clas = params[&#39;Class&#39;]
            match clas:
                case &#39;SG&#39;:
                    self.strains[clas][i] = synthetic.sine_gaussian_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        f0=self.metadata.at[i,&#39;f0&#39;],
                        Q=self.metadata.at[i,&#39;Q&#39;],
                        hrss=self.metadata.at[i,&#39;hrss&#39;]
                    )
                case &#39;G&#39;:
                    self.strains[clas][i] = synthetic.gaussian_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        hrss=self.metadata.at[i,&#39;hrss&#39;],
                        duration=self.metadata.at[i,&#39;duration&#39;],
                        amp_threshold=self.amp_threshold
                    )
                case &#39;RD&#39;:
                    self.strains[clas][i] = synthetic.ring_down_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        f0=self.metadata.at[i,&#39;f0&#39;],
                        Q=self.metadata.at[i,&#39;Q&#39;],
                        hrss=self.metadata.at[i,&#39;hrss&#39;]
                    )
        
        self._dict_depth = dictools.get_depth(self.strains)

        self._apply_threshold_windowing()
    
    def _random_log_uniform(self, min, max):
        &#34;&#34;&#34;Returns a random number between [min, max] spaced logarithmically.&#34;&#34;&#34;

        exponent = self.rng.uniform(np.log10(min), np.log10(max))
        random = 10**exponent

        return random
    
    def _random_log_int(self, min, max):
        &#34;&#34;&#34;Returns a random integer between [min, max] spaced logarithmically.&#34;&#34;&#34;

        return int(self._random_log_uniform(min, max))

    
    def _gen_parameters_sine_gaussian(self):
        &#34;&#34;&#34;Generate random parameters for a single Sine Gaussian.&#34;&#34;&#34;

        limits = self.wave_parameters_limits
        thres = self.amp_threshold
        f0   = self._random_log_int(limits[&#39;mf0&#39;], limits[&#39;Mf0&#39;])  # Central frequency
        Q    = self._random_log_int(limits[&#39;mQ&#39;], limits[&#39;MQ&#39;]+1)  # Quality factor
        hrss = self._random_log_uniform(limits[&#39;mhrss&#39;], limits[&#39;Mhrss&#39;])
        duration = 2 * Q / (np.pi * f0) * np.sqrt(-np.log(thres))
        
        return (f0, Q, hrss, duration)

    def _gen_parameters_gaussian(self):
        &#34;&#34;&#34;Generate random parameters for a single Gaussian.&#34;&#34;&#34;

        lims = self.wave_parameters_limits
        f0   = None  #  Casted to np.nan afterwards.
        Q    = None  #-/
        hrss = self._random_log_uniform(lims[&#39;mhrss&#39;], lims[&#39;Mhrss&#39;])
        duration = self._random_log_uniform(lims[&#39;mT&#39;], lims[&#39;MT&#39;])  # Duration
        
        return (f0, Q, hrss, duration)

    def _gen_parameters_ring_down(self):
        &#34;&#34;&#34;Generate random parameters for a single Ring-Down.&#34;&#34;&#34;

        lims = self.wave_parameters_limits
        thres = self.amp_threshold
        f0   = self._random_log_int(lims[&#39;mf0&#39;], lims[&#39;Mf0&#39;])  # Central frequency
        Q    = self._random_log_int(lims[&#39;mQ&#39;], lims[&#39;MQ&#39;]+1)  # Quality factor
        hrss = self._random_log_uniform(lims[&#39;mhrss&#39;], lims[&#39;Mhrss&#39;])
        duration = -np.sqrt(2) * Q / (np.pi * f0) * np.log(thres)
        
        return (f0, Q, hrss, duration)

    _gen_parameters = {
        &#39;SG&#39;: _gen_parameters_sine_gaussian,
        &#39;G&#39;: _gen_parameters_gaussian,
        &#39;RD&#39;: _gen_parameters_ring_down
    }

    def _apply_threshold_windowing(self):
        &#34;&#34;&#34;Shrink waves in the dataset and update its duration in the metadata.

        Shrink them according to their pre-computed duration in the metadata to
        avoid almost-but-not-zero edges, and correct those marginal durations
        longer than the window.

        &#34;&#34;&#34;
        for i in range(len(self)):
            clas = self.metadata.at[i,&#39;Class&#39;]
            duration = self.metadata.at[i,&#39;duration&#39;]
            ref_length = int(duration * self.sample_rate)
            
            if clas == &#39;RD&#39;:
                # Ring-Down waves begin at the center. However we want to
                # emphasize their energetic beginning, therefore we will leave
                # a symmetric part before their start with zeros.
                i0 = self.max_length // 2 - ref_length
                i1 = i0 + 2*ref_length
            else:
                # SG and G are both centered.
                i0 = (self.max_length - ref_length) // 2
                i1 = self.max_length - i0

            new_lenght = i1 - i0
            if i0 &lt; 0:
                new_lenght += i0
                i0 = 0
            if i1 &gt; self.max_length:
                new_lenght -= i1 - self.max_length
                i1 = self.max_length

            window = sp.signal.windows.tukey(new_lenght, alpha=self.tukey_alpha)
            # Shrink and window
            self.strains[clas][i] = self.strains[clas][i][i0:i1] * window

            self.metadata.at[i,&#39;duration&#39;] = new_lenght / self.sample_rate


class InjectedSyntheticWaves(BaseInjected):
    &#34;&#34;&#34;TODO
    
    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: SyntheticWaves,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        super().__init__(
            clean_dataset, psd=psd, detector=detector, noise_length=noise_length,
            freq_cutoff=freq_cutoff, freq_butter_order=freq_butter_order, random_seed=random_seed
        )

        # Initialize the Train/Test subsets inheriting the indices of the input
        # clean dataset instance.
        self.Xtrain = dictools._replicate_structure_nested_dict(clean_dataset.Xtrain)
        self.Xtest = dictools._replicate_structure_nested_dict(clean_dataset.Xtest)
        self.Ytrain = dictools._replicate_structure_nested_dict(clean_dataset.Ytrain)
        self.Ytest = dictools._replicate_structure_nested_dict(clean_dataset.Ytest)


class CoReWaves(Base):
    &#34;&#34;&#34;Manage all operations needed to perform over a noiseless CoRe dataset.

    Initial strains and metadata are obtained from a CoReManager instance.

    NOTE: This class treats as different classes (categories) each equation of
    state (EOS) present in the CoReManager instance.

    NOTE^2: This class adds a time attribute with time samples related to each
    GW.

    Workflow:
    
    - Load the strains from a CoreWaEasy instance, discarding or cropping those
      indicated with their respective arguments.
    
    - Resample.
    
    - Project onto the ET detector arms.
    
    - Change units and scale from geometrized to IS and vice versa.
    
    - Export the (latest version of) dataset to a HDF5.
    
    - Export the (latest version of) dataset to a GWF.

    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).
        The keys are the name of the Equation of State (EOS) used to describe
        the physics behind the simulation which produced each strain.
    
    strains : dict {class: {id: gw_strain} }
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        The class key is the name of the class, a string which must exist in
        the &#39;classes&#39; list.
        The &#39;id&#39; is an unique identifier for each strain, and must exist in the
        `self.metadata.index` column of the metadata DataFrame.
        Initially, an extra depth layer is defined to store the polarizations
        of the CoRe GW simulated data. After the projection this layer will be
        collapsed to a single strain.
    
    times : dict {class: {id: gw_time_points} }
        Time samples associated with the strains, following the same structure.
        Useful when the sampling rate is variable or different between strains.
    
    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the id.).
        Example:
        
        ```
        metadata[eos][key] = {
            &#39;id&#39;: str,
            &#39;mass&#39;: float,
            &#39;mass_ratio&#39;: float,
            &#39;eccentricity&#39;: float,
            &#39;mass_starA&#39;: float,
            &#39;mass_starB&#39;: float,
            &#39;spin_starA&#39;: float,
            &#39;spin_starB&#39;: float
        }
        ```
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.
    
    sample_rate : int, optional
        Initially this attribute is None because the initial GW from CoRe are
        sampled at different and non-constant sampling rates. After the
        resampling, this attribute will be set to the new global sampling rate.

        Caveat: If the &#39;times&#39; attribute is present, this value is ignored.
        Otherwise it is assumed all strains are constantly sampled to this.

    &#34;&#34;&#34;
    def __init__(self,
                 *,
                 coredb: ioo.CoReManager,
                 classes: dict[str],
                 discarded: set,
                 cropped: dict,
                 # Source:
                 distance: float,
                 inclination: float,
                 phi: float):
        &#34;&#34;&#34;Initialize a CoReWaves dataset.

        TODO

        Parameters
        ----------
        coredb : ioo.CoReManager
            Instance of CoReManager with the actual data.
        
        classes : dict[str]
            Dictionary with the Equation of State (class) name as key and the
            corresponding label index as value.
        
        discarded : set[str]
            Set of GW IDs to discard from the dataset.
        
        cropped : dict[str]
            Dictionary with the class name as key and the corresponding
            cropping range as value. The range is given as a tuple of the form
            (start_index, stop_index).
        
        distance : float
            Distance to the source in Mpc.
        
        inclination : float
            Inclination of the source in radians.
        
        phi : float
            Azimuthal angle of the source in radians.

        &#34;&#34;&#34;
        self._check_classes_dict(classes)
        self.classes = classes
        self.discarded = discarded
        self.cropped = cropped
        # Source parameters
        self.distance = distance
        self.inclination = inclination
        self.phi = phi

        self.units = &#39;IS&#39;
        self.strains, self.times, self.metadata = self._get_strain_and_metadata(coredb)
        self._track_times = True
        self._dict_depth = dictools.get_depth(self.strains)
        self.labels = self._gen_labels()
        self.max_length = self._find_max_length()

        self.sample_rate = None  # Set up after resampling
        self.random_seed = None  # Set if calling the &#39;build_train_test_subsets&#39; method.

        self.whitened = False
        self.whiten_params = {}
        self.nonwhiten_strains = None

        # Train/Test subset splits (views into the same &#39;self.strains&#39;).
        #   Timeseries:
        self.Xtrain: np.ndarray = None
        self.Xtest: np.ndarray = None
        #   Labels:
        self.Ytrain: np.ndarray = None
        self.Ytest: np.ndarray = None
    
    def _get_strain_and_metadata(self, coredb: ioo.CoReManager) -&gt; tuple[dict, dict, pd.DataFrame]:
        &#34;&#34;&#34;Obtain the strain and metadata from a CoReManager instance.

        The strains are the Pluss and Cross polarizations obtained from the
        direct output of numerical relativistic simulations. They are expected
        to be projected at the detector afterwards, collapsing the polarization
        layer to a single strain per GW.
        
        Returns
        -------
        strains : dict{eos: {id: {pol: strain} } }
        
        times : dict{&#39;eos&#39;: {&#39;id&#39;: {pol: time_samples}} }
            Time samples associated to each GW.
            Since it has to follow the same nested structure as &#39;strains&#39;, but
            the time samples are the same among polarizations, for each GW both
            polarizations point to the same array in memory.
        
        metadata : pandas.DataFrame
            All parameters and data related to the strains.
            The order is the same as inside &#39;strains&#39; if unrolled to a flat list
            of strains up to the second depth level (the id.).
        
        &#34;&#34;&#34;
        strains = self._init_strains_dict()
        times = self._init_strains_dict()
        # Metadata columns/keys:
        index: list[str] = []
        mass: list[float] = []
        mass_ratio: list[float] = []
        eccentricity: list[float] = []
        mass_starA: list[float] = []
        mass_starB: list[float] = []
        spin_starA: list[float] = []
        spin_starB: list[float] = []
        merger_pos: list[int] = []  # Index position of the merger inside the array.

        for eos in self.classes:
            # Get and filter out GW simulations.
            ids = set(coredb.filter_by(&#39;id_eos&#39;, eos).index)
            try:
                ids -= self.discarded[eos]
            except KeyError:
                pass  # No discards.
            ids = sorted(ids)  # IMPORTANT!!! Keep order to be able to trace back simulations.
            
            for id_ in ids:
                # CoRe Rh data (in IS units):
                times_, h_plus, h_cros = coredb.gen_strain(
                    id_, self.distance, self.inclination, self.phi
                )

                # Crop those indicated at the parameter file, and leave whole
                # the rest.
                try:
                    t0, t1 = self.cropped[eos][id_]
                except KeyError:
                    crop = slice(None)
                else:
                    crop = slice(
                        np.argmin(np.abs(times_-t0)),
                        np.argmin(np.abs(times_-t1))
                    )
                strains[eos][id_] = {
                    &#39;plus&#39;: h_plus[crop],
                    &#39;cross&#39;: h_cros[crop]
                }
                # Both polarizations have the same sampling times, hence we
                # point each time polarization to the same array in memory.
                times[eos][id_] = {}
                times[eos][id_][&#39;plus&#39;] = times[eos][id_][&#39;cross&#39;] = times_[crop]
                
                # The time is centered at the merger.
                i_merger = tat.find_time_origin(times_[crop])

                # Associated metadata:
                md = coredb.metadata.loc[id_]
                index.append(md[&#39;database_key&#39;])
                mass.append(md[&#39;id_mass&#39;])
                mass_ratio.append(md[&#39;id_mass_ratio&#39;])
                eccentricity.append(md[&#39;id_eccentricity&#39;])
                mass_starA.append(md[&#39;id_mass_starA&#39;])
                mass_starB.append(md[&#39;id_mass_starB&#39;])
                spin_starA.append(md[&#39;id_spin_starA&#39;])
                spin_starB.append(md[&#39;id_spin_starB&#39;])
                merger_pos.append(i_merger)
        
        metadata = pd.DataFrame(
            data=dict(
                mass=mass, mass_ratio=mass_ratio, eccentricity=eccentricity,
                mass_starA=mass_starA, mass_starB=mass_starB,
                spin_starA=spin_starA, spin_starB=spin_starB,
                merger_pos=merger_pos
            ),
            index=index
        )
        
        return strains, times, metadata
    
    def find_merger(self, strain: np.ndarray) -&gt; int:
        from clawdia.estimators import find_merger
        return find_merger(strain)

    def _update_merger_positions(self):
        &#34;&#34;&#34;Update all &#39;merger_pos&#39; tags inside the metadata attribute.
        
        Time arrays are defined with the origin at the merger. When the length
        of the strain arrays is modified, the index position of the merger
        must be updated.

        NOTE: This method updates ALL the merger positions.
        
        &#34;&#34;&#34;
        for clas, id_ in self.keys(max_depth=2):
            times = self.times[clas][id_]
            # If more layers are present, only get the first instance of times
            # since all will be the same.
            if isinstance(times, dict):
                times = dictools._get_next_item(times)
            self.metadata.at[id_,&#39;merger_pos&#39;] = tat.find_time_origin(times)
    
    def resample(self, sample_rate, verbose=False) -&gt; None:
        &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

        Resample CoRe strains (from NR simulations) to a constant rate.

        This method updates the sample_rate, the max_length and the merger_pos
        inside the metadata attribute.

        Parameters
        ----------
        sample_rate : int
            The new sampling rate in Hz.

        verbose : bool
            If True, print information about the resampling.
        
        &#34;&#34;&#34;
        super().resample(sample_rate, verbose)

        # Update side-effect attributes.
        self._update_merger_positions()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def project(self, *, detector: str, ra: float, dec: float, geo_time: float, psi: float):
        &#34;&#34;&#34;Project strains into the chosen detector at specified coordinates.

        Project strains into the chosen detector at specified coordinates,
        using Bilby.

        This collapses the polarization layer in &#39;strains&#39; and &#39;times&#39; to a
        single strain.
        The times are rebuilt taking as a reference point the merger (t = 0).
        
        Parameters
        ----------
        detector : str
            Name of the ET arm in Bilby for InterferometerList().
        
        ra, dec : float
            Sky position in equatorial coordinates.
        
        geo_time : int | float
            Time of injection in GPS.
        
        psi : float
            Polarization angle.

        Caveats
        -------
        - The detector&#39;s name must exist in Bilby&#39;s InterferometerList().
        - Only one arm can be chosen.
        
        &#34;&#34;&#34;
        project_pars = dict(ra=ra, dec=dec, geocent_time=geo_time, psi=psi)
        for clas, id_ in self.keys(max_depth=2):
            hp = self.strains[clas][id_][&#39;plus&#39;]
            hc = self.strains[clas][id_][&#39;cross&#39;]
            
            # Drop the polarization layer.
            strain = detectors.project(
                hp, hc, parameters=project_pars, sf=self.sample_rate, 
                nfft=2*self.sample_rate, detector=detector
            )
            self.strains[clas][id_] = strain
            
            # Regenerate the time array with the merger located at the origin.
            duration = len(strain) / self.sample_rate
            t_merger = self.find_merger(strain) / self.sample_rate
            t0 = -t_merger
            t1 = duration - t_merger
            self.times[clas][id_] = tat.gen_time_array(t0, t1, self.sample_rate)
        
        # Update side-effect attributes
        self._dict_depth = dictools.get_depth(self.strains)
        self._update_merger_positions()
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def shrink_to_merger(self, offset: int = 0) -&gt; None:
        &#34;&#34;&#34;Shrink strains and time arrays w.r.t. the merger.

        Shrink strains (and their associated time arrays) discarding the left
        side of the merger (inspiral), with a given offset in samples.

        This also updates the metadata column &#39;merger_pos&#39;.

        NOTE: This is an irreversible action.

        Parameters
        ----------
        offset : int
            Offset in samples relative to the merger position.

        &#34;&#34;&#34;
        limits = {}
        for clas, id, *keys in self.keys():
            i_merger = self.metadata.at[id, &#39;merger_pos&#39;]
            # Same shrinking limits for all possible strains below ID layer.
            limits[id] = (i_merger+offset, -1)
        
        self.shrink_strains(limits)

        # Update side-effect attributes.
        self._update_merger_positions()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def convert_to_IS_units(self) -&gt; None:
        &#34;&#34;&#34;Convert data from scaled geometrized units to IS units.

        Convert strains and times from geometrized units (scaled to the mass
        of the system and the source distance) to IS units.
        
        Will raise an error if the data is already in IS units.
        
        &#34;&#34;&#34;
        if self.units == &#39;IS&#39;:
            raise RuntimeError(&#34;data already in IS units&#34;)

        for keys in self.keys():
            id_ = keys[1]
            mass = self.metadata.at[id_,&#39;mass&#39;]
            strain = self.get_strain(*keys)
            times = self.get_times(*keys)

            strain *=  mass * MSUN_MET / (self.distance * MPC_MET)
            times *= mass * MSUN_SEC

        self.units = &#39;IS&#39;

        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def convert_to_scaled_geometrized_units(self) -&gt; None:
        &#34;&#34;&#34;Convert data from IS to scaled geometrized units.
        
        Convert strains and times from IS to geometrized units, and scaled to the mass
        of the system and the source distance.

        Will raise an error if the data is already in geometrized units.
        
        &#34;&#34;&#34;
        if self.units == &#39;geometrized&#39;:
            raise RuntimeError(&#34;data already in geometrized units&#34;)
        
        for keys in self.keys():
            id_ = keys[1]
            mass = self.metadata.at[id_,&#39;mass&#39;]
            strain = self.get_strain(*keys)
            times = self.get_times(*keys)
            
            strain /=  mass * MSUN_MET / (self.distance * MPC_MET)
            times /= mass * MSUN_SEC

        self.units = &#39;geometrized&#39;

        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()


class InjectedCoReWaves(BaseInjected):
    &#34;&#34;&#34;Manage injections of GW data from CoRe dataset.
    
    - Tracks index position of the merger.
    
    - Computes the SNR only at the ring-down starting from the merger.
    
    - Computes also the usual SNR over the whole signal and stores it for
      later reference (attr. &#39;whole_snr_list&#39;).

    Attributes
    ----------
    snr_list : list
        Partial SNR values at which each signal is injected.
        This SNR is computed ONLY over the Ring-Down section of the waveform
        starting from the merger, hence the name &#39;partial SNR&#39;.

    whole_snr : dict
        Nested dictionary storing for each injection the equivalent SNR value
        computed over the whole signal, hence the name &#39;whole SNR&#39;.
        Structure: {id_: {partial_snr: whole_snr}}

    TODO

    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: Base,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 whiten_params: dict,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        &#34;&#34;&#34;
        Initializes an instance of the InjectedCoReWaves class.

        Parameters
        ----------
        clean_dataset : Base
            An instance of a BaseDataset class with noiseless signals.
        
        psd : np.ndarray | Callable
            Power Spectral Density of the detector&#39;s sensitivity in the
            range of frequencies of interest.
            Can be given as a callable function whose argument is
            expected to be an array of frequencies, or as a 2d-array
            with shape (2, psd_length) so that

            ```
                psd[0] = frequency_samples
                psd[1] = psd_samples.
            ```
            
            NOTE: It is also used to compute the &#39;asd&#39; attribute (ASD).
        
        detector : str
            GW detector name.
        
        noise_length : int
            Length of the background noise array to be generated for
            later use.
            It should be at least longer than the longest signal
            expected to be injected.
        
        whiten_params : dict
            Parameters to be passed to the &#39;whiten&#39; method of the
            &#39;BaseInjected&#39; class.
        
        freq_cutoff : int | float
            Frequency cutoff for the filter applied to the signal.
        
        freq_butter_order : int | float
            Order of the Butterworth filter applied to the signal.
        
        random_seed : int
            Random seed for generating random numbers.
        
        &#34;&#34;&#34;
        super().__init__(
            clean_dataset, psd=psd, detector=detector, noise_length=noise_length,
            whiten_params=whiten_params, freq_cutoff=freq_cutoff,
            freq_butter_order=freq_butter_order, random_seed=random_seed
        )

        self.whole_snr = {id_: {} for id_ in self.metadata.index}

    def _update_merger_positions(self):
        &#34;&#34;&#34;Update all &#39;merger_pos&#39; tags inside the metadata attribute.
        
        Time arrays are defined with the origin at the merger. When the length
        of the strain arrays is modified, the index position of the merger
        must be updated.

        NOTE: This method updates ALL the merger positions.
        
        &#34;&#34;&#34;
        for clas, id_ in self.keys(max_depth=2):
            # Same time array for all SNR variations.
            times = next(iter(self.times[clas][id_].values()))
            self.metadata.at[id_,&#39;merger_pos&#39;] = tat.find_time_origin(times)
    
    def gen_injections(self, snr: int | float | list, pad: int = 0):
        super().gen_injections(snr, pad)

        self._update_merger_positions()
    
    def _inject(self,
                strain: np.ndarray,
                snr: int | float,
                *,
                id: str,
                snr_offset: int) -&gt; np.ndarray:
        &#34;&#34;&#34;Inject a strain at &#39;snr&#39; into noise using &#39;self.noise&#39; instance.

        Parameters
        ----------
        strain : NDArray
            Signal to be injected into noise.
        
        snr : int | float
            Targeted signal-to-noise ratio.
        
        id : str
            Signal identifier (2nd layer of &#39;strains&#39; dict).
        
        snr_offset : int
            Offset (w.r.t. the merger) added to the start of the range for
            computing the SNR.
        
        Returns
        -------
        injected : NDArray
            Injected signal.
        
        NOTES
        -----
        - The SNR is computed over the Ring-Down only, starting from the
          position of the merger.
        - The metadata is expected to reflect the original state of the strains
          previous to any padding performed right before calling this function,
          which may be done to avoid the vignette effect.
        
        &#34;&#34;&#34;
        clas = self.find_class(id)
        merger_pos = self.metadata.at[id,&#39;merger_pos&#39;]
        original_length = len(self.strains_clean[clas][id])

        i0 = merger_pos + snr_offset
        i1 = (original_length - merger_pos) + snr_offset
        injected, scale = self.noise.inject(strain, snr=snr, snr_lim=(i0, i1))

        # Compute the equivalent SNR over the entire waveform.
        self.whole_snr[id][snr] = self.noise.snr(strain*scale)

        return injected
    
    def whiten(self):
        super().whiten()

        self._update_merger_positions()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gwadama.datasets.Base"><code class="flex name class">
<span>class <span class="ident">Base</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all datasets.</p>
<p>TODO: Update docstring.</p>
<p>Any dataset made of 'clean' (noiseless) GW must inherit this class.
It is designed to store strains as nested dictionaries, with each level's
key identifying a class/property of the strain. Each individual strain is a
1D NDArray containing the features.</p>
<p>By default there are two basic levels:</p>
<pre><code>- Class; to group up strains in categories.

- Id; An unique identifier for each strain, which must exist in the
  metadata DataFrame as Index.
</code></pre>
<p>Extra depths can be added, and will be thought of as modifications of the
same original strains from the upper identifier level. If splitting the
dataset into train and test susbsets, only combinations of (Class, Id) will
be considered.</p>
<p>NOTE: This class shall not be called directly. Use one of its subclasses.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict of strings and their integer labels, one per class (category).</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>All parameters and data related to the strains.
The order is the same as inside 'strains' if unrolled to a flat list
of strains up to the second depth level (the ID).
The total number of different waves must be equal to <code>len(metadata)</code>;
this does not include possible variations such polarizations or
multiple scallings of the same waveform when performing injections.</dd>
<dt><strong><code>strains</code></strong> :&ensp;<code>dict[dict [&hellip;]]</code></dt>
<dd>
<p>Strains stored as a nested dictionary, with each strain in an
independent array to provide more flexibility with data of a wide
range of lengths.</p>
<ul>
<li>
<p>Shape: {class: {id: strain} }</p>
</li>
<li>
<p>The 'class' key is the name of the class, a string which must exist
in the 'classes' list.</p>
</li>
<li>
<p>The 'id' is a unique identifier for each strain, and must exist in
the index of the 'metadata' (DataFrame) attribute.</p>
</li>
<li>
<p>Extra depths can be added as variations of each strain, such as
polarizations.</p>
</li>
</ul>
</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>dict</code></dt>
<dd>Class label of each wave ID, with shape {id: class_label}.
Each ID points to the label of its class in the 'classes' attribute.
Can be automatically constructed by calling the '_gen_labels()' method.</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the longest strain in the dataset.
Remember to update it if modifying the strains length.</dd>
<dt><strong><code>times</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Time samples associated with the strains, following the same structure
up to the second depth level: {class: {id: time_points} }
Useful when the sampling rate is variable or different between strains.
If None, all strains are assumed to be constantly sampled to the
sampling rate indicated by the 'sample_rate' attribute.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>
<p>If the 'times' attribute is present, this value is ignored. Otherwise
it is assumed all strains are constantly sampled to this value.</p>
<p>NOTE: If dealing with variable sampling rates, avoid setting this
attribute to anything other than None.</p>
</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Value passed to 'sklearn.model_selection.train_test_split' to generate
the Train and Test subsets. Saved for reproducibility purposes.</dd>
<dt><strong><code>Xtrain</code></strong>, <strong><code>Xtest</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Train and test subsets randomly split using SKLearn train_test_split
function with stratified labels.
Shape: {id: strain}.
The 'id' corresponds to the strain's index at 'self.metadata'.
They are just another views into the same data stored at 'self.strains',
so no copies are performed.</dd>
<dt><strong><code>Ytrain</code></strong>, <strong><code>Ytest</code></strong> :&ensp;<code>NDArray[int]</code>, optional</dt>
<dd>1D Array containing the labels in the same order as 'Xtrain' and
'Xtest' respectively.
See the attribute 'labels' for more info.</dd>
</dl>
<h2 id="caveats">Caveats</h2>
<ul>
<li>The additional depths in the strains nested dictionary can't be directly
tracked by the metadata Dataframe.</li>
<li>If working with two polarizations, they can be stored with just an
extra depth layer.</li>
</ul>
<p>Overwrite when inheriting!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Base:
    &#34;&#34;&#34;Base class for all datasets.

    TODO: Update docstring.

    Any dataset made of &#39;clean&#39; (noiseless) GW must inherit this class.
    It is designed to store strains as nested dictionaries, with each level&#39;s
    key identifying a class/property of the strain. Each individual strain is a
    1D NDArray containing the features.
    
    By default there are two basic levels:
        
        - Class; to group up strains in categories.
        
        - Id; An unique identifier for each strain, which must exist in the
          metadata DataFrame as Index.
    
    Extra depths can be added, and will be thought of as modifications of the
    same original strains from the upper identifier level. If splitting the
    dataset into train and test susbsets, only combinations of (Class, Id) will
    be considered.

    NOTE: This class shall not be called directly. Use one of its subclasses.
    
    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).

    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the ID).
        The total number of different waves must be equal to `len(metadata)`;
        this does not include possible variations such polarizations or
        multiple scallings of the same waveform when performing injections.
    
    strains : dict[dict [...]]
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        
        - Shape: {class: {id: strain} }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        - Extra depths can be added as variations of each strain, such as
          polarizations.
    
    labels : dict
        Class label of each wave ID, with shape {id: class_label}.
        Each ID points to the label of its class in the &#39;classes&#39; attribute.
        Can be automatically constructed by calling the &#39;_gen_labels()&#39; method.
    
    max_length : int
        Length of the longest strain in the dataset.
        Remember to update it if modifying the strains length.
    
    times : dict, optional
        Time samples associated with the strains, following the same structure
        up to the second depth level: {class: {id: time_points} }
        Useful when the sampling rate is variable or different between strains.
        If None, all strains are assumed to be constantly sampled to the
        sampling rate indicated by the &#39;sample_rate&#39; attribute.
    
    sample_rate : int, optional
        If the &#39;times&#39; attribute is present, this value is ignored. Otherwise
        it is assumed all strains are constantly sampled to this value.
        
        NOTE: If dealing with variable sampling rates, avoid setting this
        attribute to anything other than None.
    
    random_seed : int, optional
        Value passed to &#39;sklearn.model_selection.train_test_split&#39; to generate
        the Train and Test subsets. Saved for reproducibility purposes.
    
    Xtrain, Xtest : dict, optional
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        Shape: {id: strain}.
        The &#39;id&#39; corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
        They are just another views into the same data stored at &#39;self.strains&#39;,
        so no copies are performed.
    
    Ytrain, Ytest : NDArray[int], optional
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.
        See the attribute &#39;labels&#39; for more info.
    
    Caveats
    -------
    - The additional depths in the strains nested dictionary can&#39;t be directly
      tracked by the metadata Dataframe.
    - If working with two polarizations, they can be stored with just an
      extra depth layer.
    
    &#34;&#34;&#34;
    def __init__(self):
        &#34;&#34;&#34;Overwrite when inheriting!&#34;&#34;&#34;

        raise NotImplementedError(&#34;Base class should not be called directly.&#34;)

        #----------------------------------------------------------------------
        # Attributes whose values must be set up during initialization.
        #----------------------------------------------------------------------
    
        self.strains: dict = None
        self.classes: dict[str] = None
        self._check_classes_dict(self.classes)
        self.metadata: pd.DataFrame = None
        self.labels: dict[int] = self._gen_labels()
        
        # Number of nested layers in strains&#39; dictionary. Keep updated always:
        self._dict_depth: int = dictools.get_depth(self.strains)

        self.max_length = self._find_max_length()
        self.random_seed: int = None  # SKlearn train_test_split doesn&#39;t accept a Generator yet.
        self._track_times = False  # If True, self.times must be not None.

        #----------------------------------------------------------------------
        # Attributes whose values can be set up or otherwise left as follows.
        #----------------------------------------------------------------------

        # Whitening related attributes.
        self.whitened = False
        self.whiten_params = {}
        self.nonwhiten_strains = None

        # Time tracking related attributes.
        self.sample_rate: int = None
        self.times: dict = None
        
        # Train/Test subset splits (views into the same &#39;self.strains&#39;).
        #   Timeseries:
        self.Xtrain: np.ndarray = None
        self.Xtest: np.ndarray = None
        #   Labels:
        self.Ytrain: np.ndarray = None
        self.Ytest: np.ndarray = None

    def _check_classes_dict(self, classes: dict[str]):
        if not isinstance(classes, dict):
            raise TypeError(&#34;&#39;classes&#39; must be a dictionary&#34;)
        
        if not all(isinstance(k, str) for k in classes.keys()):
            raise TypeError(&#34;&#39;classes&#39; keys must be strings&#34;)
        
        labels = classes.values()
        if not all(isinstance(label, int) for label in labels):
            raise TypeError(&#34;&#39;classes&#39; values must be integers&#34;)
        if len(set(labels)) != len(classes):
            raise ValueError(&#34;&#39;classes&#39; values must be unique&#34;)
    
    def __len__(self):
        return len(self.metadata)

    def _gen_labels(self) -&gt; dict:
        &#34;&#34;&#34;Constructs the labels&#39; dictionary.

        The labels attribute maps each ID to the integer value of its class,
        mapped in the &#39;classes&#39; attribute.
        
        Returns
        -------
        labels : dict
            Shape {id: class_label} for each GW in the dataset.
        
        &#34;&#34;&#34;
        labels = {}
        for clas, id_ in self.keys(max_depth=2):
            labels[id_] = self.classes[clas]
        
        return labels

    def _init_strains_dict(self) -&gt; dict:
        return {clas: {} for clas in self.classes}
    
    def _init_times_dict(self) -&gt; dict:
        return dictools._replicate_structure_nested_dict(self.strains)

    def _find_max_length(self) -&gt; int:
        &#34;&#34;&#34;Return the length of the longest signal present in strains.&#34;&#34;&#34;

        max_length = 0
        for *_, strain in self.items():
            l = len(strain)
            if l &gt; max_length:
                max_length = l

        return max_length

    def _gen_times(self) -&gt; dict:
        &#34;&#34;&#34;Generate the time arrays associated to the strains.

        Assumes a constant sampling rate.
        
        Returns
        -------
        times : dict
            Nested dictionary with the same shape as &#39;self.strains&#39;.
        
        &#34;&#34;&#34;
        times = self._init_times_dict()
        for *keys, strain in self.items():
            length = len(strain)
            t_end = (length - 1) / self.sample_rate
            time = np.linspace(0, t_end, length)
            dictools.set_value_to_nested_dict(times, keys, time)
        
        return times

    def keys(self, max_depth: int = None) -&gt; list:
        &#34;&#34;&#34;Return the unrolled combinations of all strain identifiers.

        Return the unrolled combinations of all keys  of the nested dictionary
        of strains by a hierarchical recursive search.
        
        It can be thought of as the extended version of Python&#39;s
        &#39;dict().keys()&#39;, although this returns a plain list.

        Parameters
        ----------
        max_depth : int, optional
            If specified, it is the number of layers to iterate to at most in
            the nested &#39;strains&#39; dictionary.
        
        Returns
        -------
        keys : list
            The unrolled combination in a Python list.
        
        &#34;&#34;&#34;
        keys = dictools._unroll_nested_dictionary_keys(self.strains, max_depth=max_depth)

        return keys

    def items(self):
        &#34;&#34;&#34;Return a new view of the dataset&#39;s items with unrolled indices.

        Each iteration consists on a tuple containing all the nested keys in
        &#39;self.strains&#39; along with the corresponding strain,
        (clas, id, *, strain).
        
        It can be thought of as an extension of Python&#39;s `dict.items()`.
        Useful to quickly iterate over all items in the dataset.

        Example of usage with an arbitrary number of keys in the nested
        dictionary of strains:
        ```
        for *keys, strain in self.items():
            print(f&#34;Number of identifiers: {len(keys)}&#34;)
            print(f&#34;Length of the strain: {len(strain)}&#34;)
            do_something(strain)
        ```
        
        &#34;&#34;&#34;
        for indices in self.keys():
            yield (*indices, self.get_strain(*indices))

    def find_class(self, id):
        &#34;&#34;&#34;Find which &#39;class&#39; corresponds the strain &#39;id&#39;.

        Finds the &#39;class&#39; of the strain represented by the unique identifier
        &#39;id&#39;.

        Parameters
        ----------
        id : str
            Unique identifier of the string, that which also appears in the
            `metadata.index` DataFrame.
        
        Returns
        -------
        clas : int | str
            Class key associated to the strain &#39;id&#39;.
        
        &#34;&#34;&#34;
        return dictools._find_level0_of_level1(self.strains, id)

    def get_strain(self, *indices, normalize=False) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single strain from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;strains&#39; attribute.

        Parameters
        ----------
        *indices : str | int
            The indices of the strain to retrieve.
        
        normalize : bool
            If True, the returned strain will be normalized to its maximum
            amplitude.
        
        Returns
        -------
        strain : np.ndarray
            The requested strain.
        
        &#34;&#34;&#34;
        if len(indices) != self._dict_depth:
            raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)

        strain = dictools._get_value_from_nested_dict(self.strains, indices)
        if normalize:
            strain /= np.max(np.abs(strain))

        return strain

    def get_strains_array(self, length: int = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Get all strains stacked in a zero-padded Numpy 2d-array.

        Stacks all signals into an homogeneous numpy array whose length
        (axis=1) is determined by either &#39;length&#39; or, if None, by the longest
        strain in the subset.
        The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;strains_array&#39;. If None, the longest signal
            determines the length.

        Returns
        -------
        strains_array : np.ndarray
            train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.

        &#34;&#34;&#34;
        strains_flat = dictools.flatten_nested_dict(self.strains)
        strains_array, lengths = dictools.dict_to_stacked_array(strains_flat, target_length=length) 

        return strains_array, lengths

    def get_times(self, *indices) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single time array from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;times&#39; attribute.
        
        &#34;&#34;&#34;        
        if len(indices) != self._dict_depth:
            raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)
        
        return dictools._get_value_from_nested_dict(self.times, indices)

    def shrink_strains(self, limits: tuple | dict) -&gt; None:
        &#34;&#34;&#34;Shrink strains to a specific interval.

        Shrink strains (and their associated time arrays if present) to the
        interval given by &#39;limits&#39;.
        
        It also updates the &#39;max_length&#39; attribute.

        Parameters
        ----------
        limits : tuple | dict
            The limits of the interval to shrink to.
            If limits is a tuple, it must be of the form (start, end) in
            samples.
            If limits is a dictionary, it must be of the form {id: (start, end)},
            where id is the identifier of each strain.
            
            NOTE: If extra layers below ID are present, they will be shrunk
            accordingly.

        &#34;&#34;&#34;
        if isinstance(limits, tuple):
            limits_d = {id: limits for id in self.metadata.index}
        else:
            limits_d = limits

        for clas, id, *keys in self.keys():
            strain = self.get_strain(clas, id, *keys)
            # Same shrinking limits for all possible strains below ID layer.
            start, end = limits_d[id]
            strain = strain[start:end]
            dictools.set_value_to_nested_dict(self.strains, [clas,id,*keys], strain)

            if self._track_times:
                times = self.get_times(clas, id, *keys)
                times = times[start:end]
                dictools.set_value_to_nested_dict(self.times, [clas,id,*keys], times)

        self.max_length = self._find_max_length()

    def resample(self, sample_rate, verbose=False) -&gt; None:
        &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

        This assumes time tracking either with time arrays or with the
        sampling rate provided during initialization, which will be used to
        generate the time arrays previous to the resampling.

        This method updates the sample_rate and the max_length.

        Parameters
        ----------
        sample_rate : int
            The new sampling rate in Hz.

        verbose : bool
            If True, print information about the resampling.
        
        &#34;&#34;&#34;
        # Set up the time points associated to each strain in case it is not
        # provided.
        #
        if self._track_times:
            times = self.times
        else:
            if sample_rate == self.sample_rate:
                raise ValueError(&#34;trying to resample to the same sampling rate&#34;)
            if self.sample_rate is None:
                raise ValueError(&#34;neither time samples nor a global sampling rate were defined&#34;)
            
            self.times = self._gen_times()
            self._track_times = True

        for *keys, strain in self.items():
            time = dictools._get_value_from_nested_dict(self.times, keys)
            strain_resampled, time_resampled, sf_up, factor_down = tat.resample(
                strain, time, sample_rate, full_output=True
            )
            dictools.set_value_to_nested_dict(self.strains, keys, strain_resampled)
            dictools.set_value_to_nested_dict(self.times, keys, time_resampled)
            
            if verbose:
                print(
                    f&#34;Strain {keys[0]}::{keys[1]} up. to {sf_up} Hz, down by factor {factor_down}&#34;
                )

        self.sample_rate = sample_rate
        self.max_length = self._find_max_length()
    
    def whiten(self,
               asd_array: np.ndarray = None,
               pad: int = 0,
               highpass: int = None,
               flength: float = None,
               normed: bool = False) -&gt; None:
        &#34;&#34;&#34;Whiten the strains.
        
        Calling this method performs the whitening of all strains.
        Optionally, strains are first zero-padded, whitened and then shrunk to
        their initial size. This is useful to remove the vignetting effect.
        
        NOTE: Original (non-whitened) strains will be stored in the
        &#39;nonwhiten_strains&#39; attribute.
        
        &#34;&#34;&#34;
        if self.whitened:
            raise RuntimeError(&#34;dataset already whitened&#34;)

        if self.strains is None:
            raise RuntimeError(&#34;no strains have been given or generated yet&#34;)
        
        self.nonwhiten_strains = deepcopy(self.strains)
        
        for *keys, strain in self.items():
            strain_w = fat.whiten(
                strain, asd=asd_array, sample_rate=self.sample_rate, flength=flength,
                highpass=highpass, pad=pad, normed=normed
            )
            # Update strains attribute.
            dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
        
        self.whitened = True
        self.whiten_params = {
            &#34;asd_array&#34;: asd_array,
            &#34;pad&#34;: pad,
            &#34;highpass&#34;: highpass,
            &#34;flength&#34;: flength,
            &#34;normed&#34;: normed
        }
        
        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def build_train_test_subsets(self, train_size: int | float, random_seed: int = None):
        &#34;&#34;&#34;Generate a random Train and Test subsets.

        Only entries in the index of &#39;metadata&#39; DataFrame are considered
        independent waveforms, any extra key (layer) in the &#39;strains&#39; dict
        is treated monolithically during the shuffle.
        
        The strain values are just new views into the &#39;strains&#39; attribute.
        The shuffling is performed by Scikit-Learn&#39;s function
        &#39;train_test_split&#39;, with stratification enabled.

        Parameters
        ----------
        train_size : int | float
            If float, should be between 0.0 and 1.0 and represent the proportion
            of the dataset to include in the train subset.
            If int, represents the absolute number of train waves.
            
            Ref: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
        
        random_seed : int, optional
            Passed directly to &#39;sklearn.model_selection.train_test_split&#39;.
            It is also saved in its homonymous attribute.
            
        &#34;&#34;&#34;
        indices = list(self.metadata.index)
        i_train, i_test = train_test_split(
            indices,
            train_size=train_size,
            random_state=random_seed,
            shuffle=True,
            stratify=list(self.labels.values())
        )
        self.Xtrain, self.Ytrain = self._build_subset_strains(i_train)
        self.Xtest, self.Ytest = self._build_subset_strains(i_test)
        self.random_seed = random_seed
    
    def _build_subset_strains(self, indices):
        &#34;&#34;&#34;Return a subset of strains and their labels based on their ID.

        Return a new view into &#39;self.strains&#39; using the input indices (ID) as
        the first layer of the nested dictionary.

        This collapses the first layer, the class, leaving the unique
        identifier ID as first layer. Nevertheless, the rest of possible layers
        beneath &#39;ID&#39; are monolithically preserved.
        
        Parameters
        ----------
        indices : array-like
            The indices are w.r.t. the Pandas &#39;self.metadata.index&#39;.

        Returns
        -------
        strains : dict {id: strain}
            The id key is the strain&#39;s index at &#39;self.metadata&#39;.
        
        labels : NDArray
            1D Array containing the labels associated to &#39;strains&#39;.
        
        &#34;&#34;&#34;
        strains = {}
        labels = np.empty(len(indices), dtype=int)
        for i, id_ in enumerate(indices):
            labels[i] = self.labels[id_]
            clas = self.find_class(id_)
            strains[id_] = self.strains[clas][id_]
        
        return strains, labels

    def _update_train_test_subsets(self):
        &#34;&#34;&#34;Builds again the Train/Test subsets from the main strains attribute.&#34;&#34;&#34;

        id_train = list(self.Xtrain.keys())
        id_test = list(self.Xtest.keys())
        self.Xtrain, self.Ytrain = self._build_subset_strains(id_train)
        self.Xtest, self.Ytest = self._build_subset_strains(id_test)
    
    def get_xtrain_array(self, length=None):
        &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the train subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;train_array&#39;. If None, the longest signal
            determines the length.

        Returns
        -------
        train_array : np.ndarray
            train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.

        &#34;&#34;&#34;
        return dictools.dict_to_stacked_array(self.Xtrain, target_length=length)
    
    def get_xtest_array(self, length=None):
        &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the test subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Parameters
        ----------
        length : int, optional

        Returns
        -------
        test_array : np.ndarray
            test subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;test_array&#39;.

        &#34;&#34;&#34;
        return dictools.dict_to_stacked_array(self.Xtest, target_length=length)
    
    def get_ytrain_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered training labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.

        with_id : bool
            If True, return also the list of related IDs.

        with_index : bool
            If True, return also the related GLOBAL indices; w.r.t. the stacked
            arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
            False by default.

        Returns
        -------
        np.ndarray
            Filtered train labels.

        np.ndarray, optional
            IDs associated to the filtered train labels.
        
        np.ndarray, optional
            Indices associated to the filtered train labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytrain, list(self.Xtrain), classes,
            with_id=with_id, with_index=with_index
        )

    def get_ytest_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered test labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.

        with_id : bool
            If True, return also the list of related IDs.

        with_index : bool
            If True, return also the related GLOBAL indices; w.r.t. the stacked
            arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

        Returns
        -------
        np.ndarray
            Filtered test labels.

        np.ndarray, optional
            IDs associated to the filtered test labels.
        
        np.ndarray, optional
            Indices associated to the filtered test labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytest, list(self.Xtest), classes,
            with_id=with_id, with_index=with_index
        )
    
    def _filter_labels(self, labels, labels_id, classes, with_id=False, with_index=False):
        &#34;&#34;&#34;Filter labels based on &#39;classes&#39;.

        This is a helper function for &#39;get_ytrain_array&#39; and &#39;get_ytest_array&#39;.
        
        Parameters
        ----------
        labels : np.ndarray
            The array containing the labels.
        
        labels_id : list
            IDs associated to the labels.
        
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related indices w.r.t. the stacked array
            returned by &#39;_stack_subset&#39; given the strains related to &#39;labels&#39;
            WITHOUT filters.
            False by default.

        Returns
        -------
        filtered_labels : np.ndarray
            Filtered labels.

        filtered_ids : np.ndarray, optional
            IDs associated to the filtered labels.

        filtered_indices : np.ndarray, optional
            Indices associated to the filtered labels.

        &#34;&#34;&#34;
        if len(labels) != len(labels_id):
            raise ValueError(&#34;&#39;labels&#39; and &#39;labels_id&#39; must have the same length.&#34;)

        if isinstance(classes, str):
            if classes == &#39;all&#39;:
                return labels
            else:
                classes = [classes]
        elif not isinstance(classes, list):
            raise TypeError(&#34;&#39;classes&#39; must be a string or list of strings.&#34;)
        
        filtered_labels = []
        filtered_ids = []
        filtered_indices = []
        
        i = 0
        for label, id in zip(labels, labels_id):
            if self.find_class(id) in classes:
                filtered_labels.append(label)
                filtered_ids.append(id)
                filtered_indices.append(i)
                i += 1  # Indices w.r.t. the FILTERED set!!!
        
        filtered_labels = np.array(filtered_labels)
        filtered_ids = np.array(filtered_ids)
        filtered_indices = np.array(filtered_indices)
        
        if with_id and with_index:
            return filtered_labels, filtered_ids, filtered_indices
        if with_id:
            return filtered_labels, filtered_ids
        if with_index:
            return filtered_labels, filtered_indices
        return filtered_labels
        
    def stack_by_id(self, id_list: list, length: int = None):
        &#34;&#34;&#34;Stack an subset of strains by their ID into a Numpy array.

        Stack an arbitrary selection of strains by their original ID into a
        zero-padded 2d-array. The resulting order is the same as the order of
        that in &#39;id_list&#39;.

        Parameters
        ----------
        id_list : list
            The IDs of the strains to be stacked.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        Notes
        -----
        - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
          not filter by &#39;classes&#39; since it would be redundant, as IDs are
          unique.

        &#34;&#34;&#34;
        if not isinstance(id_list, list):
            raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)

        # Collapse the Class layer.
        strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

        # Filter out those not in the &#39;id_list&#39;.
        strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)
        assert len(strains) == len(id_list)

        # Sort them to match the order in &#39;id_list&#39;.
        strains = {id: strains[id] for id in id_list}

        strains = dictools.flatten_nested_dict(strains)
        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        return stacked_signals, lengths</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></li>
<li><a title="gwadama.datasets.CoReWaves" href="#gwadama.datasets.CoReWaves">CoReWaves</a></li>
<li><a title="gwadama.datasets.SyntheticWaves" href="#gwadama.datasets.SyntheticWaves">SyntheticWaves</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="gwadama.datasets.Base.build_train_test_subsets"><code class="name flex">
<span>def <span class="ident">build_train_test_subsets</span></span>(<span>self, train_size:Â intÂ |Â float, random_seed:Â intÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a random Train and Test subsets.</p>
<p>Only entries in the index of 'metadata' DataFrame are considered
independent waveforms, any extra key (layer) in the 'strains' dict
is treated monolithically during the shuffle.</p>
<p>The strain values are just new views into the 'strains' attribute.
The shuffling is performed by Scikit-Learn's function
'train_test_split', with stratification enabled.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_size</code></strong> :&ensp;<code>int | float</code></dt>
<dd>
<p>If float, should be between 0.0 and 1.0 and represent the proportion
of the dataset to include in the train subset.
If int, represents the absolute number of train waves.</p>
<p>Ref: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a></p>
</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Passed directly to 'sklearn.model_selection.train_test_split'.
It is also saved in its homonymous attribute.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_train_test_subsets(self, train_size: int | float, random_seed: int = None):
    &#34;&#34;&#34;Generate a random Train and Test subsets.

    Only entries in the index of &#39;metadata&#39; DataFrame are considered
    independent waveforms, any extra key (layer) in the &#39;strains&#39; dict
    is treated monolithically during the shuffle.
    
    The strain values are just new views into the &#39;strains&#39; attribute.
    The shuffling is performed by Scikit-Learn&#39;s function
    &#39;train_test_split&#39;, with stratification enabled.

    Parameters
    ----------
    train_size : int | float
        If float, should be between 0.0 and 1.0 and represent the proportion
        of the dataset to include in the train subset.
        If int, represents the absolute number of train waves.
        
        Ref: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
    
    random_seed : int, optional
        Passed directly to &#39;sklearn.model_selection.train_test_split&#39;.
        It is also saved in its homonymous attribute.
        
    &#34;&#34;&#34;
    indices = list(self.metadata.index)
    i_train, i_test = train_test_split(
        indices,
        train_size=train_size,
        random_state=random_seed,
        shuffle=True,
        stratify=list(self.labels.values())
    )
    self.Xtrain, self.Ytrain = self._build_subset_strains(i_train)
    self.Xtest, self.Ytest = self._build_subset_strains(i_test)
    self.random_seed = random_seed</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.find_class"><code class="name flex">
<span>def <span class="ident">find_class</span></span>(<span>self, id)</span>
</code></dt>
<dd>
<div class="desc"><p>Find which 'class' corresponds the strain 'id'.</p>
<p>Finds the 'class' of the strain represented by the unique identifier
'id'.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique identifier of the string, that which also appears in the
<code>metadata.index</code> DataFrame.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clas</code></strong> :&ensp;<code>int | str</code></dt>
<dd>Class key associated to the strain 'id'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_class(self, id):
    &#34;&#34;&#34;Find which &#39;class&#39; corresponds the strain &#39;id&#39;.

    Finds the &#39;class&#39; of the strain represented by the unique identifier
    &#39;id&#39;.

    Parameters
    ----------
    id : str
        Unique identifier of the string, that which also appears in the
        `metadata.index` DataFrame.
    
    Returns
    -------
    clas : int | str
        Class key associated to the strain &#39;id&#39;.
    
    &#34;&#34;&#34;
    return dictools._find_level0_of_level1(self.strains, id)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_strain"><code class="name flex">
<span>def <span class="ident">get_strain</span></span>(<span>self, *indices, normalize=False) ->Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get a single strain from the complete index coordinates.</p>
<p>This is just a shortcut to avoid having to write several squared
brackets.</p>
<p>NOTE: The returned strain is not a copy; if its contents are modified,
the changes will be reflected inside the 'strains' attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*indices</code></strong> :&ensp;<code>str | int</code></dt>
<dd>The indices of the strain to retrieve.</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the returned strain will be normalized to its maximum
amplitude.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>strain</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The requested strain.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_strain(self, *indices, normalize=False) -&gt; np.ndarray:
    &#34;&#34;&#34;Get a single strain from the complete index coordinates.
    
    This is just a shortcut to avoid having to write several squared
    brackets.

    NOTE: The returned strain is not a copy; if its contents are modified,
    the changes will be reflected inside the &#39;strains&#39; attribute.

    Parameters
    ----------
    *indices : str | int
        The indices of the strain to retrieve.
    
    normalize : bool
        If True, the returned strain will be normalized to its maximum
        amplitude.
    
    Returns
    -------
    strain : np.ndarray
        The requested strain.
    
    &#34;&#34;&#34;
    if len(indices) != self._dict_depth:
        raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)

    strain = dictools._get_value_from_nested_dict(self.strains, indices)
    if normalize:
        strain /= np.max(np.abs(strain))

    return strain</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_strains_array"><code class="name flex">
<span>def <span class="ident">get_strains_array</span></span>(<span>self, length:Â intÂ =Â None) ->Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get all strains stacked in a zero-padded Numpy 2d-array.</p>
<p>Stacks all signals into an homogeneous numpy array whose length
(axis=1) is determined by either 'length' or, if None, by the longest
strain in the subset.
The remaining space is zeroed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Target length of the 'strains_array'. If None, the longest signal
determines the length.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>strains_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>train subset.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>Original length of each strain, following the same order as the
first axis of 'train_array'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_strains_array(self, length: int = None) -&gt; np.ndarray:
    &#34;&#34;&#34;Get all strains stacked in a zero-padded Numpy 2d-array.

    Stacks all signals into an homogeneous numpy array whose length
    (axis=1) is determined by either &#39;length&#39; or, if None, by the longest
    strain in the subset.
    The remaining space is zeroed.

    Parameters
    ----------
    length : int, optional
        Target length of the &#39;strains_array&#39;. If None, the longest signal
        determines the length.

    Returns
    -------
    strains_array : np.ndarray
        train subset.
    
    lengths : list
        Original length of each strain, following the same order as the
        first axis of &#39;train_array&#39;.

    &#34;&#34;&#34;
    strains_flat = dictools.flatten_nested_dict(self.strains)
    strains_array, lengths = dictools.dict_to_stacked_array(strains_flat, target_length=length) 

    return strains_array, lengths</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_times"><code class="name flex">
<span>def <span class="ident">get_times</span></span>(<span>self, *indices) ->Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get a single time array from the complete index coordinates.</p>
<p>This is just a shortcut to avoid having to write several squared
brackets.</p>
<p>NOTE: The returned strain is not a copy; if its contents are modified,
the changes will be reflected inside the 'times' attribute.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_times(self, *indices) -&gt; np.ndarray:
    &#34;&#34;&#34;Get a single time array from the complete index coordinates.
    
    This is just a shortcut to avoid having to write several squared
    brackets.

    NOTE: The returned strain is not a copy; if its contents are modified,
    the changes will be reflected inside the &#39;times&#39; attribute.
    
    &#34;&#34;&#34;        
    if len(indices) != self._dict_depth:
        raise ValueError(&#34;indices must match the depth of &#39;self.strains&#39;&#34;)
    
    return dictools._get_value_from_nested_dict(self.times, indices)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_xtest_array"><code class="name flex">
<span>def <span class="ident">get_xtest_array</span></span>(<span>self, length=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the test subset stacked in a zero-padded Numpy 2d-array.</p>
<p>Stacks all signals in the test subset into an homogeneous numpy array
whose length (axis=1) is determined by either 'length' or, if None, by
the longest strain in the subset. The remaining space is zeroed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>test_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>test subset.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>Original length of each strain, following the same order as the
first axis of 'test_array'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xtest_array(self, length=None):
    &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

    Stacks all signals in the test subset into an homogeneous numpy array
    whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
    the longest strain in the subset. The remaining space is zeroed.

    Parameters
    ----------
    length : int, optional

    Returns
    -------
    test_array : np.ndarray
        test subset.
    
    lengths : list
        Original length of each strain, following the same order as the
        first axis of &#39;test_array&#39;.

    &#34;&#34;&#34;
    return dictools.dict_to_stacked_array(self.Xtest, target_length=length)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_xtrain_array"><code class="name flex">
<span>def <span class="ident">get_xtrain_array</span></span>(<span>self, length=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the train subset stacked in a zero-padded Numpy 2d-array.</p>
<p>Stacks all signals in the train subset into an homogeneous numpy array
whose length (axis=1) is determined by either 'length' or, if None, by
the longest strain in the subset. The remaining space is zeroed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Target length of the 'train_array'. If None, the longest signal
determines the length.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>train_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>train subset.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>Original length of each strain, following the same order as the
first axis of 'train_array'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xtrain_array(self, length=None):
    &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

    Stacks all signals in the train subset into an homogeneous numpy array
    whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
    the longest strain in the subset. The remaining space is zeroed.

    Parameters
    ----------
    length : int, optional
        Target length of the &#39;train_array&#39;. If None, the longest signal
        determines the length.

    Returns
    -------
    train_array : np.ndarray
        train subset.
    
    lengths : list
        Original length of each strain, following the same order as the
        first axis of &#39;train_array&#39;.

    &#34;&#34;&#34;
    return dictools.dict_to_stacked_array(self.Xtrain, target_length=length)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_ytest_array"><code class="name flex">
<span>def <span class="ident">get_ytest_array</span></span>(<span>self, classes='all', with_id=False, with_index=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the filtered test labels.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str] | 'all'</code></dt>
<dd>The classes to include in the labels.
All classes are included by default.</dd>
<dt><strong><code>with_id</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the list of related IDs.</dd>
<dt><strong><code>with_index</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related GLOBAL indices; w.r.t. the stacked
arrays returned by 'get_xtest_array' WITHOUT filters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Filtered test labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>IDs associated to the filtered test labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>Indices associated to the filtered test labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ytest_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
    &#34;&#34;&#34;Get the filtered test labels.

    Parameters
    ----------
    classes : str | list[str] | &#39;all&#39;
        The classes to include in the labels.
        All classes are included by default.

    with_id : bool
        If True, return also the list of related IDs.

    with_index : bool
        If True, return also the related GLOBAL indices; w.r.t. the stacked
        arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

    Returns
    -------
    np.ndarray
        Filtered test labels.

    np.ndarray, optional
        IDs associated to the filtered test labels.
    
    np.ndarray, optional
        Indices associated to the filtered test labels.

    &#34;&#34;&#34;
    return self._filter_labels(
        self.Ytest, list(self.Xtest), classes,
        with_id=with_id, with_index=with_index
    )</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.get_ytrain_array"><code class="name flex">
<span>def <span class="ident">get_ytrain_array</span></span>(<span>self, classes='all', with_id=False, with_index=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the filtered training labels.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str] | 'all'</code></dt>
<dd>The classes to include in the labels.
All classes are included by default.</dd>
<dt><strong><code>with_id</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the list of related IDs.</dd>
<dt><strong><code>with_index</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related GLOBAL indices; w.r.t. the stacked
arrays returned by 'get_xtrain_array' WITHOUT filters.
False by default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Filtered train labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>IDs associated to the filtered train labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>Indices associated to the filtered train labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ytrain_array(self, classes=&#39;all&#39;, with_id=False, with_index=False):
    &#34;&#34;&#34;Get the filtered training labels.

    Parameters
    ----------
    classes : str | list[str] | &#39;all&#39;
        The classes to include in the labels.
        All classes are included by default.

    with_id : bool
        If True, return also the list of related IDs.

    with_index : bool
        If True, return also the related GLOBAL indices; w.r.t. the stacked
        arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
        False by default.

    Returns
    -------
    np.ndarray
        Filtered train labels.

    np.ndarray, optional
        IDs associated to the filtered train labels.
    
    np.ndarray, optional
        Indices associated to the filtered train labels.

    &#34;&#34;&#34;
    return self._filter_labels(
        self.Ytrain, list(self.Xtrain), classes,
        with_id=with_id, with_index=with_index
    )</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a new view of the dataset's items with unrolled indices.</p>
<p>Each iteration consists on a tuple containing all the nested keys in
'self.strains' along with the corresponding strain,
(clas, id, *, strain).</p>
<p>It can be thought of as an extension of Python's <code>dict.items()</code>.
Useful to quickly iterate over all items in the dataset.</p>
<p>Example of usage with an arbitrary number of keys in the nested
dictionary of strains:</p>
<pre><code>for *keys, strain in self.items():
    print(f&quot;Number of identifiers: {len(keys)}&quot;)
    print(f&quot;Length of the strain: {len(strain)}&quot;)
    do_something(strain)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    &#34;&#34;&#34;Return a new view of the dataset&#39;s items with unrolled indices.

    Each iteration consists on a tuple containing all the nested keys in
    &#39;self.strains&#39; along with the corresponding strain,
    (clas, id, *, strain).
    
    It can be thought of as an extension of Python&#39;s `dict.items()`.
    Useful to quickly iterate over all items in the dataset.

    Example of usage with an arbitrary number of keys in the nested
    dictionary of strains:
    ```
    for *keys, strain in self.items():
        print(f&#34;Number of identifiers: {len(keys)}&#34;)
        print(f&#34;Length of the strain: {len(strain)}&#34;)
        do_something(strain)
    ```
    
    &#34;&#34;&#34;
    for indices in self.keys():
        yield (*indices, self.get_strain(*indices))</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.keys"><code class="name flex">
<span>def <span class="ident">keys</span></span>(<span>self, max_depth:Â intÂ =Â None) ->Â list</span>
</code></dt>
<dd>
<div class="desc"><p>Return the unrolled combinations of all strain identifiers.</p>
<p>Return the unrolled combinations of all keys
of the nested dictionary
of strains by a hierarchical recursive search.</p>
<p>It can be thought of as the extended version of Python's
'dict().keys()', although this returns a plain list.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>max_depth</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>If specified, it is the number of layers to iterate to at most in
the nested 'strains' dictionary.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>keys</code></strong> :&ensp;<code>list</code></dt>
<dd>The unrolled combination in a Python list.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keys(self, max_depth: int = None) -&gt; list:
    &#34;&#34;&#34;Return the unrolled combinations of all strain identifiers.

    Return the unrolled combinations of all keys  of the nested dictionary
    of strains by a hierarchical recursive search.
    
    It can be thought of as the extended version of Python&#39;s
    &#39;dict().keys()&#39;, although this returns a plain list.

    Parameters
    ----------
    max_depth : int, optional
        If specified, it is the number of layers to iterate to at most in
        the nested &#39;strains&#39; dictionary.
    
    Returns
    -------
    keys : list
        The unrolled combination in a Python list.
    
    &#34;&#34;&#34;
    keys = dictools._unroll_nested_dictionary_keys(self.strains, max_depth=max_depth)

    return keys</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.resample"><code class="name flex">
<span>def <span class="ident">resample</span></span>(<span>self, sample_rate, verbose=False) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Resample strain and time arrays to a constant rate.</p>
<p>This assumes time tracking either with time arrays or with the
sampling rate provided during initialization, which will be used to
generate the time arrays previous to the resampling.</p>
<p>This method updates the sample_rate and the max_length.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>The new sampling rate in Hz.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, print information about the resampling.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resample(self, sample_rate, verbose=False) -&gt; None:
    &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

    This assumes time tracking either with time arrays or with the
    sampling rate provided during initialization, which will be used to
    generate the time arrays previous to the resampling.

    This method updates the sample_rate and the max_length.

    Parameters
    ----------
    sample_rate : int
        The new sampling rate in Hz.

    verbose : bool
        If True, print information about the resampling.
    
    &#34;&#34;&#34;
    # Set up the time points associated to each strain in case it is not
    # provided.
    #
    if self._track_times:
        times = self.times
    else:
        if sample_rate == self.sample_rate:
            raise ValueError(&#34;trying to resample to the same sampling rate&#34;)
        if self.sample_rate is None:
            raise ValueError(&#34;neither time samples nor a global sampling rate were defined&#34;)
        
        self.times = self._gen_times()
        self._track_times = True

    for *keys, strain in self.items():
        time = dictools._get_value_from_nested_dict(self.times, keys)
        strain_resampled, time_resampled, sf_up, factor_down = tat.resample(
            strain, time, sample_rate, full_output=True
        )
        dictools.set_value_to_nested_dict(self.strains, keys, strain_resampled)
        dictools.set_value_to_nested_dict(self.times, keys, time_resampled)
        
        if verbose:
            print(
                f&#34;Strain {keys[0]}::{keys[1]} up. to {sf_up} Hz, down by factor {factor_down}&#34;
            )

    self.sample_rate = sample_rate
    self.max_length = self._find_max_length()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.shrink_strains"><code class="name flex">
<span>def <span class="ident">shrink_strains</span></span>(<span>self, limits:Â tupleÂ |Â dict) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Shrink strains to a specific interval.</p>
<p>Shrink strains (and their associated time arrays if present) to the
interval given by 'limits'.</p>
<p>It also updates the 'max_length' attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>limits</code></strong> :&ensp;<code>tuple | dict</code></dt>
<dd>
<p>The limits of the interval to shrink to.
If limits is a tuple, it must be of the form (start, end) in
samples.
If limits is a dictionary, it must be of the form {id: (start, end)},
where id is the identifier of each strain.</p>
<p>NOTE: If extra layers below ID are present, they will be shrunk
accordingly.</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shrink_strains(self, limits: tuple | dict) -&gt; None:
    &#34;&#34;&#34;Shrink strains to a specific interval.

    Shrink strains (and their associated time arrays if present) to the
    interval given by &#39;limits&#39;.
    
    It also updates the &#39;max_length&#39; attribute.

    Parameters
    ----------
    limits : tuple | dict
        The limits of the interval to shrink to.
        If limits is a tuple, it must be of the form (start, end) in
        samples.
        If limits is a dictionary, it must be of the form {id: (start, end)},
        where id is the identifier of each strain.
        
        NOTE: If extra layers below ID are present, they will be shrunk
        accordingly.

    &#34;&#34;&#34;
    if isinstance(limits, tuple):
        limits_d = {id: limits for id in self.metadata.index}
    else:
        limits_d = limits

    for clas, id, *keys in self.keys():
        strain = self.get_strain(clas, id, *keys)
        # Same shrinking limits for all possible strains below ID layer.
        start, end = limits_d[id]
        strain = strain[start:end]
        dictools.set_value_to_nested_dict(self.strains, [clas,id,*keys], strain)

        if self._track_times:
            times = self.get_times(clas, id, *keys)
            times = times[start:end]
            dictools.set_value_to_nested_dict(self.times, [clas,id,*keys], times)

    self.max_length = self._find_max_length()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.stack_by_id"><code class="name flex">
<span>def <span class="ident">stack_by_id</span></span>(<span>self, id_list:Â list, length:Â intÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Stack an subset of strains by their ID into a Numpy array.</p>
<p>Stack an arbitrary selection of strains by their original ID into a
zero-padded 2d-array. The resulting order is the same as the order of
that in 'id_list'.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id_list</code></strong> :&ensp;<code>list</code></dt>
<dd>The IDs of the strains to be stacked.</dd>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The target length of the stacked array. If None, the longest signal
determines the length.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stacked_signals</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The array containing the stacked strains.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>The original lengths of each strain, following the same order as
the first axis of 'stacked_signals'.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>Unlike in 'get_xtrain_array' and 'get_xtest_array', this method does
not filter by 'classes' since it would be redundant, as IDs are
unique.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stack_by_id(self, id_list: list, length: int = None):
    &#34;&#34;&#34;Stack an subset of strains by their ID into a Numpy array.

    Stack an arbitrary selection of strains by their original ID into a
    zero-padded 2d-array. The resulting order is the same as the order of
    that in &#39;id_list&#39;.

    Parameters
    ----------
    id_list : list
        The IDs of the strains to be stacked.

    length : int, optional
        The target length of the stacked array. If None, the longest signal
        determines the length.

    Returns
    -------
    stacked_signals : np.ndarray
        The array containing the stacked strains.

    lengths : list
        The original lengths of each strain, following the same order as
        the first axis of &#39;stacked_signals&#39;.

    Notes
    -----
    - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
      not filter by &#39;classes&#39; since it would be redundant, as IDs are
      unique.

    &#34;&#34;&#34;
    if not isinstance(id_list, list):
        raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)

    # Collapse the Class layer.
    strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

    # Filter out those not in the &#39;id_list&#39;.
    strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)
    assert len(strains) == len(id_list)

    # Sort them to match the order in &#39;id_list&#39;.
    strains = {id: strains[id] for id in id_list}

    strains = dictools.flatten_nested_dict(strains)
    stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
    
    return stacked_signals, lengths</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.Base.whiten"><code class="name flex">
<span>def <span class="ident">whiten</span></span>(<span>self, asd_array:Â numpy.ndarrayÂ =Â None, pad:Â intÂ =Â 0, highpass:Â intÂ =Â None, flength:Â floatÂ =Â None, normed:Â boolÂ =Â False) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Whiten the strains.</p>
<p>Calling this method performs the whitening of all strains.
Optionally, strains are first zero-padded, whitened and then shrunk to
their initial size. This is useful to remove the vignetting effect.</p>
<p>NOTE: Original (non-whitened) strains will be stored in the
'nonwhiten_strains' attribute.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def whiten(self,
           asd_array: np.ndarray = None,
           pad: int = 0,
           highpass: int = None,
           flength: float = None,
           normed: bool = False) -&gt; None:
    &#34;&#34;&#34;Whiten the strains.
    
    Calling this method performs the whitening of all strains.
    Optionally, strains are first zero-padded, whitened and then shrunk to
    their initial size. This is useful to remove the vignetting effect.
    
    NOTE: Original (non-whitened) strains will be stored in the
    &#39;nonwhiten_strains&#39; attribute.
    
    &#34;&#34;&#34;
    if self.whitened:
        raise RuntimeError(&#34;dataset already whitened&#34;)

    if self.strains is None:
        raise RuntimeError(&#34;no strains have been given or generated yet&#34;)
    
    self.nonwhiten_strains = deepcopy(self.strains)
    
    for *keys, strain in self.items():
        strain_w = fat.whiten(
            strain, asd=asd_array, sample_rate=self.sample_rate, flength=flength,
            highpass=highpass, pad=pad, normed=normed
        )
        # Update strains attribute.
        dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
    
    self.whitened = True
    self.whiten_params = {
        &#34;asd_array&#34;: asd_array,
        &#34;pad&#34;: pad,
        &#34;highpass&#34;: highpass,
        &#34;flength&#34;: flength,
        &#34;normed&#34;: normed
    }
    
    # Update side-effect attributes.
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gwadama.datasets.BaseInjected"><code class="flex name class">
<span>class <span class="ident">BaseInjected</span></span>
<span>(</span><span>clean_dataset:Â <a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a>, *, psd:Â Union[numpy.ndarray,Â Callable], detector:Â str, noise_length:Â int, whiten_params:Â dict, freq_cutoff:Â intÂ |Â float, freq_butter_order:Â intÂ |Â float, random_seed:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Manage an injected dataset with multiple SNR values.</p>
<p>It is designed to store strains as nested dictionaries, with each level's
key identifying a class/property of the strain. Each individual strain is a
1D NDArray containing the features.</p>
<p>NOTE: Instances of this class or any other Class(BaseInjected) are
initialized from an instance of any Class(Base) instance (clean dataset).</p>
<p>By default there are THREE basic levels:</p>
<pre><code>- Class; to group up strains in categories.

- Id; An unique identifier for each strain, which must exist in the
  metadata DataFrame as Index.

- SNR; the signal-to-noise ratio at which has been injected w.r.t. a
  power spectral density of reference (e.g. the sensitivity of a GW
  detector).
</code></pre>
<p>Extra depths can be added, and will be thought of as modifications of the
same original strains from the upper identifier level. However they should
be added between the 'Id' and 'SNR' layer, since the SNR is the final
realization of any variations made of a given (Class, Id) signal.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of labels, one per class (category).</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>All parameters and data related to the original strains, inherited
(copied) from a clean Class(Base) instance.
The order is the same as inside 'strains' if unrolled to a flat list
of strains up to the second depth level (the ID).
The total number of different waves must be equal to <code>len(metadata)</code>;
this does not include possible variations such polarizations or
multiple scallings of the same waveform when performing injections.</dd>
<dt><strong><code>strains_clean</code></strong> :&ensp;<code>dict[dict]</code></dt>
<dd>
<p>Strains inherited (copied) from a clean Class(Base) instance.
This copy is kept in order to perform new injections.</p>
<ul>
<li>
<p>Shape: {class: {id: strain} }</p>
</li>
<li>
<p>The 'class' key is the name of the class, a string which must exist
in the 'classes' list.</p>
</li>
<li>
<p>The 'id' is a unique identifier for each strain, and must exist in
the index of the 'metadata' (DataFrame) attribute.</p>
</li>
</ul>
<p>NOTE: These strains should be not modified. If new clean strains are
needed, create a new clean dataset instance first, and then initialise
this class with it.</p>
</dd>
<dt><strong><code>strains</code></strong> :&ensp;<code>dict[dict]</code></dt>
<dd>
<p>Injected trains stored as a nested dictionary, with each strain in an
independent array to provide more flexibility with data of a wide
range of lengths.</p>
<ul>
<li>
<p>Shape: {class: {id: {snr: strain} } }</p>
</li>
<li>
<p>The 'class' key is the name of the class, a string which must exist
in the 'classes' list.</p>
</li>
<li>
<p>The 'id' is a unique identifier for each strain, and must exist in
the index of the 'metadata' (DataFrame) attribute.</p>
</li>
<li>
<p>Extra depths can be added as variations of each strain, such as
polarizations. However they should be added between the 'id' and
the 'snr' layer!</p>
</li>
<li>
<p>The 'snr' key is an integer indicating the signal-to-noise ratio of
the injection.</p>
</li>
</ul>
</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>dict</code></dt>
<dd>Indices of the class of each wave ID, inherited from a clean
Class(Base) instance, with shape {id: class_index}.
Each ID points to the index of its class in the 'classes' attribute.</dd>
<dt><strong><code>units</code></strong> :&ensp;<code>str</code></dt>
<dd>Flag indicating whether the data is in 'geometrized' or 'IS' units.</dd>
<dt><strong><code>times</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Time samples associated with the strains, following the same structure.
Useful when the sampling rate is variable or different between strains.
If None, all strains are assumed to be constantly sampled to the
sampling rate indicated by the 'sample_rate' attribute.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Inherited from the parent Class(Base) instance.</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the longest strain in the dataset.
Remember to update it if manually changing strains' length.</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Value passed to 'sklearn.model_selection.train_test_split' to generate
the Train and Test subsets. Saved for reproducibility purposes.
Also used to initialize Numpy's default RandomGenerator.</dd>
<dt><strong><code>rng</code></strong> :&ensp;<code>np.random.Generator</code></dt>
<dd>Random number generator used for sampling the background noise.
Initialized with <code>np.random.default_rng(random_seed)</code>.</dd>
<dt><strong><code>detector</code></strong> :&ensp;<code>str</code></dt>
<dd>GW detector name.</dd>
<dt><strong><code>psd_</code></strong> :&ensp;<code>NDArray</code></dt>
<dd>Numerical representation of the Power Spectral Density (PSD) of the
detector's sensitivity.</dd>
<dt><strong><code>asd_</code></strong> :&ensp;<code>NDArray</code></dt>
<dd>Numerical representation of the Amplitude Spectral Density (ASD) of the
detector's sensitivity.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>gwadama.ioo.NonwhiteGaussianNoise</code></dt>
<dd>Background noise instance from NonwhiteGaussianNoise.</dd>
<dt><strong><code>snr_list</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>pad</code></strong> :&ensp;<code>dict</code></dt>
<dd>Padding introduced at each SNR injection, used in case the strains will
be whitened after, to remove the vigneting at edges.
It is associated to SNR values because the only implemented way to
pad the signals is during the signal injection.</dd>
<dt><strong><code>whitened</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flat indicating whether the dataset has been whitened. Initially will
be set to False, and changed to True after calling the 'whiten' method.
Once whitened, this flag will remain True, since the whitening is
implemented to be irreversible instance-wise.</dd>
<dt><strong><code>whiten_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>TODO</p>
<p>freq_cutoff : int | float
Frequency cutoff below which no noise bins will be generated in the
frequency space, and also used for the high-pass filter applied to
clean signals before injection.</p>
<p>freq_butter_order : int
Butterworth filter order.
See (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html</a>)
for more information.</p>
</dd>
<dt><strong><code>Xtrain</code></strong>, <strong><code>Xtest</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Train and test subsets randomly split using SKLearn train_test_split
function with stratified labels.
Shape adds the SNR layer: {id: {snr: strain}}.
The 'id' corresponds to the strain's index at 'self.metadata'.</dd>
<dt><strong><code>Ytrain</code></strong>, <strong><code>Ytest</code></strong> :&ensp;<code>NDArray[int]</code>, optional</dt>
<dd>
<p>1D Array containing the labels in the same order as 'Xtrain' and
'Xtest' respectively.</p>
<p>NOTE: Does not include the SNR layer, therefore labels are not repeated.</p>
</dd>
</dl>
<p>Base constructor for injected datasets.</p>
<p>TODO: Update docstring.</p>
<p>When inheriting from this class, it is recommended to run this method
first in your <strong>init</strong> function.</p>
<p>Relevant attributes are inherited from the 'clean_dataset' instance,
which can be any inherited from BaseDataset whose strains have not
been injected yet.</p>
<p>If train/test subsets are present, they too are updated when performing
injections or changing units, but only through re-building them from
the main 'strains' attribute using the already generated indices.
Original train/test subsets from the clean dataset are not inherited.</p>
<dl>
<dt><strong><code>WARNING</code></strong> :&ensp;<code>Initializing this class does not perform the injections! For</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>that use the method 'gen_injections'.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clean_dataset</code></strong> :&ensp;<code><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></code></dt>
<dd>Instance of a Class(Base) with noiseless signals.</dd>
<dt><strong><code>psd</code></strong> :&ensp;<code>np.ndarray | Callable</code></dt>
<dd>
<p>Power Spectral Density of the detector's sensitivity in the range
of frequencies of interest. Can be given as a callable function
whose argument is expected to be an array of frequencies, or as a
2d-array with shape (2, psd_length) so that</p>
<p><code>psd[0] = frequency_samples
psd[1] = psd_samples</code>.</p>
<p>NOTE: It is also used to compute the 'asd' attribute (ASD).</p>
</dd>
<dt><strong><code>detector</code></strong> :&ensp;<code>str</code></dt>
<dd>GW detector name.
Not used, just for identification.</dd>
<dt><strong><code>noise_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the background noise array to be generated for later use.
It should be at least longer than the longest signal expected to be
injected.</dd>
<dt><strong><code>whiten_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>Parameters of the whitening filter, with the following entries:</p>
<ul>
<li>
<p>'flength' : int
Length (in samples) of the time-domain FIR whitening.</p>
</li>
<li>
<p>'highpass' : float
Frequency cutoff.</p>
</li>
<li>
<p>'normed' : bool
Normalization applied after the whitening filter.</p>
</li>
</ul>
</dd>
<dt><strong><code>freq_cutoff</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Frequency cutoff below which no noise bins will be generated in the
frequency space, and also used for the high-pass filter applied to
clean signals before injection.</dd>
<dt><strong><code>freq_butter_order</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Butterworth filter order.
See (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html</a>)
for more information.</dd>
<dt><strong><code>flength</code></strong> :&ensp;<code>int</code></dt>
<dd>Length (in samples) of the time-domain FIR whitening filter.</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Value passed to 'sklearn.model_selection.train_test_split' to
generate the Train and Test subsets.
Saved for reproducibility purposes, and also used to initialize
Numpy's default RandomGenerator.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseInjected(Base):
    &#34;&#34;&#34;Manage an injected dataset with multiple SNR values.

    It is designed to store strains as nested dictionaries, with each level&#39;s
    key identifying a class/property of the strain. Each individual strain is a
    1D NDArray containing the features.

    NOTE: Instances of this class or any other Class(BaseInjected) are
    initialized from an instance of any Class(Base) instance (clean dataset).
    
    By default there are THREE basic levels:
        
        - Class; to group up strains in categories.
        
        - Id; An unique identifier for each strain, which must exist in the
          metadata DataFrame as Index.
        
        - SNR; the signal-to-noise ratio at which has been injected w.r.t. a
          power spectral density of reference (e.g. the sensitivity of a GW
          detector).
    
    Extra depths can be added, and will be thought of as modifications of the
    same original strains from the upper identifier level. However they should
    be added between the &#39;Id&#39; and &#39;SNR&#39; layer, since the SNR is the final
    realization of any variations made of a given (Class, Id) signal.


    Attributes
    ----------
    classes : list[str]
        List of labels, one per class (category).
    
    metadata : pandas.DataFrame
        All parameters and data related to the original strains, inherited
        (copied) from a clean Class(Base) instance.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the ID).
        The total number of different waves must be equal to `len(metadata)`;
        this does not include possible variations such polarizations or
        multiple scallings of the same waveform when performing injections.
    
    strains_clean : dict[dict]
        Strains inherited (copied) from a clean Class(Base) instance.
        This copy is kept in order to perform new injections.
        
        - Shape: {class: {id: strain} }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        NOTE: These strains should be not modified. If new clean strains are
        needed, create a new clean dataset instance first, and then initialise
        this class with it.
    
    strains : dict[dict]
        Injected trains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        
        - Shape: {class: {id: {snr: strain} } }
        
        - The &#39;class&#39; key is the name of the class, a string which must exist
          in the &#39;classes&#39; list.
        
        - The &#39;id&#39; is a unique identifier for each strain, and must exist in
          the index of the &#39;metadata&#39; (DataFrame) attribute.
        
        - Extra depths can be added as variations of each strain, such as
          polarizations. However they should be added between the &#39;id&#39; and
          the &#39;snr&#39; layer!
        
        - The &#39;snr&#39; key is an integer indicating the signal-to-noise ratio of
          the injection.
        
    labels : dict
        Indices of the class of each wave ID, inherited from a clean
        Class(Base) instance, with shape {id: class_index}.
        Each ID points to the index of its class in the &#39;classes&#39; attribute.
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.

    times : dict, optional
        Time samples associated with the strains, following the same structure.
        Useful when the sampling rate is variable or different between strains.
        If None, all strains are assumed to be constantly sampled to the
        sampling rate indicated by the &#39;sample_rate&#39; attribute.
    
    sample_rate : int
        Inherited from the parent Class(Base) instance.
    
    max_length : int
        Length of the longest strain in the dataset.
        Remember to update it if manually changing strains&#39; length.
    
    random_seed : int
        Value passed to &#39;sklearn.model_selection.train_test_split&#39; to generate
        the Train and Test subsets. Saved for reproducibility purposes.
        Also used to initialize Numpy&#39;s default RandomGenerator.

    rng : np.random.Generator
        Random number generator used for sampling the background noise.
        Initialized with `np.random.default_rng(random_seed)`.

    detector : str
        GW detector name.

    psd_ : NDArray
        Numerical representation of the Power Spectral Density (PSD) of the
        detector&#39;s sensitivity.
    
    asd_ : NDArray
        Numerical representation of the Amplitude Spectral Density (ASD) of the
        detector&#39;s sensitivity.

    noise : gwadama.ioo.NonwhiteGaussianNoise
        Background noise instance from NonwhiteGaussianNoise.

    snr_list : list

    pad : dict
        Padding introduced at each SNR injection, used in case the strains will
        be whitened after, to remove the vigneting at edges.
        It is associated to SNR values because the only implemented way to
        pad the signals is during the signal injection.
    
    whitened : bool
        Flat indicating whether the dataset has been whitened. Initially will
        be set to False, and changed to True after calling the &#39;whiten&#39; method.
        Once whitened, this flag will remain True, since the whitening is
        implemented to be irreversible instance-wise.
    
    whiten_params : dict
        TODO

        freq_cutoff : int | float
            Frequency cutoff below which no noise bins will be generated in the
            frequency space, and also used for the high-pass filter applied to
            clean signals before injection.

        freq_butter_order : int
            Butterworth filter order.
            See (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html)
            for more information.

    Xtrain, Xtest : dict, optional
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        Shape adds the SNR layer: {id: {snr: strain}}.
        The &#39;id&#39; corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
    
    Ytrain, Ytest : NDArray[int], optional
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.
        
        NOTE: Does not include the SNR layer, therefore labels are not repeated.

    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: Base,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 whiten_params: dict,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        &#34;&#34;&#34;Base constructor for injected datasets.

        TODO: Update docstring.

        When inheriting from this class, it is recommended to run this method
        first in your __init__ function.

        Relevant attributes are inherited from the &#39;clean_dataset&#39; instance,
        which can be any inherited from BaseDataset whose strains have not
        been injected yet.

        If train/test subsets are present, they too are updated when performing
        injections or changing units, but only through re-building them from
        the main &#39;strains&#39; attribute using the already generated indices.
        Original train/test subsets from the clean dataset are not inherited.
        
        WARNING: Initializing this class does not perform the injections! For
        that use the method &#39;gen_injections&#39;.

        Parameters
        ----------
        clean_dataset : Base
            Instance of a Class(Base) with noiseless signals.

        psd : np.ndarray | Callable
            Power Spectral Density of the detector&#39;s sensitivity in the range
            of frequencies of interest. Can be given as a callable function
            whose argument is expected to be an array of frequencies, or as a
            2d-array with shape (2, psd_length) so that
            
            ```
            psd[0] = frequency_samples
            psd[1] = psd_samples
            ```.
            
            NOTE: It is also used to compute the &#39;asd&#39; attribute (ASD).

        detector : str
            GW detector name.
            Not used, just for identification.

        noise_length : int
            Length of the background noise array to be generated for later use.
            It should be at least longer than the longest signal expected to be
            injected.
        
        whiten_params : dict
            Parameters of the whitening filter, with the following entries:
            
            - &#39;flength&#39; : int
                Length (in samples) of the time-domain FIR whitening.
            
            - &#39;highpass&#39; : float
                Frequency cutoff.
            
            - &#39;normed&#39; : bool
                Normalization applied after the whitening filter.

        freq_cutoff : int | float
            Frequency cutoff below which no noise bins will be generated in the
            frequency space, and also used for the high-pass filter applied to
            clean signals before injection.

        freq_butter_order : int | float
            Butterworth filter order.
            See (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html)
            for more information.
        
        flength : int
            Length (in samples) of the time-domain FIR whitening filter.

        random_seed : int
            Value passed to &#39;sklearn.model_selection.train_test_split&#39; to
            generate the Train and Test subsets.
            Saved for reproducibility purposes, and also used to initialize
            Numpy&#39;s default RandomGenerator.
        
        &#34;&#34;&#34;
        # Inherit clean strain instance attributes.
        #----------------------------------------------------------------------

        self.classes = clean_dataset.classes.copy()
        self._check_classes_dict(self.classes)
        self.labels = clean_dataset.labels.copy()
        self.metadata = deepcopy(clean_dataset.metadata)
        self.strains_clean = deepcopy(clean_dataset.strains)
        self._track_times = clean_dataset._track_times
        if self._track_times:
            self.times = deepcopy(clean_dataset.times)
        self.sample_rate = clean_dataset.sample_rate
        self.max_length = clean_dataset.max_length

        # Noise instance and related attributes.
        #----------------------------------------------------------------------

        self.random_seed = random_seed
        self.rng = np.random.default_rng(random_seed)
        self.detector = detector
        # Highpass parameters applied when generating the noise array.
        self.freq_cutoff = freq_cutoff
        self.freq_butter_order = freq_butter_order
    
        self._psd, self.psd_array = self._setup_psd(psd)
        self._asd, self.asd_array = self._setup_asd_from_psd(psd)
        self.noise = self._generate_background_noise(noise_length)

        # Injection related:
        #----------------------------------------------------------------------

        # TODO: Â¿Implement the case when clean_dataset is already whitened?
        # It should mark it and use the clean copy of nonwhitened data instead.

        self.strains = None
        self._dict_depth = clean_dataset._dict_depth + 1  # Depth of the strains dict.
        self.snr_list = []
        self.pad = {}  # {snr: pad}
        self.whitened = False  # Switched to True after calling self.whiten().
        self.whiten_params = whiten_params
        # NOTE: I designed this while building the InjectedCoReWaves class, so
        # chances are this is not general enough.
        self.whiten_params.update({
            &#39;asd_array&#39;: self.asd_array,  # Referenced here again for consistency.
            &#39;pad&#39;: 0,  # Signals are expected to be already padded.
            &#39;unpad&#39;: self.pad,  # Referenced here again for consistency.
            &#39;highpass&#39;: self.freq_cutoff  # Referenced here again for consistency.
        })

        # Train/Test subset views:
        #----------------------------------------------------------------------

        if clean_dataset.Xtrain is not None:
            self.Xtrain = {k: None for k in clean_dataset.Xtrain.keys()}
            self.Xtest = {k: None for k in clean_dataset.Xtest.keys()}
            self.Ytrain = clean_dataset.Ytrain
            self.Ytest = clean_dataset.Ytest
        else:
            self.Xtrain = None
            self.Xtest = None
            self.Ytrain = None
            self.Ytest = None
    
    def __getstate__(self):
        &#34;&#34;&#34;Avoid error when trying to pickle PSD and ASD interpolants.
        
        Turns out Pickle tries to serialize the PSD and ASD interpolants,
        however Pickle is not able to serialize encapsulated functions.
        This is solved by removing said functions and computing the
        interpolants from their array representations when unpickling.

        NOTE: The loss of accuracy over repeated (de)serialization using this
        method has not been studied, use at your own discretion.
        
        &#34;&#34;&#34;
        state = self.__dict__.copy()
        del state[&#39;_psd&#39;]
        del state[&#39;_asd&#39;]
        
        return state
    
    def __setstate__(self, state):
        &#34;&#34;&#34;Avoid error when trying to unpickle PSD and ASD interpolants.
        
        Turns out Pickle tries to serialize the PSD and ASD interpolants,
        however Pickle is not able to serialize encapsulated functions.
        This is solved by removing said functions and computing the
        interpolants from their array representations when unpickling.
        
        NOTE: The loss of accuracy over repeated (de)serialization using this
        method has not been studied, use at your own discretion.
        
        &#34;&#34;&#34;
        _psd, _ = self._setup_psd(state[&#39;psd_array&#39;])
        _asd, _ = self._setup_asd_from_psd(state[&#39;psd_array&#39;])
        state[&#39;_psd&#39;] = _psd
        state[&#39;_asd&#39;] = _asd
        self.__dict__.update(state)
    
    def _setup_psd(self, psd: np.ndarray | Callable) -&gt; tuple[Callable, np.ndarray]:
        &#34;&#34;&#34;Setup the PSD function or array depending on the input.
        
        Setup the power spectral density function and array from any of those.
        
        &#34;&#34;&#34;
        if callable(psd):
            psd_fun = psd
            # Compute a realization of the PSD function with 16 bins per
            # integer frequency to ensure the numerical representation has
            # enough precision.
            freqs = np.linspace(0, self.sample_rate//2, self.sample_rate*8)
            psd_array = np.stack([freqs, psd(freqs)])
        
        elif isinstance(psd, np.ndarray):
            # Build a spline quadratic interpolant for the input PSD array.
            psd_fun = sp_make_interp_spline(psd[0], psd[1], k=2)
            psd_array = np.asarray(psd)
        
        else:
            raise TypeError(&#34;&#39;psd&#39; type not recognized&#34;)
            
        return psd_fun, psd_array

    def _setup_asd_from_psd(self, psd):
        &#34;&#34;&#34;Setup the ASD function or array depending on the input.
        
        Setup the amplitude spectral density function and array from any of
        those.
        
        &#34;&#34;&#34;
        if callable(psd):
            asd_fun = lambda f: np.sqrt(psd)
            # Compute a realization of the ASD function with 16 bins per
            # integer frequency to ensure the numerical representation has
            # enough precision.
            freqs = np.linspace(0, self.sample_rate//2, self.sample_rate*8)
            asd_array = np.stack([freqs, asd_fun(freqs)])
        
        elif isinstance(psd, np.ndarray):
            # Build a spline quadratic interpolant for the input ASD array.
            asd_array = psd.copy()
            asd_array[1] = np.sqrt(psd[1])
            asd_fun = sp_make_interp_spline(asd_array[0], asd_array[1], k=2)
        
        else:
            raise TypeError(&#34;&#39;psd&#39; type not recognized&#34;)
            
        return asd_fun, asd_array

    def psd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
        &#34;&#34;&#34;Power spectral density (PSD) of the detector at given frequencies.

        Interpolates the PSD at the given frequencies from their array
        representation. If during initialization the PSD was given as its
        array representation, the interpolant is computed using SciPy&#39;s
        quadratic spline interpolant function.

        &#34;&#34;&#34;
        return self._psd(frequencies)

    def asd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
        &#34;&#34;&#34;Amplitude spectral density (ASD) of the detector at given frequencies.

        Interpolates the ASD at the given frequencies from their array
        representation. If during initialization the ASD was given as its
        array representation, the interpolant is computed using SciPy&#39;s
        quadratic spline interpolant function.

        &#34;&#34;&#34;
        return self._asd(frequencies)
    
    def _generate_background_noise(self, noise_length: int) -&gt; synthetic.NonwhiteGaussianNoise:
        &#34;&#34;&#34;The noise realization is generated by NonwhiteGaussianNoise.&#34;&#34;&#34;

        d: float = noise_length / self.sample_rate
        noise = synthetic.NonwhiteGaussianNoise(
            duration=d, psd=self.psd, sample_rate=self.sample_rate,
            rng=self.rng, freq_cutoff=self.freq_cutoff
        )

        return noise
    
    def _init_strains_dict(self) -&gt; dict[dict[dict]]:
        &#34;&#34;&#34;Initializes the nested dictionary of strains.
        
        Initializes the nested dictionary of strains following the hierarchy
        in the clean strains attribute, and adding the (lowest) SNR layer.
        
        &#34;&#34;&#34;
        strains_dict = dictools._replicate_structure_nested_dict(self.strains_clean)
        for indices in dictools._unroll_nested_dictionary_keys(strains_dict):
            dictools.set_value_to_nested_dict(strains_dict, indices, {})

        return strains_dict
    
    def get_times(self, *indices) -&gt; np.ndarray:
        &#34;&#34;&#34;Get a single time array from the complete index coordinates.
        
        This is just a shortcut to avoid having to write several squared
        brackets.

        NOTE: The returned strain is not a copy; if its contents are modified,
        the changes will be reflected inside the &#39;times&#39; attribute.
        
        &#34;&#34;&#34;
        return dictools._get_value_from_nested_dict(self.times, indices)
    
    def gen_injections(self, snr: int|float|list, pad: int = 0):
        &#34;&#34;&#34;Inject all strains in simulated noise with the given SNR values.

        
        - The SNR is computed using a matched filter against the noise PSD.
        
        - If `pad &gt; 0`, it also updates the time arrays.
        
        - If strain units are in geometrized, they will be converted first to
          IS, injected, and converted back to geometrized.
        
        - After each injection, applies a highpass filter at the low-cut
          frequency specified at __init__.
        
        - If the method &#39;whiten&#39; has been already called, all further
          injections will automatically be whitened and their pad removed.
        
        Parameters
        ----------
        snr : int | float | list
        
        pad : int
            Number of zeros to pad the signal at both ends before the
            injection.
        
        Notes
        -----
        - If whitening is intended to be applied afterwards it is useful to
          pad the signal in order to avoid the window vignetting produced by
          the whitening itself. This pad will be cropped afterwards.
        
        - New injections are stored at the &#39;strains&#39; atrribute, with the pad
          associated to all the injections performed at once. Even when
          whitening is also performed right after the injections.
        
        Raises
        ------
        ValueError
            Once injections have been performed at a certain SNR value, there
            cannot be injected again at the same value. Trying it will trigger
            this exception.
        
        &#34;&#34;&#34;
        if isinstance(snr, (int, float)):
            snr_list = list(snr)
        elif isinstance(snr, list):
            snr_list = snr
        else:
            raise TypeError(f&#34;&#39;{type(snr)}&#39; is not a valid &#39;snr&#39; type&#34;)
        
        if set(snr_list) &amp; set(self.snr_list):
            raise ValueError(&#34;one or more SNR values are already present in the dataset&#34;)

        if self._track_times:
            # Replaced temporarily because when injecting for the first time
            # we need to keep the original time arrays.
            times_new = self.times

        # 1st time making injections.
        if self.strains is None:
            self.strains = self._init_strains_dict()
            if self._track_times:
                # Redo the dictionary structure to include the SNR layer.
                times_new = self._init_strains_dict()
        
        for clas, id_ in dictools._unroll_nested_dictionary_keys(self.strains_clean):
            gw_clean = self.strains_clean[clas][id_]
            strain_clean_padded = np.pad(gw_clean, pad)
            # NOTE: Do not update the metadata nor times with this pad in case
            # the whitening is applied immediately after the injections.

            # Highpass filter to the clean signal.
            # NOTE: The noise realization is already generated without
            # frequency components lower than the cutoff (they are set to
            # 0 during the random sampling).
            strain_clean_padded = self.noise.highpass_filter(
                strain_clean_padded, f_cut=self.freq_cutoff, f_order=self.freq_butter_order
            )

            
            # Strain injections
            for snr_ in snr_list:
                # &#39;pad&#39; is added to &#39;snr_offset&#39; to compensate for the padding
                # which has not been updated in the &#39;metadata&#39; yet.
                injected = self._inject(
                    strain_clean_padded, snr_, id=id_, snr_offset=pad
                )
                if self.whitened:
                    injected = fat.whiten(
                        injected, asd=self.asd_array, unpad=pad, sample_rate=self.sample_rate,
                        # Parameters for GWpy&#39;s whiten() function:
                        highpass=self.freq_cutoff, flength=self.flength
                    )
                self.strains[clas][id_][snr_] = injected
            
            # Time arrays:
            # - All SNR entries pointing to the SAME time array.
            # - Enlarge if the strains were padded and no whitening followed.
            if self._track_times:
                times_i = self.get_times(clas, id_)
                if pad &gt; 0 and not self.whitened:
                    times_i = tat.pad_time_array(times_i, pad)
                for snr_ in snr_list:
                    times_new[clas][id_][snr_] = times_i
        
        if self._track_times:
            self.times = times_new
        
        # Record new SNR values and related padding.
        # NOTE: Even if whitening is applied (and hence the length unaltered)
        # pad values are still registered, just in case.
        self.snr_list += snr_list
        for snr_ in snr_list:
            self.pad[snr_] = pad
        
        # Side-effect attributes updated.
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def _inject(self, strain: np.ndarray, snr: int|float, **_) -&gt; np.ndarray:
        &#34;&#34;&#34;Inject &#39;strain&#39; at &#39;snr&#39; into noise using the &#39;self.noise&#39; instance.

        NOTE: This is writen as an independent method to allow for other
        classes inheriting this to modify its behaviour without having to
        rewrite the entire &#39;gen_injections&#39; method.

        Parameters
        ----------
        strain : NDArray
            Signal to be injected into noise.
        
        snr : int | float
            Signal to noise ratio.
        
        Returns
        -------
        injected : NDArray
            Injected signal.
        
        &#34;&#34;&#34;
        injected, _ = self.noise.inject(strain, snr=snr)

        return injected
    
    def export_strains_to_gwf(self,
                              path: str,
                              channel: str,  # Name of the channel in which to save strains in the GWFs.
                              t0_gps: float = 0,
                              verbose=False) -&gt; None:
        &#34;&#34;&#34;Export all strains to GWF format, one file per strain.&#34;&#34;&#34;

        from pathlib import Path

        for indices in self.keys():
            strain = self.get_strain(*indices)
            times = self.get_times(*indices)
            ts = TimeSeries(
                data=strain,
                times=t0_gps + times,
                channel=channel
            )
            key = indices[1].replace(&#39;:&#39;, &#39;_&#39;) + &#39;_snr&#39; + str(indices[2])
            fields = [
                self.detector,
                key,
                str(int(t0_gps)),
                str(int(ts.duration.value * 1000))  # In milliseconds
            ]
            file = Path(path) / (&#39;-&#39;.join(fields) + &#39;.gwf&#39;)
            ts.write(file)

            if verbose:
                print(&#34;Strain exported to&#34;, file)
    
    def whiten(self):
        &#34;&#34;&#34;Whiten injected strains.
        
        Calling this method performs the whitening of all injected strains.
        Strains are later cut to their original size before adding the pad,
        to remove the vigneting.
        
        NOTE: This is an irreversible action; if the original injections need
        to be preserved it is advised to make a copy of the instance before
        performing the whitening.
        
        &#34;&#34;&#34;
        if self.whitened:
            raise RuntimeError(&#34;dataset already whitened&#34;)

        if self.strains is None:
            raise RuntimeError(&#34;no injections have been performed yet&#34;)

        flength = self.whiten_params[&#39;flength&#39;]
        asd_array = self.whiten_params[&#39;asd_array&#39;]
        pad = self.whiten_params[&#39;pad&#39;]
        unpad = self.whiten_params[&#39;unpad&#39;]
        highpass = self.whiten_params[&#39;highpass&#39;]
        
        for *keys, strain in self.items():
            snr = keys[-1]  # Shape of self.strains dict-&gt; {class: {id: {snr: strain}}}

            # NOTE: I designed this while building the InjectedCoReWaves class,
            # so chances are the parameters here are not passed in the most
            # generalized way.
            strain_w = fat.whiten(
                strain, asd=asd_array, pad=pad, unpad=unpad[snr], sample_rate=self.sample_rate,
                highpass=highpass, flength=flength
            )
            # Update strains attribute.
            dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
        
        # Shrink time arrays accordingly.
        if self._track_times:
            key_layers = dictools._unroll_nested_dictionary_keys(
                self.strains,
                max_depth=self._dict_depth-1  # same signal at different SNR has same time points.
            )
            for keys in key_layers:
                # Since all time arrays below SNR layer are the same, get the first one:
                times_i = dictools._get_value_from_nested_dict(self.times, keys)
                snr0 = next(iter(times_i.keys()))
                times_i = times_i[snr0]
                times_i = tat.shrink_time_array(times_i, unpad[snr0])
                for snr in self.snr_list:
                    keys_all = keys + [snr]
                    dictools.set_value_to_nested_dict(self.times, keys_all, times_i)
        
        self.whitened = True
        
        # Side-effect attributes updated.
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def get_xtrain_array(self,
                         length: int = None,
                         classes: str | list = &#39;all&#39;,
                         snr: int | list | str = &#39;all&#39;,
                         with_metadata: bool = False):
        &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the train subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Allows the possibility to filter by class and SNR.

        NOTE: Same signals injected at different SNR are stacked continuously.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;train_array&#39;. If None, the longest signal
            determines the length.

        classes : str | list[str]
            Classes which to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            SNR injections which to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                 ...
            ```
            
            All injections are included by default.
        
        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            train array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with an
            additional column for the SNR values.
            False by default.

        Returns
        -------
        train_array : np.ndarray
            Train subset.
        
        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;train_array&#39;.
        
        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;train_array&#39;.

        &#34;&#34;&#34;
        return self._stack_subset(self.Xtrain, length, classes, snr, with_metadata)
    
    def get_xtest_array(self,
                        length: int = None,
                        classes: str | list = &#39;all&#39;,
                        snr: int | list | str = &#39;all&#39;,
                        with_metadata: bool = False):
        &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

        Stacks all signals in the test subset into an homogeneous numpy array
        whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
        the longest strain in the subset. The remaining space is zeroed.

        Allows the possibility to filter by class and SNR.

        NOTE: Same signals injected at different SNR are stacked continuously.

        Parameters
        ----------
        length : int, optional
            Target length of the &#39;test_array&#39;. If None, the longest signal
            determines the length.

        classes : str | list[str]
            Classes which to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            SNR injections which to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                 ...
            ```
            
            All injections are included by default.

        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            test array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with an
            additional column for the SNR values.
            False by default.

        Returns
        -------
        test_array : np.ndarray
            Test subset.

        lengths : list
            Original length of each strain, following the same order as the
            first axis of &#39;test_array&#39;.

        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;test_array&#39;.
        
        &#34;&#34;&#34;
        return self._stack_subset(self.Xtest, length, classes, snr, with_metadata)

    def _stack_subset(self,
                      strains: dict,
                      length:  int = None,
                      classes: str | list = &#39;all&#39;,
                      snr: int | list | str = &#39;all&#39;,
                      with_metadata: bool = False):
        &#34;&#34;&#34;Stack a subset of strains into a zero-padded 2d-array.

        This is a helper function for &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;.

        Parameters
        ----------
        strains : dict
            A dictionary containing the strains to be stacked.
            The keys of the first layer are the IDs of the strains.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        classes : str | list[str]
            The classes to include in the stack.
            All classes are included by default.

        snr : int | list[int] | str
            The SNR injections to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            eos0 id0 snr0
            eos0 id0 snr1
                ...
            ```
            
            All injections are included by default.

        with_metadata : bool
            If True, the associated metadata is returned in addition to the
            stacked array in a Pandas DataFrame instance.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with
            an additional column for the SNR values.
            False by default.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        metadata : pd.DataFrame, optional
            If &#39;with_metadata&#39; is True, the associated metadata is returned
            with its entries in the same order as the &#39;stacked_signals&#39;.
            This metadata is obtained from the original &#39;metadata&#39; attribute,
            with the former index inserted as the first column, &#39;id&#39;, and with
            an additional column for the SNR values.

        Raises
        ------
        ValueError
            If the value of &#39;classes&#39; or &#39;snr&#39; is not valid.

        &#34;&#34;&#34;
        if isinstance(classes, (str, list)) and classes != &#39;all&#39;:
            if isinstance(classes, str):
                classes = [classes]

            # NOTE: Here there is no &#39;class&#39; layer, therefore it must be
            # traced back from the ID, and filtered over this same layer.
            def filter_class(id):
                clas = self.find_class(id)
                return clas in classes
            strains = dictools.filter_nested_dict(strains, filter_class, layer=0)

        elif classes != &#39;all&#39;:
            raise TypeError(&#34;the type of &#39;classes&#39; is not valid&#34;)


        if isinstance(snr, (int, list)):
            if isinstance(snr, int):
                snr = [snr]

            # NOTE: Here SNR is in Layer 1 because the Train/Test subset
            # dictionaries do not have the &#39;class&#39; first layer.
            strains = dictools.filter_nested_dict(strains, lambda k: k in snr, layer=1)

        # If `snr == &#39;all&#39;`, no filter is applied over &#39;strains&#39;.
        elif isinstance(snr, str) and snr != &#39;all&#39;:
            raise ValueError(&#34;the value of &#39;snr&#39; is not valid&#34;)
        
        else:
            raise TypeError(&#34;the type of &#39;snr&#39; is not valid&#34;)
        

        strains = dictools.flatten_nested_dict(strains)

        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        if with_metadata:
            id_list = [k[0] for k in strains]
            snr_list = [k[1] for k in strains]
            metadata = self.metadata.loc[id_list]
            metadata.reset_index(inplace=True, names=&#39;id&#39;)
            metadata.insert(1, &#39;snr&#39;, snr_list)  # after &#39;id&#39;.
            
            return stacked_signals, lengths, metadata
        return stacked_signals, lengths

    def get_ytrain_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered training labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related GLOBAL indices w.r.t. the stacked
            arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
            False by default.

        Returns
        -------
        np.ndarray
            Filtered train labels.

        np.ndarray, optional
            IDs associated to the filtered train labels.
        
        np.ndarray, optional
            Indices associated to the filtered train labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytrain, list(self.Xtrain), classes, snr,
            with_id=with_id, with_index=with_index
        )

    def get_ytest_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
        &#34;&#34;&#34;Get the filtered test labels.

        Parameters
        ----------
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related GLOBAL indices w.r.t. the stacked
            arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

        Returns
        -------
        np.ndarray
            Filtered test labels.

        np.ndarray, optional
            IDs associated to the filtered test labels.
        
        np.ndarray, optional
            Indices associated to the filtered test labels.

        &#34;&#34;&#34;
        return self._filter_labels(
            self.Ytest, list(self.Xtest), classes, snr,
            with_id=with_id, with_index=with_index
        )
    
    def _filter_labels(self, labels, labels_id, classes, snr, with_id=False, with_index=False):
        &#34;&#34;&#34;Filter labels based on &#39;classes&#39; and &#39;snr&#39;.

        This is a helper function for &#39;get_ytrain_array&#39; and &#39;get_ytest_array&#39;.
        
        Parameters
        ----------
        labels : np.ndarray
            The array containing the labels.

        labels_id : list
            IDs associated to the labels.
        
        classes : str | list[str] | &#39;all&#39;
            The classes to include in the labels.
            All classes are included by default.
        
        snr : int | list[int] | str
            The SNR injections to include in the labels.
            All injections are included by default.

        with_id : bool
            If True, return also the related IDs.
            False by default.

        with_index : bool
            If True, return also the related indices w.r.t. the stacked array
            returned by &#39;_stack_subset&#39; given the strains related to &#39;labels&#39;
            WITHOUT filters.
            False by default.

        Returns
        -------
        filtered_labels : np.ndarray
            Filtered labels.

        filtered_ids : np.ndarray, optional
            IDs associated to the filtered labels.

        filtered_indices : np.ndarray, optional
            Indices associated to the filtered labels.

        &#34;&#34;&#34;
        # First get labels and IDs filtered by &#39;classes&#39;.
        filtered_labels, filtered_ids, filtered_indices = super()._filter_labels(
            labels, labels_id, classes, with_id=True, with_index=True
        )

        if isinstance(snr, str):
            if snr != &#39;all&#39;:
                raise ValueError(&#34;only the str &#39;all&#39; is allowed for &#39;snr&#39;.&#34;)
        elif isinstance(snr, int):
            snr = [snr]
        elif not isinstance(snr, list):
            raise TypeError(&#34;the type of &#39;snr&#39; is not valid&#34;)
        
        n_snr_total = len(self.snr_list)

        # Next repeat all by the total number of SNR values.
        filtered_labels = np.repeat(filtered_labels, n_snr_total)
        filtered_ids = np.repeat(filtered_ids, n_snr_total)
        filtered_indices = np.repeat(filtered_indices, n_snr_total)

        n_filtered = len(filtered_labels)

        # Then convert the indices to include the TOTAL number of SNR repetitions.
        for i in range(0, n_filtered, n_snr_total):
            i_old = filtered_indices[i]
            i_new0 = i_old * n_snr_total
            i_new1 = i_new0 + n_snr_total
            filtered_indices[i:i+n_snr_total] = np.arange(i_new0, i_new1)

        if snr != &#39;all&#39;:
            # Finally filter out those not present in the &#39;snr&#39; list.
            mask = np.isin(self.snr_list, snr)
            mask = np.tile(mask, n_filtered//n_snr_total)
            filtered_labels = filtered_labels[mask]
            filtered_ids = filtered_ids[mask]
            filtered_indices = filtered_indices[mask]

        if with_id and with_index:
            return filtered_labels, filtered_ids, filtered_indices
        if with_id:
            return filtered_labels, filtered_ids
        if with_index:
            return filtered_labels, filtered_indices
        return filtered_labels

    def stack_by_id(self,
                    id_list: list,
                    length: int = None,
                    snr_included: int | list[int] | str = &#39;all&#39;):
        &#34;&#34;&#34;Stack a subset of strains by ID into a zero-padded 2d-array.

        This may allow (for example) to group up strains by their original ID
        without leaking differnet injections (SNR) of the same strain into
        different splits.

        Parameters
        ----------
        id_list : array-like
            The IDs of the strains to be stacked.

        length : int, optional
            The target length of the stacked array. If None, the longest signal
            determines the length.

        snr_included : int | list[int] | str, optional
            The SNR injections to include in the stack. If more than one are
            selected, they are stacked zipped as follows:
            
            ```
            id0 snr0
            id0 snr1
              ...
            ```
            
            All injections are included by default.

        Returns
        -------
        stacked_signals : np.ndarray
            The array containing the stacked strains.

        lengths : list
            The original lengths of each strain, following the same order as
            the first axis of &#39;stacked_signals&#39;.

        Notes
        -----
        - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
          not filter by &#39;classes&#39; since it would be redundant, as IDs are
          unique.

        Raises
        ------
        ValueError
            If the value of &#39;snr&#39; is not valid.

        &#34;&#34;&#34;
        if not isinstance(id_list, list):
            raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)
        
        # Collapse the Class layer.
        strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

        # Filter out those not in the &#39;id_list&#39;.
        strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)

        # Filter out those injections whose SNR isnot in the &#39;snr&#39; list.
        if isinstance(snr_included, (int, list)):
            if isinstance(snr_included, int):
                snr_included = [snr_included]
            # NOTE: Here SNR is in Layer 1 because we collapsed the Class layer.
            strains = dictools.filter_nested_dict(strains, lambda k: k in snr_included, layer=1)
        elif snr_included != &#39;all&#39;:
            raise ValueError(&#34;the value of &#39;snr&#39; is not valid&#34;)

        strains = dictools.flatten_nested_dict(strains)  # keys: &#34;(id, snr)&#34;
        stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
        
        return stacked_signals, lengths</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.InjectedCoReWaves" href="#gwadama.datasets.InjectedCoReWaves">InjectedCoReWaves</a></li>
<li><a title="gwadama.datasets.InjectedSyntheticWaves" href="#gwadama.datasets.InjectedSyntheticWaves">InjectedSyntheticWaves</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="gwadama.datasets.BaseInjected.asd"><code class="name flex">
<span>def <span class="ident">asd</span></span>(<span>self, frequencies:Â floatÂ |Â numpy.ndarray[float]) ->Â numpy.ndarray[float]</span>
</code></dt>
<dd>
<div class="desc"><p>Amplitude spectral density (ASD) of the detector at given frequencies.</p>
<p>Interpolates the ASD at the given frequencies from their array
representation. If during initialization the ASD was given as its
array representation, the interpolant is computed using SciPy's
quadratic spline interpolant function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def asd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
    &#34;&#34;&#34;Amplitude spectral density (ASD) of the detector at given frequencies.

    Interpolates the ASD at the given frequencies from their array
    representation. If during initialization the ASD was given as its
    array representation, the interpolant is computed using SciPy&#39;s
    quadratic spline interpolant function.

    &#34;&#34;&#34;
    return self._asd(frequencies)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.export_strains_to_gwf"><code class="name flex">
<span>def <span class="ident">export_strains_to_gwf</span></span>(<span>self, path:Â str, channel:Â str, t0_gps:Â floatÂ =Â 0, verbose=False) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Export all strains to GWF format, one file per strain.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_strains_to_gwf(self,
                          path: str,
                          channel: str,  # Name of the channel in which to save strains in the GWFs.
                          t0_gps: float = 0,
                          verbose=False) -&gt; None:
    &#34;&#34;&#34;Export all strains to GWF format, one file per strain.&#34;&#34;&#34;

    from pathlib import Path

    for indices in self.keys():
        strain = self.get_strain(*indices)
        times = self.get_times(*indices)
        ts = TimeSeries(
            data=strain,
            times=t0_gps + times,
            channel=channel
        )
        key = indices[1].replace(&#39;:&#39;, &#39;_&#39;) + &#39;_snr&#39; + str(indices[2])
        fields = [
            self.detector,
            key,
            str(int(t0_gps)),
            str(int(ts.duration.value * 1000))  # In milliseconds
        ]
        file = Path(path) / (&#39;-&#39;.join(fields) + &#39;.gwf&#39;)
        ts.write(file)

        if verbose:
            print(&#34;Strain exported to&#34;, file)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.gen_injections"><code class="name flex">
<span>def <span class="ident">gen_injections</span></span>(<span>self, snr:Â intÂ |Â floatÂ |Â list, pad:Â intÂ =Â 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Inject all strains in simulated noise with the given SNR values.</p>
<ul>
<li>
<p>The SNR is computed using a matched filter against the noise PSD.</p>
</li>
<li>
<p>If <code>pad &gt; 0</code>, it also updates the time arrays.</p>
</li>
<li>
<p>If strain units are in geometrized, they will be converted first to
IS, injected, and converted back to geometrized.</p>
</li>
<li>
<p>After each injection, applies a highpass filter at the low-cut
frequency specified at <strong>init</strong>.</p>
</li>
<li>
<p>If the method 'whiten' has been already called, all further
injections will automatically be whitened and their pad removed.</p>
</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>snr</code></strong> :&ensp;<code>int | float | list</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>pad</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of zeros to pad the signal at both ends before the
injection.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>
<p>If whitening is intended to be applied afterwards it is useful to
pad the signal in order to avoid the window vignetting produced by
the whitening itself. This pad will be cropped afterwards.</p>
</li>
<li>
<p>New injections are stored at the 'strains' atrribute, with the pad
associated to all the injections performed at once. Even when
whitening is also performed right after the injections.</p>
</li>
</ul>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Once injections have been performed at a certain SNR value, there
cannot be injected again at the same value. Trying it will trigger
this exception.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_injections(self, snr: int|float|list, pad: int = 0):
    &#34;&#34;&#34;Inject all strains in simulated noise with the given SNR values.

    
    - The SNR is computed using a matched filter against the noise PSD.
    
    - If `pad &gt; 0`, it also updates the time arrays.
    
    - If strain units are in geometrized, they will be converted first to
      IS, injected, and converted back to geometrized.
    
    - After each injection, applies a highpass filter at the low-cut
      frequency specified at __init__.
    
    - If the method &#39;whiten&#39; has been already called, all further
      injections will automatically be whitened and their pad removed.
    
    Parameters
    ----------
    snr : int | float | list
    
    pad : int
        Number of zeros to pad the signal at both ends before the
        injection.
    
    Notes
    -----
    - If whitening is intended to be applied afterwards it is useful to
      pad the signal in order to avoid the window vignetting produced by
      the whitening itself. This pad will be cropped afterwards.
    
    - New injections are stored at the &#39;strains&#39; atrribute, with the pad
      associated to all the injections performed at once. Even when
      whitening is also performed right after the injections.
    
    Raises
    ------
    ValueError
        Once injections have been performed at a certain SNR value, there
        cannot be injected again at the same value. Trying it will trigger
        this exception.
    
    &#34;&#34;&#34;
    if isinstance(snr, (int, float)):
        snr_list = list(snr)
    elif isinstance(snr, list):
        snr_list = snr
    else:
        raise TypeError(f&#34;&#39;{type(snr)}&#39; is not a valid &#39;snr&#39; type&#34;)
    
    if set(snr_list) &amp; set(self.snr_list):
        raise ValueError(&#34;one or more SNR values are already present in the dataset&#34;)

    if self._track_times:
        # Replaced temporarily because when injecting for the first time
        # we need to keep the original time arrays.
        times_new = self.times

    # 1st time making injections.
    if self.strains is None:
        self.strains = self._init_strains_dict()
        if self._track_times:
            # Redo the dictionary structure to include the SNR layer.
            times_new = self._init_strains_dict()
    
    for clas, id_ in dictools._unroll_nested_dictionary_keys(self.strains_clean):
        gw_clean = self.strains_clean[clas][id_]
        strain_clean_padded = np.pad(gw_clean, pad)
        # NOTE: Do not update the metadata nor times with this pad in case
        # the whitening is applied immediately after the injections.

        # Highpass filter to the clean signal.
        # NOTE: The noise realization is already generated without
        # frequency components lower than the cutoff (they are set to
        # 0 during the random sampling).
        strain_clean_padded = self.noise.highpass_filter(
            strain_clean_padded, f_cut=self.freq_cutoff, f_order=self.freq_butter_order
        )

        
        # Strain injections
        for snr_ in snr_list:
            # &#39;pad&#39; is added to &#39;snr_offset&#39; to compensate for the padding
            # which has not been updated in the &#39;metadata&#39; yet.
            injected = self._inject(
                strain_clean_padded, snr_, id=id_, snr_offset=pad
            )
            if self.whitened:
                injected = fat.whiten(
                    injected, asd=self.asd_array, unpad=pad, sample_rate=self.sample_rate,
                    # Parameters for GWpy&#39;s whiten() function:
                    highpass=self.freq_cutoff, flength=self.flength
                )
            self.strains[clas][id_][snr_] = injected
        
        # Time arrays:
        # - All SNR entries pointing to the SAME time array.
        # - Enlarge if the strains were padded and no whitening followed.
        if self._track_times:
            times_i = self.get_times(clas, id_)
            if pad &gt; 0 and not self.whitened:
                times_i = tat.pad_time_array(times_i, pad)
            for snr_ in snr_list:
                times_new[clas][id_][snr_] = times_i
    
    if self._track_times:
        self.times = times_new
    
    # Record new SNR values and related padding.
    # NOTE: Even if whitening is applied (and hence the length unaltered)
    # pad values are still registered, just in case.
    self.snr_list += snr_list
    for snr_ in snr_list:
        self.pad[snr_] = pad
    
    # Side-effect attributes updated.
    self.max_length = self._find_max_length()
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.get_xtest_array"><code class="name flex">
<span>def <span class="ident">get_xtest_array</span></span>(<span>self, length:Â intÂ =Â None, classes:Â strÂ |Â listÂ =Â 'all', snr:Â intÂ |Â listÂ |Â strÂ =Â 'all', with_metadata:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the test subset stacked in a zero-padded Numpy 2d-array.</p>
<p>Stacks all signals in the test subset into an homogeneous numpy array
whose length (axis=1) is determined by either 'length' or, if None, by
the longest strain in the subset. The remaining space is zeroed.</p>
<p>Allows the possibility to filter by class and SNR.</p>
<p>NOTE: Same signals injected at different SNR are stacked continuously.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Target length of the 'test_array'. If None, the longest signal
determines the length.</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str]</code></dt>
<dd>Classes which to include in the stack.
All classes are included by default.</dd>
<dt><strong><code>snr</code></strong> :&ensp;<code>int | list[int] | str</code></dt>
<dd>
<p>SNR injections which to include in the stack. If more than one are
selected, they are stacked zipped as follows:</p>
<p><code>eos0 id0 snr0
eos0 id0 snr1
...</code></p>
<p>All injections are included by default.</p>
</dd>
<dt><strong><code>with_metadata</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the associated metadata is returned in addition to the
test array in a Pandas DataFrame instance.
This metadata is obtained from the original 'metadata' attribute,
with the former index inserted as the first column, 'id', and with an
additional column for the SNR values.
False by default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>test_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Test subset.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>Original length of each strain, following the same order as the
first axis of 'test_array'.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pd.DataFrame</code>, optional</dt>
<dd>If 'with_metadata' is True, the associated metadata is returned
with its entries in the same order as the 'test_array'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xtest_array(self,
                    length: int = None,
                    classes: str | list = &#39;all&#39;,
                    snr: int | list | str = &#39;all&#39;,
                    with_metadata: bool = False):
    &#34;&#34;&#34;Get the test subset stacked in a zero-padded Numpy 2d-array.

    Stacks all signals in the test subset into an homogeneous numpy array
    whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
    the longest strain in the subset. The remaining space is zeroed.

    Allows the possibility to filter by class and SNR.

    NOTE: Same signals injected at different SNR are stacked continuously.

    Parameters
    ----------
    length : int, optional
        Target length of the &#39;test_array&#39;. If None, the longest signal
        determines the length.

    classes : str | list[str]
        Classes which to include in the stack.
        All classes are included by default.

    snr : int | list[int] | str
        SNR injections which to include in the stack. If more than one are
        selected, they are stacked zipped as follows:
        
        ```
        eos0 id0 snr0
        eos0 id0 snr1
             ...
        ```
        
        All injections are included by default.

    with_metadata : bool
        If True, the associated metadata is returned in addition to the
        test array in a Pandas DataFrame instance.
        This metadata is obtained from the original &#39;metadata&#39; attribute,
        with the former index inserted as the first column, &#39;id&#39;, and with an
        additional column for the SNR values.
        False by default.

    Returns
    -------
    test_array : np.ndarray
        Test subset.

    lengths : list
        Original length of each strain, following the same order as the
        first axis of &#39;test_array&#39;.

    metadata : pd.DataFrame, optional
        If &#39;with_metadata&#39; is True, the associated metadata is returned
        with its entries in the same order as the &#39;test_array&#39;.
    
    &#34;&#34;&#34;
    return self._stack_subset(self.Xtest, length, classes, snr, with_metadata)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.get_xtrain_array"><code class="name flex">
<span>def <span class="ident">get_xtrain_array</span></span>(<span>self, length:Â intÂ =Â None, classes:Â strÂ |Â listÂ =Â 'all', snr:Â intÂ |Â listÂ |Â strÂ =Â 'all', with_metadata:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the train subset stacked in a zero-padded Numpy 2d-array.</p>
<p>Stacks all signals in the train subset into an homogeneous numpy array
whose length (axis=1) is determined by either 'length' or, if None, by
the longest strain in the subset. The remaining space is zeroed.</p>
<p>Allows the possibility to filter by class and SNR.</p>
<p>NOTE: Same signals injected at different SNR are stacked continuously.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Target length of the 'train_array'. If None, the longest signal
determines the length.</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str]</code></dt>
<dd>Classes which to include in the stack.
All classes are included by default.</dd>
<dt><strong><code>snr</code></strong> :&ensp;<code>int | list[int] | str</code></dt>
<dd>
<p>SNR injections which to include in the stack. If more than one are
selected, they are stacked zipped as follows:</p>
<p><code>eos0 id0 snr0
eos0 id0 snr1
...</code></p>
<p>All injections are included by default.</p>
</dd>
<dt><strong><code>with_metadata</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the associated metadata is returned in addition to the
train array in a Pandas DataFrame instance.
This metadata is obtained from the original 'metadata' attribute,
with the former index inserted as the first column, 'id', and with an
additional column for the SNR values.
False by default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>train_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Train subset.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>Original length of each strain, following the same order as the
first axis of 'train_array'.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pd.DataFrame</code>, optional</dt>
<dd>If 'with_metadata' is True, the associated metadata is returned
with its entries in the same order as the 'train_array'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xtrain_array(self,
                     length: int = None,
                     classes: str | list = &#39;all&#39;,
                     snr: int | list | str = &#39;all&#39;,
                     with_metadata: bool = False):
    &#34;&#34;&#34;Get the train subset stacked in a zero-padded Numpy 2d-array.

    Stacks all signals in the train subset into an homogeneous numpy array
    whose length (axis=1) is determined by either &#39;length&#39; or, if None, by
    the longest strain in the subset. The remaining space is zeroed.

    Allows the possibility to filter by class and SNR.

    NOTE: Same signals injected at different SNR are stacked continuously.

    Parameters
    ----------
    length : int, optional
        Target length of the &#39;train_array&#39;. If None, the longest signal
        determines the length.

    classes : str | list[str]
        Classes which to include in the stack.
        All classes are included by default.

    snr : int | list[int] | str
        SNR injections which to include in the stack. If more than one are
        selected, they are stacked zipped as follows:
        
        ```
        eos0 id0 snr0
        eos0 id0 snr1
             ...
        ```
        
        All injections are included by default.
    
    with_metadata : bool
        If True, the associated metadata is returned in addition to the
        train array in a Pandas DataFrame instance.
        This metadata is obtained from the original &#39;metadata&#39; attribute,
        with the former index inserted as the first column, &#39;id&#39;, and with an
        additional column for the SNR values.
        False by default.

    Returns
    -------
    train_array : np.ndarray
        Train subset.
    
    lengths : list
        Original length of each strain, following the same order as the
        first axis of &#39;train_array&#39;.
    
    metadata : pd.DataFrame, optional
        If &#39;with_metadata&#39; is True, the associated metadata is returned
        with its entries in the same order as the &#39;train_array&#39;.

    &#34;&#34;&#34;
    return self._stack_subset(self.Xtrain, length, classes, snr, with_metadata)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.get_ytest_array"><code class="name flex">
<span>def <span class="ident">get_ytest_array</span></span>(<span>self, classes='all', snr='all', with_id=False, with_index=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the filtered test labels.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str] | 'all'</code></dt>
<dd>The classes to include in the labels.
All classes are included by default.</dd>
<dt><strong><code>snr</code></strong> :&ensp;<code>int | list[int] | str</code></dt>
<dd>The SNR injections to include in the labels.
All injections are included by default.</dd>
<dt><strong><code>with_id</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related IDs.
False by default.</dd>
<dt><strong><code>with_index</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related GLOBAL indices w.r.t. the stacked
arrays returned by 'get_xtest_array' WITHOUT filters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Filtered test labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>IDs associated to the filtered test labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>Indices associated to the filtered test labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ytest_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
    &#34;&#34;&#34;Get the filtered test labels.

    Parameters
    ----------
    classes : str | list[str] | &#39;all&#39;
        The classes to include in the labels.
        All classes are included by default.
    
    snr : int | list[int] | str
        The SNR injections to include in the labels.
        All injections are included by default.

    with_id : bool
        If True, return also the related IDs.
        False by default.

    with_index : bool
        If True, return also the related GLOBAL indices w.r.t. the stacked
        arrays returned by &#39;get_xtest_array&#39; WITHOUT filters.

    Returns
    -------
    np.ndarray
        Filtered test labels.

    np.ndarray, optional
        IDs associated to the filtered test labels.
    
    np.ndarray, optional
        Indices associated to the filtered test labels.

    &#34;&#34;&#34;
    return self._filter_labels(
        self.Ytest, list(self.Xtest), classes, snr,
        with_id=with_id, with_index=with_index
    )</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.get_ytrain_array"><code class="name flex">
<span>def <span class="ident">get_ytrain_array</span></span>(<span>self, classes='all', snr='all', with_id=False, with_index=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the filtered training labels.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>str | list[str] | 'all'</code></dt>
<dd>The classes to include in the labels.
All classes are included by default.</dd>
<dt><strong><code>snr</code></strong> :&ensp;<code>int | list[int] | str</code></dt>
<dd>The SNR injections to include in the labels.
All injections are included by default.</dd>
<dt><strong><code>with_id</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related IDs.
False by default.</dd>
<dt><strong><code>with_index</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return also the related GLOBAL indices w.r.t. the stacked
arrays returned by 'get_xtrain_array' WITHOUT filters.
False by default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Filtered train labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>IDs associated to the filtered train labels.</dd>
<dt><code>np.ndarray</code>, optional</dt>
<dd>Indices associated to the filtered train labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ytrain_array(self, classes=&#39;all&#39;, snr=&#39;all&#39;, with_id=False, with_index=False):
    &#34;&#34;&#34;Get the filtered training labels.

    Parameters
    ----------
    classes : str | list[str] | &#39;all&#39;
        The classes to include in the labels.
        All classes are included by default.
    
    snr : int | list[int] | str
        The SNR injections to include in the labels.
        All injections are included by default.

    with_id : bool
        If True, return also the related IDs.
        False by default.

    with_index : bool
        If True, return also the related GLOBAL indices w.r.t. the stacked
        arrays returned by &#39;get_xtrain_array&#39; WITHOUT filters.
        False by default.

    Returns
    -------
    np.ndarray
        Filtered train labels.

    np.ndarray, optional
        IDs associated to the filtered train labels.
    
    np.ndarray, optional
        Indices associated to the filtered train labels.

    &#34;&#34;&#34;
    return self._filter_labels(
        self.Ytrain, list(self.Xtrain), classes, snr,
        with_id=with_id, with_index=with_index
    )</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.psd"><code class="name flex">
<span>def <span class="ident">psd</span></span>(<span>self, frequencies:Â floatÂ |Â numpy.ndarray[float]) ->Â numpy.ndarray[float]</span>
</code></dt>
<dd>
<div class="desc"><p>Power spectral density (PSD) of the detector at given frequencies.</p>
<p>Interpolates the PSD at the given frequencies from their array
representation. If during initialization the PSD was given as its
array representation, the interpolant is computed using SciPy's
quadratic spline interpolant function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def psd(self, frequencies: float | np.ndarray[float]) -&gt; np.ndarray[float]:
    &#34;&#34;&#34;Power spectral density (PSD) of the detector at given frequencies.

    Interpolates the PSD at the given frequencies from their array
    representation. If during initialization the PSD was given as its
    array representation, the interpolant is computed using SciPy&#39;s
    quadratic spline interpolant function.

    &#34;&#34;&#34;
    return self._psd(frequencies)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.stack_by_id"><code class="name flex">
<span>def <span class="ident">stack_by_id</span></span>(<span>self, id_list:Â list, length:Â intÂ =Â None, snr_included:Â intÂ |Â list[int]Â |Â strÂ =Â 'all')</span>
</code></dt>
<dd>
<div class="desc"><p>Stack a subset of strains by ID into a zero-padded 2d-array.</p>
<p>This may allow (for example) to group up strains by their original ID
without leaking differnet injections (SNR) of the same strain into
different splits.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id_list</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The IDs of the strains to be stacked.</dd>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The target length of the stacked array. If None, the longest signal
determines the length.</dd>
<dt><strong><code>snr_included</code></strong> :&ensp;<code>int | list[int] | str</code>, optional</dt>
<dd>
<p>The SNR injections to include in the stack. If more than one are
selected, they are stacked zipped as follows:</p>
<p><code>id0 snr0
id0 snr1
...</code></p>
<p>All injections are included by default.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stacked_signals</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The array containing the stacked strains.</dd>
<dt><strong><code>lengths</code></strong> :&ensp;<code>list</code></dt>
<dd>The original lengths of each strain, following the same order as
the first axis of 'stacked_signals'.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>Unlike in 'get_xtrain_array' and 'get_xtest_array', this method does
not filter by 'classes' since it would be redundant, as IDs are
unique.</li>
</ul>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the value of 'snr' is not valid.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stack_by_id(self,
                id_list: list,
                length: int = None,
                snr_included: int | list[int] | str = &#39;all&#39;):
    &#34;&#34;&#34;Stack a subset of strains by ID into a zero-padded 2d-array.

    This may allow (for example) to group up strains by their original ID
    without leaking differnet injections (SNR) of the same strain into
    different splits.

    Parameters
    ----------
    id_list : array-like
        The IDs of the strains to be stacked.

    length : int, optional
        The target length of the stacked array. If None, the longest signal
        determines the length.

    snr_included : int | list[int] | str, optional
        The SNR injections to include in the stack. If more than one are
        selected, they are stacked zipped as follows:
        
        ```
        id0 snr0
        id0 snr1
          ...
        ```
        
        All injections are included by default.

    Returns
    -------
    stacked_signals : np.ndarray
        The array containing the stacked strains.

    lengths : list
        The original lengths of each strain, following the same order as
        the first axis of &#39;stacked_signals&#39;.

    Notes
    -----
    - Unlike in &#39;get_xtrain_array&#39; and &#39;get_xtest_array&#39;, this method does
      not filter by &#39;classes&#39; since it would be redundant, as IDs are
      unique.

    Raises
    ------
    ValueError
        If the value of &#39;snr&#39; is not valid.

    &#34;&#34;&#34;
    if not isinstance(id_list, list):
        raise TypeError(&#34;&#39;id_list&#39; must be a list of IDs.&#34;)
    
    # Collapse the Class layer.
    strains = {id: ds for sub_strains in self.strains.values() for id, ds in sub_strains.items()}

    # Filter out those not in the &#39;id_list&#39;.
    strains = dictools.filter_nested_dict(strains, lambda k: k in id_list, layer=0)

    # Filter out those injections whose SNR isnot in the &#39;snr&#39; list.
    if isinstance(snr_included, (int, list)):
        if isinstance(snr_included, int):
            snr_included = [snr_included]
        # NOTE: Here SNR is in Layer 1 because we collapsed the Class layer.
        strains = dictools.filter_nested_dict(strains, lambda k: k in snr_included, layer=1)
    elif snr_included != &#39;all&#39;:
        raise ValueError(&#34;the value of &#39;snr&#39; is not valid&#34;)

    strains = dictools.flatten_nested_dict(strains)  # keys: &#34;(id, snr)&#34;
    stacked_signals, lengths = dictools.dict_to_stacked_array(strains, target_length=length)
    
    return stacked_signals, lengths</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.BaseInjected.whiten"><code class="name flex">
<span>def <span class="ident">whiten</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Whiten injected strains.</p>
<p>Calling this method performs the whitening of all injected strains.
Strains are later cut to their original size before adding the pad,
to remove the vigneting.</p>
<p>NOTE: This is an irreversible action; if the original injections need
to be preserved it is advised to make a copy of the instance before
performing the whitening.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def whiten(self):
    &#34;&#34;&#34;Whiten injected strains.
    
    Calling this method performs the whitening of all injected strains.
    Strains are later cut to their original size before adding the pad,
    to remove the vigneting.
    
    NOTE: This is an irreversible action; if the original injections need
    to be preserved it is advised to make a copy of the instance before
    performing the whitening.
    
    &#34;&#34;&#34;
    if self.whitened:
        raise RuntimeError(&#34;dataset already whitened&#34;)

    if self.strains is None:
        raise RuntimeError(&#34;no injections have been performed yet&#34;)

    flength = self.whiten_params[&#39;flength&#39;]
    asd_array = self.whiten_params[&#39;asd_array&#39;]
    pad = self.whiten_params[&#39;pad&#39;]
    unpad = self.whiten_params[&#39;unpad&#39;]
    highpass = self.whiten_params[&#39;highpass&#39;]
    
    for *keys, strain in self.items():
        snr = keys[-1]  # Shape of self.strains dict-&gt; {class: {id: {snr: strain}}}

        # NOTE: I designed this while building the InjectedCoReWaves class,
        # so chances are the parameters here are not passed in the most
        # generalized way.
        strain_w = fat.whiten(
            strain, asd=asd_array, pad=pad, unpad=unpad[snr], sample_rate=self.sample_rate,
            highpass=highpass, flength=flength
        )
        # Update strains attribute.
        dictools.set_value_to_nested_dict(self.strains, keys, strain_w)
    
    # Shrink time arrays accordingly.
    if self._track_times:
        key_layers = dictools._unroll_nested_dictionary_keys(
            self.strains,
            max_depth=self._dict_depth-1  # same signal at different SNR has same time points.
        )
        for keys in key_layers:
            # Since all time arrays below SNR layer are the same, get the first one:
            times_i = dictools._get_value_from_nested_dict(self.times, keys)
            snr0 = next(iter(times_i.keys()))
            times_i = times_i[snr0]
            times_i = tat.shrink_time_array(times_i, unpad[snr0])
            for snr in self.snr_list:
                keys_all = keys + [snr]
                dictools.set_value_to_nested_dict(self.times, keys_all, times_i)
    
    self.whitened = True
    
    # Side-effect attributes updated.
    self.max_length = self._find_max_length()
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></b></code>:
<ul class="hlist">
<li><code><a title="gwadama.datasets.Base.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.Base.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.Base.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.Base.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.Base.resample" href="#gwadama.datasets.Base.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.Base.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gwadama.datasets.CoReWaves"><code class="flex name class">
<span>class <span class="ident">CoReWaves</span></span>
<span>(</span><span>*, coredb:Â <a title="gwadama.ioo.CoReManager" href="ioo.html#gwadama.ioo.CoReManager">CoReManager</a>, classes:Â dict[str], discarded:Â set, cropped:Â dict, distance:Â float, inclination:Â float, phi:Â float)</span>
</code></dt>
<dd>
<div class="desc"><p>Manage all operations needed to perform over a noiseless CoRe dataset.</p>
<p>Initial strains and metadata are obtained from a CoReManager instance.</p>
<p>NOTE: This class treats as different classes (categories) each equation of
state (EOS) present in the CoReManager instance.</p>
<p>NOTE^2: This class adds a time attribute with time samples related to each
GW.</p>
<p>Workflow:</p>
<ul>
<li>
<p>Load the strains from a CoreWaEasy instance, discarding or cropping those
indicated with their respective arguments.</p>
</li>
<li>
<p>Resample.</p>
</li>
<li>
<p>Project onto the ET detector arms.</p>
</li>
<li>
<p>Change units and scale from geometrized to IS and vice versa.</p>
</li>
<li>
<p>Export the (latest version of) dataset to a HDF5.</p>
</li>
<li>
<p>Export the (latest version of) dataset to a GWF.</p>
</li>
</ul>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict of strings and their integer labels, one per class (category).
The keys are the name of the Equation of State (EOS) used to describe
the physics behind the simulation which produced each strain.</dd>
<dt><strong><code>strains</code></strong> :&ensp;<code>dict {class: {id: gw_strain} }</code></dt>
<dd>Strains stored as a nested dictionary, with each strain in an
independent array to provide more flexibility with data of a wide
range of lengths.
The class key is the name of the class, a string which must exist in
the 'classes' list.
The 'id' is an unique identifier for each strain, and must exist in the
<code>self.metadata.index</code> column of the metadata DataFrame.
Initially, an extra depth layer is defined to store the polarizations
of the CoRe GW simulated data. After the projection this layer will be
collapsed to a single strain.</dd>
<dt><strong><code>times</code></strong> :&ensp;<code>dict {class: {id: gw_time_points} }</code></dt>
<dd>Time samples associated with the strains, following the same structure.
Useful when the sampling rate is variable or different between strains.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>
<p>All parameters and data related to the strains.
The order is the same as inside 'strains' if unrolled to a flat list
of strains up to the second depth level (the id.).
Example:</p>
<p><code>metadata[eos][key] = {
'id': str,
'mass': float,
'mass_ratio': float,
'eccentricity': float,
'mass_starA': float,
'mass_starB': float,
'spin_starA': float,
'spin_starB': float
}</code></p>
</dd>
<dt><strong><code>units</code></strong> :&ensp;<code>str</code></dt>
<dd>Flag indicating whether the data is in 'geometrized' or 'IS' units.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>
<p>Initially this attribute is None because the initial GW from CoRe are
sampled at different and non-constant sampling rates. After the
resampling, this attribute will be set to the new global sampling rate.</p>
<p>Caveat: If the 'times' attribute is present, this value is ignored.
Otherwise it is assumed all strains are constantly sampled to this.</p>
</dd>
</dl>
<p>Initialize a CoReWaves dataset.</p>
<dl>
<dt><strong><code>TODO</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coredb</code></strong> :&ensp;<code>ioo.CoReManager</code></dt>
<dd>Instance of CoReManager with the actual data.</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>dict[str]</code></dt>
<dd>Dictionary with the Equation of State (class) name as key and the
corresponding label index as value.</dd>
<dt><strong><code>discarded</code></strong> :&ensp;<code>set[str]</code></dt>
<dd>Set of GW IDs to discard from the dataset.</dd>
<dt><strong><code>cropped</code></strong> :&ensp;<code>dict[str]</code></dt>
<dd>Dictionary with the class name as key and the corresponding
cropping range as value. The range is given as a tuple of the form
(start_index, stop_index).</dd>
<dt><strong><code>distance</code></strong> :&ensp;<code>float</code></dt>
<dd>Distance to the source in Mpc.</dd>
<dt><strong><code>inclination</code></strong> :&ensp;<code>float</code></dt>
<dd>Inclination of the source in radians.</dd>
<dt><strong><code>phi</code></strong> :&ensp;<code>float</code></dt>
<dd>Azimuthal angle of the source in radians.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoReWaves(Base):
    &#34;&#34;&#34;Manage all operations needed to perform over a noiseless CoRe dataset.

    Initial strains and metadata are obtained from a CoReManager instance.

    NOTE: This class treats as different classes (categories) each equation of
    state (EOS) present in the CoReManager instance.

    NOTE^2: This class adds a time attribute with time samples related to each
    GW.

    Workflow:
    
    - Load the strains from a CoreWaEasy instance, discarding or cropping those
      indicated with their respective arguments.
    
    - Resample.
    
    - Project onto the ET detector arms.
    
    - Change units and scale from geometrized to IS and vice versa.
    
    - Export the (latest version of) dataset to a HDF5.
    
    - Export the (latest version of) dataset to a GWF.

    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).
        The keys are the name of the Equation of State (EOS) used to describe
        the physics behind the simulation which produced each strain.
    
    strains : dict {class: {id: gw_strain} }
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        The class key is the name of the class, a string which must exist in
        the &#39;classes&#39; list.
        The &#39;id&#39; is an unique identifier for each strain, and must exist in the
        `self.metadata.index` column of the metadata DataFrame.
        Initially, an extra depth layer is defined to store the polarizations
        of the CoRe GW simulated data. After the projection this layer will be
        collapsed to a single strain.
    
    times : dict {class: {id: gw_time_points} }
        Time samples associated with the strains, following the same structure.
        Useful when the sampling rate is variable or different between strains.
    
    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains up to the second depth level (the id.).
        Example:
        
        ```
        metadata[eos][key] = {
            &#39;id&#39;: str,
            &#39;mass&#39;: float,
            &#39;mass_ratio&#39;: float,
            &#39;eccentricity&#39;: float,
            &#39;mass_starA&#39;: float,
            &#39;mass_starB&#39;: float,
            &#39;spin_starA&#39;: float,
            &#39;spin_starB&#39;: float
        }
        ```
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.
    
    sample_rate : int, optional
        Initially this attribute is None because the initial GW from CoRe are
        sampled at different and non-constant sampling rates. After the
        resampling, this attribute will be set to the new global sampling rate.

        Caveat: If the &#39;times&#39; attribute is present, this value is ignored.
        Otherwise it is assumed all strains are constantly sampled to this.

    &#34;&#34;&#34;
    def __init__(self,
                 *,
                 coredb: ioo.CoReManager,
                 classes: dict[str],
                 discarded: set,
                 cropped: dict,
                 # Source:
                 distance: float,
                 inclination: float,
                 phi: float):
        &#34;&#34;&#34;Initialize a CoReWaves dataset.

        TODO

        Parameters
        ----------
        coredb : ioo.CoReManager
            Instance of CoReManager with the actual data.
        
        classes : dict[str]
            Dictionary with the Equation of State (class) name as key and the
            corresponding label index as value.
        
        discarded : set[str]
            Set of GW IDs to discard from the dataset.
        
        cropped : dict[str]
            Dictionary with the class name as key and the corresponding
            cropping range as value. The range is given as a tuple of the form
            (start_index, stop_index).
        
        distance : float
            Distance to the source in Mpc.
        
        inclination : float
            Inclination of the source in radians.
        
        phi : float
            Azimuthal angle of the source in radians.

        &#34;&#34;&#34;
        self._check_classes_dict(classes)
        self.classes = classes
        self.discarded = discarded
        self.cropped = cropped
        # Source parameters
        self.distance = distance
        self.inclination = inclination
        self.phi = phi

        self.units = &#39;IS&#39;
        self.strains, self.times, self.metadata = self._get_strain_and_metadata(coredb)
        self._track_times = True
        self._dict_depth = dictools.get_depth(self.strains)
        self.labels = self._gen_labels()
        self.max_length = self._find_max_length()

        self.sample_rate = None  # Set up after resampling
        self.random_seed = None  # Set if calling the &#39;build_train_test_subsets&#39; method.

        self.whitened = False
        self.whiten_params = {}
        self.nonwhiten_strains = None

        # Train/Test subset splits (views into the same &#39;self.strains&#39;).
        #   Timeseries:
        self.Xtrain: np.ndarray = None
        self.Xtest: np.ndarray = None
        #   Labels:
        self.Ytrain: np.ndarray = None
        self.Ytest: np.ndarray = None
    
    def _get_strain_and_metadata(self, coredb: ioo.CoReManager) -&gt; tuple[dict, dict, pd.DataFrame]:
        &#34;&#34;&#34;Obtain the strain and metadata from a CoReManager instance.

        The strains are the Pluss and Cross polarizations obtained from the
        direct output of numerical relativistic simulations. They are expected
        to be projected at the detector afterwards, collapsing the polarization
        layer to a single strain per GW.
        
        Returns
        -------
        strains : dict{eos: {id: {pol: strain} } }
        
        times : dict{&#39;eos&#39;: {&#39;id&#39;: {pol: time_samples}} }
            Time samples associated to each GW.
            Since it has to follow the same nested structure as &#39;strains&#39;, but
            the time samples are the same among polarizations, for each GW both
            polarizations point to the same array in memory.
        
        metadata : pandas.DataFrame
            All parameters and data related to the strains.
            The order is the same as inside &#39;strains&#39; if unrolled to a flat list
            of strains up to the second depth level (the id.).
        
        &#34;&#34;&#34;
        strains = self._init_strains_dict()
        times = self._init_strains_dict()
        # Metadata columns/keys:
        index: list[str] = []
        mass: list[float] = []
        mass_ratio: list[float] = []
        eccentricity: list[float] = []
        mass_starA: list[float] = []
        mass_starB: list[float] = []
        spin_starA: list[float] = []
        spin_starB: list[float] = []
        merger_pos: list[int] = []  # Index position of the merger inside the array.

        for eos in self.classes:
            # Get and filter out GW simulations.
            ids = set(coredb.filter_by(&#39;id_eos&#39;, eos).index)
            try:
                ids -= self.discarded[eos]
            except KeyError:
                pass  # No discards.
            ids = sorted(ids)  # IMPORTANT!!! Keep order to be able to trace back simulations.
            
            for id_ in ids:
                # CoRe Rh data (in IS units):
                times_, h_plus, h_cros = coredb.gen_strain(
                    id_, self.distance, self.inclination, self.phi
                )

                # Crop those indicated at the parameter file, and leave whole
                # the rest.
                try:
                    t0, t1 = self.cropped[eos][id_]
                except KeyError:
                    crop = slice(None)
                else:
                    crop = slice(
                        np.argmin(np.abs(times_-t0)),
                        np.argmin(np.abs(times_-t1))
                    )
                strains[eos][id_] = {
                    &#39;plus&#39;: h_plus[crop],
                    &#39;cross&#39;: h_cros[crop]
                }
                # Both polarizations have the same sampling times, hence we
                # point each time polarization to the same array in memory.
                times[eos][id_] = {}
                times[eos][id_][&#39;plus&#39;] = times[eos][id_][&#39;cross&#39;] = times_[crop]
                
                # The time is centered at the merger.
                i_merger = tat.find_time_origin(times_[crop])

                # Associated metadata:
                md = coredb.metadata.loc[id_]
                index.append(md[&#39;database_key&#39;])
                mass.append(md[&#39;id_mass&#39;])
                mass_ratio.append(md[&#39;id_mass_ratio&#39;])
                eccentricity.append(md[&#39;id_eccentricity&#39;])
                mass_starA.append(md[&#39;id_mass_starA&#39;])
                mass_starB.append(md[&#39;id_mass_starB&#39;])
                spin_starA.append(md[&#39;id_spin_starA&#39;])
                spin_starB.append(md[&#39;id_spin_starB&#39;])
                merger_pos.append(i_merger)
        
        metadata = pd.DataFrame(
            data=dict(
                mass=mass, mass_ratio=mass_ratio, eccentricity=eccentricity,
                mass_starA=mass_starA, mass_starB=mass_starB,
                spin_starA=spin_starA, spin_starB=spin_starB,
                merger_pos=merger_pos
            ),
            index=index
        )
        
        return strains, times, metadata
    
    def find_merger(self, strain: np.ndarray) -&gt; int:
        from clawdia.estimators import find_merger
        return find_merger(strain)

    def _update_merger_positions(self):
        &#34;&#34;&#34;Update all &#39;merger_pos&#39; tags inside the metadata attribute.
        
        Time arrays are defined with the origin at the merger. When the length
        of the strain arrays is modified, the index position of the merger
        must be updated.

        NOTE: This method updates ALL the merger positions.
        
        &#34;&#34;&#34;
        for clas, id_ in self.keys(max_depth=2):
            times = self.times[clas][id_]
            # If more layers are present, only get the first instance of times
            # since all will be the same.
            if isinstance(times, dict):
                times = dictools._get_next_item(times)
            self.metadata.at[id_,&#39;merger_pos&#39;] = tat.find_time_origin(times)
    
    def resample(self, sample_rate, verbose=False) -&gt; None:
        &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

        Resample CoRe strains (from NR simulations) to a constant rate.

        This method updates the sample_rate, the max_length and the merger_pos
        inside the metadata attribute.

        Parameters
        ----------
        sample_rate : int
            The new sampling rate in Hz.

        verbose : bool
            If True, print information about the resampling.
        
        &#34;&#34;&#34;
        super().resample(sample_rate, verbose)

        # Update side-effect attributes.
        self._update_merger_positions()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def project(self, *, detector: str, ra: float, dec: float, geo_time: float, psi: float):
        &#34;&#34;&#34;Project strains into the chosen detector at specified coordinates.

        Project strains into the chosen detector at specified coordinates,
        using Bilby.

        This collapses the polarization layer in &#39;strains&#39; and &#39;times&#39; to a
        single strain.
        The times are rebuilt taking as a reference point the merger (t = 0).
        
        Parameters
        ----------
        detector : str
            Name of the ET arm in Bilby for InterferometerList().
        
        ra, dec : float
            Sky position in equatorial coordinates.
        
        geo_time : int | float
            Time of injection in GPS.
        
        psi : float
            Polarization angle.

        Caveats
        -------
        - The detector&#39;s name must exist in Bilby&#39;s InterferometerList().
        - Only one arm can be chosen.
        
        &#34;&#34;&#34;
        project_pars = dict(ra=ra, dec=dec, geocent_time=geo_time, psi=psi)
        for clas, id_ in self.keys(max_depth=2):
            hp = self.strains[clas][id_][&#39;plus&#39;]
            hc = self.strains[clas][id_][&#39;cross&#39;]
            
            # Drop the polarization layer.
            strain = detectors.project(
                hp, hc, parameters=project_pars, sf=self.sample_rate, 
                nfft=2*self.sample_rate, detector=detector
            )
            self.strains[clas][id_] = strain
            
            # Regenerate the time array with the merger located at the origin.
            duration = len(strain) / self.sample_rate
            t_merger = self.find_merger(strain) / self.sample_rate
            t0 = -t_merger
            t1 = duration - t_merger
            self.times[clas][id_] = tat.gen_time_array(t0, t1, self.sample_rate)
        
        # Update side-effect attributes
        self._dict_depth = dictools.get_depth(self.strains)
        self._update_merger_positions()
        self.max_length = self._find_max_length()
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def shrink_to_merger(self, offset: int = 0) -&gt; None:
        &#34;&#34;&#34;Shrink strains and time arrays w.r.t. the merger.

        Shrink strains (and their associated time arrays) discarding the left
        side of the merger (inspiral), with a given offset in samples.

        This also updates the metadata column &#39;merger_pos&#39;.

        NOTE: This is an irreversible action.

        Parameters
        ----------
        offset : int
            Offset in samples relative to the merger position.

        &#34;&#34;&#34;
        limits = {}
        for clas, id, *keys in self.keys():
            i_merger = self.metadata.at[id, &#39;merger_pos&#39;]
            # Same shrinking limits for all possible strains below ID layer.
            limits[id] = (i_merger+offset, -1)
        
        self.shrink_strains(limits)

        # Update side-effect attributes.
        self._update_merger_positions()
        if self.Xtrain is not None:
            self._update_train_test_subsets()

    def convert_to_IS_units(self) -&gt; None:
        &#34;&#34;&#34;Convert data from scaled geometrized units to IS units.

        Convert strains and times from geometrized units (scaled to the mass
        of the system and the source distance) to IS units.
        
        Will raise an error if the data is already in IS units.
        
        &#34;&#34;&#34;
        if self.units == &#39;IS&#39;:
            raise RuntimeError(&#34;data already in IS units&#34;)

        for keys in self.keys():
            id_ = keys[1]
            mass = self.metadata.at[id_,&#39;mass&#39;]
            strain = self.get_strain(*keys)
            times = self.get_times(*keys)

            strain *=  mass * MSUN_MET / (self.distance * MPC_MET)
            times *= mass * MSUN_SEC

        self.units = &#39;IS&#39;

        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()
    
    def convert_to_scaled_geometrized_units(self) -&gt; None:
        &#34;&#34;&#34;Convert data from IS to scaled geometrized units.
        
        Convert strains and times from IS to geometrized units, and scaled to the mass
        of the system and the source distance.

        Will raise an error if the data is already in geometrized units.
        
        &#34;&#34;&#34;
        if self.units == &#39;geometrized&#39;:
            raise RuntimeError(&#34;data already in geometrized units&#34;)
        
        for keys in self.keys():
            id_ = keys[1]
            mass = self.metadata.at[id_,&#39;mass&#39;]
            strain = self.get_strain(*keys)
            times = self.get_times(*keys)
            
            strain /=  mass * MSUN_MET / (self.distance * MPC_MET)
            times /= mass * MSUN_SEC

        self.units = &#39;geometrized&#39;

        # Update side-effect attributes.
        if self.Xtrain is not None:
            self._update_train_test_subsets()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="gwadama.datasets.CoReWaves.convert_to_IS_units"><code class="name flex">
<span>def <span class="ident">convert_to_IS_units</span></span>(<span>self) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Convert data from scaled geometrized units to IS units.</p>
<p>Convert strains and times from geometrized units (scaled to the mass
of the system and the source distance) to IS units.</p>
<p>Will raise an error if the data is already in IS units.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_to_IS_units(self) -&gt; None:
    &#34;&#34;&#34;Convert data from scaled geometrized units to IS units.

    Convert strains and times from geometrized units (scaled to the mass
    of the system and the source distance) to IS units.
    
    Will raise an error if the data is already in IS units.
    
    &#34;&#34;&#34;
    if self.units == &#39;IS&#39;:
        raise RuntimeError(&#34;data already in IS units&#34;)

    for keys in self.keys():
        id_ = keys[1]
        mass = self.metadata.at[id_,&#39;mass&#39;]
        strain = self.get_strain(*keys)
        times = self.get_times(*keys)

        strain *=  mass * MSUN_MET / (self.distance * MPC_MET)
        times *= mass * MSUN_SEC

    self.units = &#39;IS&#39;

    # Update side-effect attributes.
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.CoReWaves.convert_to_scaled_geometrized_units"><code class="name flex">
<span>def <span class="ident">convert_to_scaled_geometrized_units</span></span>(<span>self) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Convert data from IS to scaled geometrized units.</p>
<p>Convert strains and times from IS to geometrized units, and scaled to the mass
of the system and the source distance.</p>
<p>Will raise an error if the data is already in geometrized units.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_to_scaled_geometrized_units(self) -&gt; None:
    &#34;&#34;&#34;Convert data from IS to scaled geometrized units.
    
    Convert strains and times from IS to geometrized units, and scaled to the mass
    of the system and the source distance.

    Will raise an error if the data is already in geometrized units.
    
    &#34;&#34;&#34;
    if self.units == &#39;geometrized&#39;:
        raise RuntimeError(&#34;data already in geometrized units&#34;)
    
    for keys in self.keys():
        id_ = keys[1]
        mass = self.metadata.at[id_,&#39;mass&#39;]
        strain = self.get_strain(*keys)
        times = self.get_times(*keys)
        
        strain /=  mass * MSUN_MET / (self.distance * MPC_MET)
        times /= mass * MSUN_SEC

    self.units = &#39;geometrized&#39;

    # Update side-effect attributes.
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.CoReWaves.find_merger"><code class="name flex">
<span>def <span class="ident">find_merger</span></span>(<span>self, strain:Â numpy.ndarray) ->Â int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_merger(self, strain: np.ndarray) -&gt; int:
    from clawdia.estimators import find_merger
    return find_merger(strain)</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.CoReWaves.project"><code class="name flex">
<span>def <span class="ident">project</span></span>(<span>self, *, detector:Â str, ra:Â float, dec:Â float, geo_time:Â float, psi:Â float)</span>
</code></dt>
<dd>
<div class="desc"><p>Project strains into the chosen detector at specified coordinates.</p>
<p>Project strains into the chosen detector at specified coordinates,
using Bilby.</p>
<p>This collapses the polarization layer in 'strains' and 'times' to a
single strain.
The times are rebuilt taking as a reference point the merger (t = 0).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>detector</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the ET arm in Bilby for InterferometerList().</dd>
<dt><strong><code>ra</code></strong>, <strong><code>dec</code></strong> :&ensp;<code>float</code></dt>
<dd>Sky position in equatorial coordinates.</dd>
<dt><strong><code>geo_time</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Time of injection in GPS.</dd>
<dt><strong><code>psi</code></strong> :&ensp;<code>float</code></dt>
<dd>Polarization angle.</dd>
</dl>
<h2 id="caveats">Caveats</h2>
<ul>
<li>The detector's name must exist in Bilby's InterferometerList().</li>
<li>Only one arm can be chosen.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def project(self, *, detector: str, ra: float, dec: float, geo_time: float, psi: float):
    &#34;&#34;&#34;Project strains into the chosen detector at specified coordinates.

    Project strains into the chosen detector at specified coordinates,
    using Bilby.

    This collapses the polarization layer in &#39;strains&#39; and &#39;times&#39; to a
    single strain.
    The times are rebuilt taking as a reference point the merger (t = 0).
    
    Parameters
    ----------
    detector : str
        Name of the ET arm in Bilby for InterferometerList().
    
    ra, dec : float
        Sky position in equatorial coordinates.
    
    geo_time : int | float
        Time of injection in GPS.
    
    psi : float
        Polarization angle.

    Caveats
    -------
    - The detector&#39;s name must exist in Bilby&#39;s InterferometerList().
    - Only one arm can be chosen.
    
    &#34;&#34;&#34;
    project_pars = dict(ra=ra, dec=dec, geocent_time=geo_time, psi=psi)
    for clas, id_ in self.keys(max_depth=2):
        hp = self.strains[clas][id_][&#39;plus&#39;]
        hc = self.strains[clas][id_][&#39;cross&#39;]
        
        # Drop the polarization layer.
        strain = detectors.project(
            hp, hc, parameters=project_pars, sf=self.sample_rate, 
            nfft=2*self.sample_rate, detector=detector
        )
        self.strains[clas][id_] = strain
        
        # Regenerate the time array with the merger located at the origin.
        duration = len(strain) / self.sample_rate
        t_merger = self.find_merger(strain) / self.sample_rate
        t0 = -t_merger
        t1 = duration - t_merger
        self.times[clas][id_] = tat.gen_time_array(t0, t1, self.sample_rate)
    
    # Update side-effect attributes
    self._dict_depth = dictools.get_depth(self.strains)
    self._update_merger_positions()
    self.max_length = self._find_max_length()
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.CoReWaves.resample"><code class="name flex">
<span>def <span class="ident">resample</span></span>(<span>self, sample_rate, verbose=False) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Resample strain and time arrays to a constant rate.</p>
<p>Resample CoRe strains (from NR simulations) to a constant rate.</p>
<p>This method updates the sample_rate, the max_length and the merger_pos
inside the metadata attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>The new sampling rate in Hz.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, print information about the resampling.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resample(self, sample_rate, verbose=False) -&gt; None:
    &#34;&#34;&#34;Resample strain and time arrays to a constant rate.

    Resample CoRe strains (from NR simulations) to a constant rate.

    This method updates the sample_rate, the max_length and the merger_pos
    inside the metadata attribute.

    Parameters
    ----------
    sample_rate : int
        The new sampling rate in Hz.

    verbose : bool
        If True, print information about the resampling.
    
    &#34;&#34;&#34;
    super().resample(sample_rate, verbose)

    # Update side-effect attributes.
    self._update_merger_positions()
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
<dt id="gwadama.datasets.CoReWaves.shrink_to_merger"><code class="name flex">
<span>def <span class="ident">shrink_to_merger</span></span>(<span>self, offset:Â intÂ =Â 0) ->Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Shrink strains and time arrays w.r.t. the merger.</p>
<p>Shrink strains (and their associated time arrays) discarding the left
side of the merger (inspiral), with a given offset in samples.</p>
<p>This also updates the metadata column 'merger_pos'.</p>
<p>NOTE: This is an irreversible action.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>offset</code></strong> :&ensp;<code>int</code></dt>
<dd>Offset in samples relative to the merger position.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shrink_to_merger(self, offset: int = 0) -&gt; None:
    &#34;&#34;&#34;Shrink strains and time arrays w.r.t. the merger.

    Shrink strains (and their associated time arrays) discarding the left
    side of the merger (inspiral), with a given offset in samples.

    This also updates the metadata column &#39;merger_pos&#39;.

    NOTE: This is an irreversible action.

    Parameters
    ----------
    offset : int
        Offset in samples relative to the merger position.

    &#34;&#34;&#34;
    limits = {}
    for clas, id, *keys in self.keys():
        i_merger = self.metadata.at[id, &#39;merger_pos&#39;]
        # Same shrinking limits for all possible strains below ID layer.
        limits[id] = (i_merger+offset, -1)
    
    self.shrink_strains(limits)

    # Update side-effect attributes.
    self._update_merger_positions()
    if self.Xtrain is not None:
        self._update_train_test_subsets()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></b></code>:
<ul class="hlist">
<li><code><a title="gwadama.datasets.Base.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.Base.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtest_array" href="#gwadama.datasets.Base.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtrain_array" href="#gwadama.datasets.Base.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytest_array" href="#gwadama.datasets.Base.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytrain_array" href="#gwadama.datasets.Base.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.Base.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.Base.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
<li><code><a title="gwadama.datasets.Base.stack_by_id" href="#gwadama.datasets.Base.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.Base.whiten" href="#gwadama.datasets.Base.whiten">whiten</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gwadama.datasets.InjectedCoReWaves"><code class="flex name class">
<span>class <span class="ident">InjectedCoReWaves</span></span>
<span>(</span><span>clean_dataset:Â <a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a>, *, psd:Â Union[numpy.ndarray,Â Callable], detector:Â str, noise_length:Â int, whiten_params:Â dict, freq_cutoff:Â intÂ |Â float, freq_butter_order:Â intÂ |Â float, random_seed:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Manage injections of GW data from CoRe dataset.</p>
<ul>
<li>
<p>Tracks index position of the merger.</p>
</li>
<li>
<p>Computes the SNR only at the ring-down starting from the merger.</p>
</li>
<li>
<p>Computes also the usual SNR over the whole signal and stores it for
later reference (attr. 'whole_snr_list').</p>
</li>
</ul>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>snr_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Partial SNR values at which each signal is injected.
This SNR is computed ONLY over the Ring-Down section of the waveform
starting from the merger, hence the name 'partial SNR'.</dd>
<dt><strong><code>whole_snr</code></strong> :&ensp;<code>dict</code></dt>
<dd>Nested dictionary storing for each injection the equivalent SNR value
computed over the whole signal, hence the name 'whole SNR'.
Structure: {id_: {partial_snr: whole_snr}}</dd>
<dt><strong><code>TODO</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Initializes an instance of the InjectedCoReWaves class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clean_dataset</code></strong> :&ensp;<code><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></code></dt>
<dd>An instance of a BaseDataset class with noiseless signals.</dd>
<dt><strong><code>psd</code></strong> :&ensp;<code>np.ndarray | Callable</code></dt>
<dd>
<p>Power Spectral Density of the detector's sensitivity in the
range of frequencies of interest.
Can be given as a callable function whose argument is
expected to be an array of frequencies, or as a 2d-array
with shape (2, psd_length) so that</p>
<p><code>psd[0] = frequency_samples
psd[1] = psd_samples.</code></p>
<p>NOTE: It is also used to compute the 'asd' attribute (ASD).</p>
</dd>
<dt><strong><code>detector</code></strong> :&ensp;<code>str</code></dt>
<dd>GW detector name.</dd>
<dt><strong><code>noise_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the background noise array to be generated for
later use.
It should be at least longer than the longest signal
expected to be injected.</dd>
<dt><strong><code>whiten_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Parameters to be passed to the 'whiten' method of the
'BaseInjected' class.</dd>
<dt><strong><code>freq_cutoff</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Frequency cutoff for the filter applied to the signal.</dd>
<dt><strong><code>freq_butter_order</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Order of the Butterworth filter applied to the signal.</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Random seed for generating random numbers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InjectedCoReWaves(BaseInjected):
    &#34;&#34;&#34;Manage injections of GW data from CoRe dataset.
    
    - Tracks index position of the merger.
    
    - Computes the SNR only at the ring-down starting from the merger.
    
    - Computes also the usual SNR over the whole signal and stores it for
      later reference (attr. &#39;whole_snr_list&#39;).

    Attributes
    ----------
    snr_list : list
        Partial SNR values at which each signal is injected.
        This SNR is computed ONLY over the Ring-Down section of the waveform
        starting from the merger, hence the name &#39;partial SNR&#39;.

    whole_snr : dict
        Nested dictionary storing for each injection the equivalent SNR value
        computed over the whole signal, hence the name &#39;whole SNR&#39;.
        Structure: {id_: {partial_snr: whole_snr}}

    TODO

    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: Base,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 whiten_params: dict,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        &#34;&#34;&#34;
        Initializes an instance of the InjectedCoReWaves class.

        Parameters
        ----------
        clean_dataset : Base
            An instance of a BaseDataset class with noiseless signals.
        
        psd : np.ndarray | Callable
            Power Spectral Density of the detector&#39;s sensitivity in the
            range of frequencies of interest.
            Can be given as a callable function whose argument is
            expected to be an array of frequencies, or as a 2d-array
            with shape (2, psd_length) so that

            ```
                psd[0] = frequency_samples
                psd[1] = psd_samples.
            ```
            
            NOTE: It is also used to compute the &#39;asd&#39; attribute (ASD).
        
        detector : str
            GW detector name.
        
        noise_length : int
            Length of the background noise array to be generated for
            later use.
            It should be at least longer than the longest signal
            expected to be injected.
        
        whiten_params : dict
            Parameters to be passed to the &#39;whiten&#39; method of the
            &#39;BaseInjected&#39; class.
        
        freq_cutoff : int | float
            Frequency cutoff for the filter applied to the signal.
        
        freq_butter_order : int | float
            Order of the Butterworth filter applied to the signal.
        
        random_seed : int
            Random seed for generating random numbers.
        
        &#34;&#34;&#34;
        super().__init__(
            clean_dataset, psd=psd, detector=detector, noise_length=noise_length,
            whiten_params=whiten_params, freq_cutoff=freq_cutoff,
            freq_butter_order=freq_butter_order, random_seed=random_seed
        )

        self.whole_snr = {id_: {} for id_ in self.metadata.index}

    def _update_merger_positions(self):
        &#34;&#34;&#34;Update all &#39;merger_pos&#39; tags inside the metadata attribute.
        
        Time arrays are defined with the origin at the merger. When the length
        of the strain arrays is modified, the index position of the merger
        must be updated.

        NOTE: This method updates ALL the merger positions.
        
        &#34;&#34;&#34;
        for clas, id_ in self.keys(max_depth=2):
            # Same time array for all SNR variations.
            times = next(iter(self.times[clas][id_].values()))
            self.metadata.at[id_,&#39;merger_pos&#39;] = tat.find_time_origin(times)
    
    def gen_injections(self, snr: int | float | list, pad: int = 0):
        super().gen_injections(snr, pad)

        self._update_merger_positions()
    
    def _inject(self,
                strain: np.ndarray,
                snr: int | float,
                *,
                id: str,
                snr_offset: int) -&gt; np.ndarray:
        &#34;&#34;&#34;Inject a strain at &#39;snr&#39; into noise using &#39;self.noise&#39; instance.

        Parameters
        ----------
        strain : NDArray
            Signal to be injected into noise.
        
        snr : int | float
            Targeted signal-to-noise ratio.
        
        id : str
            Signal identifier (2nd layer of &#39;strains&#39; dict).
        
        snr_offset : int
            Offset (w.r.t. the merger) added to the start of the range for
            computing the SNR.
        
        Returns
        -------
        injected : NDArray
            Injected signal.
        
        NOTES
        -----
        - The SNR is computed over the Ring-Down only, starting from the
          position of the merger.
        - The metadata is expected to reflect the original state of the strains
          previous to any padding performed right before calling this function,
          which may be done to avoid the vignette effect.
        
        &#34;&#34;&#34;
        clas = self.find_class(id)
        merger_pos = self.metadata.at[id,&#39;merger_pos&#39;]
        original_length = len(self.strains_clean[clas][id])

        i0 = merger_pos + snr_offset
        i1 = (original_length - merger_pos) + snr_offset
        injected, scale = self.noise.inject(strain, snr=snr, snr_lim=(i0, i1))

        # Compute the equivalent SNR over the entire waveform.
        self.whole_snr[id][snr] = self.noise.snr(strain*scale)

        return injected
    
    def whiten(self):
        super().whiten()

        self._update_merger_positions()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></li>
<li><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></b></code>:
<ul class="hlist">
<li><code><a title="gwadama.datasets.BaseInjected.asd" href="#gwadama.datasets.BaseInjected.asd">asd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.export_strains_to_gwf" href="#gwadama.datasets.BaseInjected.export_strains_to_gwf">export_strains_to_gwf</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.gen_injections" href="#gwadama.datasets.BaseInjected.gen_injections">gen_injections</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtest_array" href="#gwadama.datasets.BaseInjected.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtrain_array" href="#gwadama.datasets.BaseInjected.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytest_array" href="#gwadama.datasets.BaseInjected.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytrain_array" href="#gwadama.datasets.BaseInjected.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.psd" href="#gwadama.datasets.BaseInjected.psd">psd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.resample" href="#gwadama.datasets.Base.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.stack_by_id" href="#gwadama.datasets.BaseInjected.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.whiten" href="#gwadama.datasets.BaseInjected.whiten">whiten</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gwadama.datasets.InjectedSyntheticWaves"><code class="flex name class">
<span>class <span class="ident">InjectedSyntheticWaves</span></span>
<span>(</span><span>clean_dataset:Â <a title="gwadama.datasets.SyntheticWaves" href="#gwadama.datasets.SyntheticWaves">SyntheticWaves</a>, *, psd:Â Union[numpy.ndarray,Â Callable], detector:Â str, noise_length:Â int, freq_cutoff:Â intÂ |Â float, freq_butter_order:Â intÂ |Â float, random_seed:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO</p>
<p>Base constructor for injected datasets.</p>
<p>TODO: Update docstring.</p>
<p>When inheriting from this class, it is recommended to run this method
first in your <strong>init</strong> function.</p>
<p>Relevant attributes are inherited from the 'clean_dataset' instance,
which can be any inherited from BaseDataset whose strains have not
been injected yet.</p>
<p>If train/test subsets are present, they too are updated when performing
injections or changing units, but only through re-building them from
the main 'strains' attribute using the already generated indices.
Original train/test subsets from the clean dataset are not inherited.</p>
<p>WARNING: Initializing this class does not perform the injections! For
that use the method 'gen_injections'.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clean_dataset</code></strong> :&ensp;<code><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></code></dt>
<dd>Instance of a Class(Base) with noiseless signals.</dd>
<dt><strong><code>psd</code></strong> :&ensp;<code>np.ndarray | Callable</code></dt>
<dd>
<p>Power Spectral Density of the detector's sensitivity in the range
of frequencies of interest. Can be given as a callable function
whose argument is expected to be an array of frequencies, or as a
2d-array with shape (2, psd_length) so that</p>
<p><code>psd[0] = frequency_samples
psd[1] = psd_samples</code>.</p>
<p>NOTE: It is also used to compute the 'asd' attribute (ASD).</p>
</dd>
<dt><strong><code>detector</code></strong> :&ensp;<code>str</code></dt>
<dd>GW detector name.
Not used, just for identification.</dd>
<dt><strong><code>noise_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the background noise array to be generated for later use.
It should be at least longer than the longest signal expected to be
injected.</dd>
<dt><strong><code>whiten_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>Parameters of the whitening filter, with the following entries:</p>
<ul>
<li>
<p>'flength' : int
Length (in samples) of the time-domain FIR whitening.</p>
</li>
<li>
<p>'highpass' : float
Frequency cutoff.</p>
</li>
<li>
<p>'normed' : bool
Normalization applied after the whitening filter.</p>
</li>
</ul>
</dd>
<dt><strong><code>freq_cutoff</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Frequency cutoff below which no noise bins will be generated in the
frequency space, and also used for the high-pass filter applied to
clean signals before injection.</dd>
<dt><strong><code>freq_butter_order</code></strong> :&ensp;<code>int | float</code></dt>
<dd>Butterworth filter order.
See (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html</a>)
for more information.</dd>
<dt><strong><code>flength</code></strong> :&ensp;<code>int</code></dt>
<dd>Length (in samples) of the time-domain FIR whitening filter.</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Value passed to 'sklearn.model_selection.train_test_split' to
generate the Train and Test subsets.
Saved for reproducibility purposes, and also used to initialize
Numpy's default RandomGenerator.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InjectedSyntheticWaves(BaseInjected):
    &#34;&#34;&#34;TODO
    
    &#34;&#34;&#34;
    def __init__(self,
                 clean_dataset: SyntheticWaves,
                 *,
                 psd: np.ndarray | Callable,
                 detector: str,
                 noise_length: int,
                 freq_cutoff: int | float,
                 freq_butter_order: int | float,
                 random_seed: int):
        super().__init__(
            clean_dataset, psd=psd, detector=detector, noise_length=noise_length,
            freq_cutoff=freq_cutoff, freq_butter_order=freq_butter_order, random_seed=random_seed
        )

        # Initialize the Train/Test subsets inheriting the indices of the input
        # clean dataset instance.
        self.Xtrain = dictools._replicate_structure_nested_dict(clean_dataset.Xtrain)
        self.Xtest = dictools._replicate_structure_nested_dict(clean_dataset.Xtest)
        self.Ytrain = dictools._replicate_structure_nested_dict(clean_dataset.Ytrain)
        self.Ytest = dictools._replicate_structure_nested_dict(clean_dataset.Ytest)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></li>
<li><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></b></code>:
<ul class="hlist">
<li><code><a title="gwadama.datasets.BaseInjected.asd" href="#gwadama.datasets.BaseInjected.asd">asd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.export_strains_to_gwf" href="#gwadama.datasets.BaseInjected.export_strains_to_gwf">export_strains_to_gwf</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.gen_injections" href="#gwadama.datasets.BaseInjected.gen_injections">gen_injections</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtest_array" href="#gwadama.datasets.BaseInjected.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtrain_array" href="#gwadama.datasets.BaseInjected.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytest_array" href="#gwadama.datasets.BaseInjected.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytrain_array" href="#gwadama.datasets.BaseInjected.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.psd" href="#gwadama.datasets.BaseInjected.psd">psd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.resample" href="#gwadama.datasets.Base.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.stack_by_id" href="#gwadama.datasets.BaseInjected.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.whiten" href="#gwadama.datasets.BaseInjected.whiten">whiten</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gwadama.datasets.SyntheticWaves"><code class="flex name class">
<span>class <span class="ident">SyntheticWaves</span></span>
<span>(</span><span>*, classes:Â dict, n_waves_per_class:Â int, wave_parameters_limits:Â dict, max_length:Â int, peak_time_max_length:Â float, amp_threshold:Â float, tukey_alpha:Â float, sample_rate:Â int, train_size:Â intÂ |Â float, random_seed:Â intÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for building synthetically generated wavforms and background noise.</p>
<p>Part of the datasets for the CLAWDIA main paper.</p>
<p>The classes are hardcoded:</p>
<pre><code>SG: Sine Gaussian,

G:  Gaussian,

RD: Ring-Down.
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>classes</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict of strings and their integer labels, one per class (category).</dd>
<dt><strong><code>strains</code></strong> :&ensp;<code>dict {class: {key: gw_strains} }</code></dt>
<dd>Strains stored as a nested dictionary, with each strain in an
independent array to provide more flexibility with data of a wide
range of lengths.
The class key is the name of the class, a string which must exist in
the 'classes' attribute.
The 'key' is an identifier of each strain.
In this case it's just the global index ranging from 0 to 'self.n_samples'.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>NDArray[int]</code></dt>
<dd>Indices of the classes, one per waveform.
Each one points its respective waveform inside 'strains' to its class
in 'classes'. The order is that of the index of 'self.metadata', and
coincides with the order of the strains inside 'self.strains' if
unrolled to a flat list of arrays.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>All parameters and data related to the strains.
The order is the same as inside 'strains' if unrolled to a flat list
of strains.</dd>
<dt><strong><code>units</code></strong> :&ensp;<code>str</code></dt>
<dd>Flag indicating whether the data is in 'geometrized' or 'IS' units.</dd>
<dt><strong><code>Xtrain</code></strong>, <strong><code>Xtest</code></strong> :&ensp;<code>dict {key: strain}</code></dt>
<dd>Train and test subsets randomly split using SKLearn train_test_split
function with stratified labels.
The key corresponds to the strain's index at 'self.metadata'.</dd>
<dt><strong><code>Ytrain</code></strong>, <strong><code>Ytest</code></strong> :&ensp;<code>NDArray[int]</code></dt>
<dd>1D Array containing the labels in the same order as 'Xtrain' and
'Xtest' respectively.</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_waves_per_class</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of waves per class to produce.</dd>
<dt><strong><code>wave_parameters_limits</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>Min/Max limits of the waveforms' parameters, 9 in total.
Keys:</p>
<ul>
<li>
<p>mf0,
Mf0:
min/Max central frequency (SG and RD).</p>
</li>
<li>
<p>mQ,
MQ:
min/Max quality factor (SG and RD).</p>
</li>
<li>
<p>mhrss, Mhrss: min/Max sum squared amplitude of the wave.</p>
</li>
<li>
<p>mT,
MT:
min/Max duration (only G).</p>
</li>
</ul>
</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of the waves. This parameter is used to generate the
initial time array with which the waveforms are computed.</dd>
<dt><strong><code>peak_time_max_length</code></strong> :&ensp;<code>float</code></dt>
<dd>Time of the peak of the envelope of the waves in the initial time
array (built with 'max_length').</dd>
<dt><strong><code>amp_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction w.r.t. the maximum absolute amplitude of the wave envelope
below which to end the wave by shrinking the array and applying a
windowing to the edges.</dd>
<dt><strong><code>tukey_alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Alpha parameter (width) of the Tukey window applied to each wave to
make sure their values end at the exact duration determined by either
the duration parameter or the amplitude threshold.</dd>
<dt><strong><code>train_size</code></strong> :&ensp;<code>int | float</code></dt>
<dd>If int, total number of samples to include in the train dataset.
If float, fraction of the total samples to include in the train
dataset.
For more details see 'sklearn.model_selection.train_test_split'
with the flag <code>stratified=True</code>.</dd>
<dt><strong><code>sample_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>random_seed : int, optional.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SyntheticWaves(Base):
    &#34;&#34;&#34;Class for building synthetically generated wavforms and background noise.

    Part of the datasets for the CLAWDIA main paper.
    
    The classes are hardcoded:
        
        SG: Sine Gaussian,
        
        G:  Gaussian,
        
        RD: Ring-Down.


    Attributes
    ----------
    classes : dict
        Dict of strings and their integer labels, one per class (category).
    
    strains : dict {class: {key: gw_strains} }
        Strains stored as a nested dictionary, with each strain in an
        independent array to provide more flexibility with data of a wide
        range of lengths.
        The class key is the name of the class, a string which must exist in
        the &#39;classes&#39; attribute.
        The &#39;key&#39; is an identifier of each strain.
        In this case it&#39;s just the global index ranging from 0 to &#39;self.n_samples&#39;.
    
    labels : NDArray[int]
        Indices of the classes, one per waveform.
        Each one points its respective waveform inside &#39;strains&#39; to its class
        in &#39;classes&#39;. The order is that of the index of &#39;self.metadata&#39;, and
        coincides with the order of the strains inside &#39;self.strains&#39; if
        unrolled to a flat list of arrays.
    
    metadata : pandas.DataFrame
        All parameters and data related to the strains.
        The order is the same as inside &#39;strains&#39; if unrolled to a flat list
        of strains.
    
    units : str
        Flag indicating whether the data is in &#39;geometrized&#39; or &#39;IS&#39; units.
    
    Xtrain, Xtest : dict {key: strain}
        Train and test subsets randomly split using SKLearn train_test_split
        function with stratified labels.
        The key corresponds to the strain&#39;s index at &#39;self.metadata&#39;.
    
    Ytrain, Ytest : NDArray[int]
        1D Array containing the labels in the same order as &#39;Xtrain&#39; and
        &#39;Xtest&#39; respectively.

    &#34;&#34;&#34;

    def __init__(self,
                 *,
                 classes: dict,
                 n_waves_per_class: int,
                 wave_parameters_limits: dict,
                 max_length: int,
                 peak_time_max_length: float,
                 amp_threshold: float,
                 tukey_alpha: float,
                 sample_rate: int,
                 train_size: int | float, 
                 random_seed: int = None):
        &#34;&#34;&#34;
        Parameters
        ----------
        n_waves_per_class : int
            Number of waves per class to produce.

        wave_parameters_limits : dict
            Min/Max limits of the waveforms&#39; parameters, 9 in total.
            Keys:
            
            - mf0,   Mf0:   min/Max central frequency (SG and RD).
            
            - mQ,    MQ:    min/Max quality factor (SG and RD).
            
            - mhrss, Mhrss: min/Max sum squared amplitude of the wave.
            
            - mT,    MT:    min/Max duration (only G).
        
        max_length : int
            Maximum length of the waves. This parameter is used to generate the
            initial time array with which the waveforms are computed.
        
        peak_time_max_length : float
            Time of the peak of the envelope of the waves in the initial time
            array (built with &#39;max_length&#39;).
        
        amp_threshold : float
            Fraction w.r.t. the maximum absolute amplitude of the wave envelope
            below which to end the wave by shrinking the array and applying a
            windowing to the edges.
        
        tukey_alpha : float
            Alpha parameter (width) of the Tukey window applied to each wave to
            make sure their values end at the exact duration determined by either
            the duration parameter or the amplitude threshold.
        
        train_size : int | float
            If int, total number of samples to include in the train dataset.
            If float, fraction of the total samples to include in the train
            dataset.
            For more details see &#39;sklearn.model_selection.train_test_split&#39;
            with the flag `stratified=True`.
        
        sample_rate : int
        
        random_seed : int, optional.
        
        &#34;&#34;&#34;
        self._check_classes_dict(self.classes)
        self.classes = classes
        self.n_waves_per_class = n_waves_per_class
        self.sample_rate = sample_rate
        self.wave_parameters_limits = wave_parameters_limits
        self.max_length = max_length
        self.peak_time_max_length = peak_time_max_length
        self.tukey_alpha = tukey_alpha
        self.amp_threshold = amp_threshold
        self.random_seed = random_seed
        self.rng = np.random.default_rng(random_seed)

        self._gen_metadata()
        self._track_times = False
        self._gen_dataset()
        self.labels = self._gen_labels()
        self.build_train_test_subsets(train_size)

    def _gen_metadata(self):
        &#34;&#34;&#34;Generate random metadata associated with each waveform.&#34;&#34;&#34;

        classes_list = []
        f0s_list = []
        Q_list = []
        hrss_list = []
        duration_list = []  # Will be modified afterwards to take into account
                            # the amplitude threshold.
        for clas in self.classes:
            for _ in range(self.n_waves_per_class):
                # Need to pass &#39;self&#39; explicitely since I&#39;m calling the methods
                # inside a dictionary attribute. Python doesn&#39;t seem to
                # recognise them as the same class methods this way.
                f0, Q, hrss, duration = self._gen_parameters[clas](self)
                classes_list.append(clas)
                f0s_list.append(f0)
                Q_list.append(Q)
                hrss_list.append(hrss)
                duration_list.append(duration)  

        self.metadata = pd.DataFrame({
            &#39;Class&#39;: classes_list,  # strings
            &#39;f0&#39;: f0s_list,
            &#39;Q&#39;: Q_list,
            &#39;hrss&#39;: hrss_list,
            &#39;duration&#39;: duration_list
        })

    def _gen_dataset(self):
        &#34;&#34;&#34;Generate the dataset from the previously generated metadata.

        After generating the waveforms with the analytical expressions it
        shrinks them to the specified duration in the metadata. This is
        necessary because the analytical expressions are infinite, so we apply
        a window to get perfect edges. However this does not necessary align
        with the exact duration provided by the metadata due to the signals
        being sampled at discrete values. Therefore after the windowing the
        final duration is computed again and updated in the metadata attribute.
        
        Attributes
        ----------
        strains : dict[dict]
            Creates the strains attribute with the properties stated at the
            class&#39; docstring.
        
        _dict_depth : int
            Number of nested layers in strains&#39; dictionary.
        
        metadata : pd.DataFrame
            Updates the duration of the waveforms after shrinking them.

        &#34;&#34;&#34;
        if self.metadata is None:
            raise AttributeError(&#34;&#39;metadata&#39; needs to be generated first!&#34;)

        self.strains = self._init_strains_dict()

        t_max = (self.max_length - 1) / self.sample_rate
        times = np.linspace(0, t_max, self.max_length)
        
        for i in range(len(self)):
            params = self.metadata.loc[i].to_dict()
            clas = params[&#39;Class&#39;]
            match clas:
                case &#39;SG&#39;:
                    self.strains[clas][i] = synthetic.sine_gaussian_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        f0=self.metadata.at[i,&#39;f0&#39;],
                        Q=self.metadata.at[i,&#39;Q&#39;],
                        hrss=self.metadata.at[i,&#39;hrss&#39;]
                    )
                case &#39;G&#39;:
                    self.strains[clas][i] = synthetic.gaussian_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        hrss=self.metadata.at[i,&#39;hrss&#39;],
                        duration=self.metadata.at[i,&#39;duration&#39;],
                        amp_threshold=self.amp_threshold
                    )
                case &#39;RD&#39;:
                    self.strains[clas][i] = synthetic.ring_down_waveform(
                        times,
                        t0=self.peak_time_max_length,
                        f0=self.metadata.at[i,&#39;f0&#39;],
                        Q=self.metadata.at[i,&#39;Q&#39;],
                        hrss=self.metadata.at[i,&#39;hrss&#39;]
                    )
        
        self._dict_depth = dictools.get_depth(self.strains)

        self._apply_threshold_windowing()
    
    def _random_log_uniform(self, min, max):
        &#34;&#34;&#34;Returns a random number between [min, max] spaced logarithmically.&#34;&#34;&#34;

        exponent = self.rng.uniform(np.log10(min), np.log10(max))
        random = 10**exponent

        return random
    
    def _random_log_int(self, min, max):
        &#34;&#34;&#34;Returns a random integer between [min, max] spaced logarithmically.&#34;&#34;&#34;

        return int(self._random_log_uniform(min, max))

    
    def _gen_parameters_sine_gaussian(self):
        &#34;&#34;&#34;Generate random parameters for a single Sine Gaussian.&#34;&#34;&#34;

        limits = self.wave_parameters_limits
        thres = self.amp_threshold
        f0   = self._random_log_int(limits[&#39;mf0&#39;], limits[&#39;Mf0&#39;])  # Central frequency
        Q    = self._random_log_int(limits[&#39;mQ&#39;], limits[&#39;MQ&#39;]+1)  # Quality factor
        hrss = self._random_log_uniform(limits[&#39;mhrss&#39;], limits[&#39;Mhrss&#39;])
        duration = 2 * Q / (np.pi * f0) * np.sqrt(-np.log(thres))
        
        return (f0, Q, hrss, duration)

    def _gen_parameters_gaussian(self):
        &#34;&#34;&#34;Generate random parameters for a single Gaussian.&#34;&#34;&#34;

        lims = self.wave_parameters_limits
        f0   = None  #  Casted to np.nan afterwards.
        Q    = None  #-/
        hrss = self._random_log_uniform(lims[&#39;mhrss&#39;], lims[&#39;Mhrss&#39;])
        duration = self._random_log_uniform(lims[&#39;mT&#39;], lims[&#39;MT&#39;])  # Duration
        
        return (f0, Q, hrss, duration)

    def _gen_parameters_ring_down(self):
        &#34;&#34;&#34;Generate random parameters for a single Ring-Down.&#34;&#34;&#34;

        lims = self.wave_parameters_limits
        thres = self.amp_threshold
        f0   = self._random_log_int(lims[&#39;mf0&#39;], lims[&#39;Mf0&#39;])  # Central frequency
        Q    = self._random_log_int(lims[&#39;mQ&#39;], lims[&#39;MQ&#39;]+1)  # Quality factor
        hrss = self._random_log_uniform(lims[&#39;mhrss&#39;], lims[&#39;Mhrss&#39;])
        duration = -np.sqrt(2) * Q / (np.pi * f0) * np.log(thres)
        
        return (f0, Q, hrss, duration)

    _gen_parameters = {
        &#39;SG&#39;: _gen_parameters_sine_gaussian,
        &#39;G&#39;: _gen_parameters_gaussian,
        &#39;RD&#39;: _gen_parameters_ring_down
    }

    def _apply_threshold_windowing(self):
        &#34;&#34;&#34;Shrink waves in the dataset and update its duration in the metadata.

        Shrink them according to their pre-computed duration in the metadata to
        avoid almost-but-not-zero edges, and correct those marginal durations
        longer than the window.

        &#34;&#34;&#34;
        for i in range(len(self)):
            clas = self.metadata.at[i,&#39;Class&#39;]
            duration = self.metadata.at[i,&#39;duration&#39;]
            ref_length = int(duration * self.sample_rate)
            
            if clas == &#39;RD&#39;:
                # Ring-Down waves begin at the center. However we want to
                # emphasize their energetic beginning, therefore we will leave
                # a symmetric part before their start with zeros.
                i0 = self.max_length // 2 - ref_length
                i1 = i0 + 2*ref_length
            else:
                # SG and G are both centered.
                i0 = (self.max_length - ref_length) // 2
                i1 = self.max_length - i0

            new_lenght = i1 - i0
            if i0 &lt; 0:
                new_lenght += i0
                i0 = 0
            if i1 &gt; self.max_length:
                new_lenght -= i1 - self.max_length
                i1 = self.max_length

            window = sp.signal.windows.tukey(new_lenght, alpha=self.tukey_alpha)
            # Shrink and window
            self.strains[clas][i] = self.strains[clas][i][i0:i1] * window

            self.metadata.at[i,&#39;duration&#39;] = new_lenght / self.sample_rate</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></b></code>:
<ul class="hlist">
<li><code><a title="gwadama.datasets.Base.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.Base.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtest_array" href="#gwadama.datasets.Base.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtrain_array" href="#gwadama.datasets.Base.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytest_array" href="#gwadama.datasets.Base.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytrain_array" href="#gwadama.datasets.Base.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.Base.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.Base.resample" href="#gwadama.datasets.Base.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.Base.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
<li><code><a title="gwadama.datasets.Base.stack_by_id" href="#gwadama.datasets.Base.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.Base.whiten" href="#gwadama.datasets.Base.whiten">whiten</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gwadama" href="index.html">gwadama</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gwadama.datasets.Base" href="#gwadama.datasets.Base">Base</a></code></h4>
<ul class="">
<li><code><a title="gwadama.datasets.Base.build_train_test_subsets" href="#gwadama.datasets.Base.build_train_test_subsets">build_train_test_subsets</a></code></li>
<li><code><a title="gwadama.datasets.Base.find_class" href="#gwadama.datasets.Base.find_class">find_class</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strain" href="#gwadama.datasets.Base.get_strain">get_strain</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_strains_array" href="#gwadama.datasets.Base.get_strains_array">get_strains_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_times" href="#gwadama.datasets.Base.get_times">get_times</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtest_array" href="#gwadama.datasets.Base.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_xtrain_array" href="#gwadama.datasets.Base.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytest_array" href="#gwadama.datasets.Base.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.get_ytrain_array" href="#gwadama.datasets.Base.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.Base.items" href="#gwadama.datasets.Base.items">items</a></code></li>
<li><code><a title="gwadama.datasets.Base.keys" href="#gwadama.datasets.Base.keys">keys</a></code></li>
<li><code><a title="gwadama.datasets.Base.resample" href="#gwadama.datasets.Base.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.Base.shrink_strains" href="#gwadama.datasets.Base.shrink_strains">shrink_strains</a></code></li>
<li><code><a title="gwadama.datasets.Base.stack_by_id" href="#gwadama.datasets.Base.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.Base.whiten" href="#gwadama.datasets.Base.whiten">whiten</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gwadama.datasets.BaseInjected" href="#gwadama.datasets.BaseInjected">BaseInjected</a></code></h4>
<ul class="">
<li><code><a title="gwadama.datasets.BaseInjected.asd" href="#gwadama.datasets.BaseInjected.asd">asd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.export_strains_to_gwf" href="#gwadama.datasets.BaseInjected.export_strains_to_gwf">export_strains_to_gwf</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.gen_injections" href="#gwadama.datasets.BaseInjected.gen_injections">gen_injections</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtest_array" href="#gwadama.datasets.BaseInjected.get_xtest_array">get_xtest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_xtrain_array" href="#gwadama.datasets.BaseInjected.get_xtrain_array">get_xtrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytest_array" href="#gwadama.datasets.BaseInjected.get_ytest_array">get_ytest_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.get_ytrain_array" href="#gwadama.datasets.BaseInjected.get_ytrain_array">get_ytrain_array</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.psd" href="#gwadama.datasets.BaseInjected.psd">psd</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.stack_by_id" href="#gwadama.datasets.BaseInjected.stack_by_id">stack_by_id</a></code></li>
<li><code><a title="gwadama.datasets.BaseInjected.whiten" href="#gwadama.datasets.BaseInjected.whiten">whiten</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gwadama.datasets.CoReWaves" href="#gwadama.datasets.CoReWaves">CoReWaves</a></code></h4>
<ul class="">
<li><code><a title="gwadama.datasets.CoReWaves.convert_to_IS_units" href="#gwadama.datasets.CoReWaves.convert_to_IS_units">convert_to_IS_units</a></code></li>
<li><code><a title="gwadama.datasets.CoReWaves.convert_to_scaled_geometrized_units" href="#gwadama.datasets.CoReWaves.convert_to_scaled_geometrized_units">convert_to_scaled_geometrized_units</a></code></li>
<li><code><a title="gwadama.datasets.CoReWaves.find_merger" href="#gwadama.datasets.CoReWaves.find_merger">find_merger</a></code></li>
<li><code><a title="gwadama.datasets.CoReWaves.project" href="#gwadama.datasets.CoReWaves.project">project</a></code></li>
<li><code><a title="gwadama.datasets.CoReWaves.resample" href="#gwadama.datasets.CoReWaves.resample">resample</a></code></li>
<li><code><a title="gwadama.datasets.CoReWaves.shrink_to_merger" href="#gwadama.datasets.CoReWaves.shrink_to_merger">shrink_to_merger</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gwadama.datasets.InjectedCoReWaves" href="#gwadama.datasets.InjectedCoReWaves">InjectedCoReWaves</a></code></h4>
</li>
<li>
<h4><code><a title="gwadama.datasets.InjectedSyntheticWaves" href="#gwadama.datasets.InjectedSyntheticWaves">InjectedSyntheticWaves</a></code></h4>
</li>
<li>
<h4><code><a title="gwadama.datasets.SyntheticWaves" href="#gwadama.datasets.SyntheticWaves">SyntheticWaves</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>